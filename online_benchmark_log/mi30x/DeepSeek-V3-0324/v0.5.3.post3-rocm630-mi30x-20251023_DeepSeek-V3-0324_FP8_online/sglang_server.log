INFO 10-23 16:51:30 __init__.py:179] Automatically detected platform rocm.
WARNING 10-23 16:51:30 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:32] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:32] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.9, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=807050224, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:32] Using default HuggingFace chat template with detected content format: string
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:40 __init__.py:179] Automatically detected platform rocm.
INFO 10-23 16:51:41 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:42 TP3] Process 294 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:42 TP3] Init torch distributed begin.
[2025-10-23 16:51:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:42 TP7] Process 298 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:42 TP4] Process 295 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:43 TP7] Init torch distributed begin.
[2025-10-23 16:51:43 TP4] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:43 TP0] Process 291 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:43 TP2] Process 293 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:61: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-23 16:51:43 TP6] Process 297 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:43 TP0] Init torch distributed begin.
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:43] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-23 16:51:43 TP5] Process 296 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:43 TP1] Process 292 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-23 16:51:43 TP2] Init torch distributed begin.
[2025-10-23 16:51:44 TP6] Init torch distributed begin.
[2025-10-23 16:51:44 TP5] Init torch distributed begin.
[2025-10-23 16:51:44 TP1] Init torch distributed begin.
[2025-10-23 16:51:45 TP0] sglang is using nccl==2.21.5
[2025-10-23 16:51:47 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-23 16:51:47 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-23 16:51:47 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-23 16:51:47 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-23 16:51:47 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-23 16:51:47 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 16:51:47 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-23 16:51:47 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-23 16:51:48 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-23 16:51:48 TP2] Load weight begin. avail mem=187.21 GB
[2025-10-23 16:51:48 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-23 16:51:48 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-23 16:51:48 TP1] Load weight begin. avail mem=187.21 GB
[2025-10-23 16:51:48 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-23 16:51:48 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-23 16:51:48 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-23 16:51:48 TP0] Detected fp8 checkpoint.
[2025-10-23 16:51:48 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:54,  2.98it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:58,  2.75it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:01<00:58,  2.73it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:51,  3.11it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:01<00:51,  3.06it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:45,  3.49it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:02<00:43,  3.56it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:02<00:44,  3.50it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:02<00:45,  3.35it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:03<00:50,  3.06it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<01:12,  2.09it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:04<00:55,  2.73it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:04<00:32,  4.53it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:04<00:19,  7.61it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:04<00:15,  9.47it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:11, 12.72it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:09, 15.03it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:07, 17.78it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:04<00:06, 19.50it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:05<00:06, 21.04it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:05<00:13,  9.43it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:05<00:11, 10.41it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:05<00:10, 11.77it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:06<00:10, 11.46it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:06<00:09, 12.65it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:06<00:07, 15.40it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:06<00:06, 17.78it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:06<00:05, 19.83it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:06<00:06, 16.53it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:07<00:05, 18.07it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:07<00:04, 20.95it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:07<00:05, 17.95it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:07<00:10,  9.27it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:08<00:08, 10.48it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:08<00:08, 10.81it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:08<00:07, 12.24it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:08<00:05, 14.80it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:08<00:04, 17.12it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:08<00:04, 17.98it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:09<00:06, 11.67it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:09<00:05, 12.43it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:09<00:04, 14.34it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:09<00:05, 13.17it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:09<00:04, 14.57it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:10<00:04, 12.90it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:10<00:06, 10.14it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:10<00:07,  7.63it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:10<00:07,  7.86it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:11<00:07,  7.94it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:11<00:10,  5.25it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:11<00:09,  5.67it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:12<00:07,  6.70it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:12<00:11,  4.60it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:12<00:10,  4.78it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:12<00:09,  5.00it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:13<00:07,  6.42it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:13<00:06,  6.91it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:13<00:04,  9.02it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:13<00:04,  8.73it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:13<00:04,  8.94it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:14<00:05,  7.40it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:14<00:04,  7.67it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:14<00:04,  8.91it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:14<00:03,  9.95it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:14<00:03, 10.17it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:14<00:02, 11.41it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:14<00:02, 12.04it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:15<00:02, 11.53it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:15<00:02, 10.94it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:15<00:01, 12.45it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:15<00:01, 14.05it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:15<00:01, 15.36it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:15<00:00, 16.32it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:16<00:01, 12.08it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:16<00:00, 13.64it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:16<00:00, 15.73it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:16<00:00, 17.75it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:17<00:00,  6.30it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:17<00:00,  7.65it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:17<00:00,  9.29it/s]

[2025-10-23 16:53:07 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.06 GB, mem usage=79.56 GB.
[2025-10-23 16:53:07 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-23 16:53:07 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-23 16:53:08 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-23 16:53:09 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-23 16:53:09 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.79 GB, mem usage=79.56 GB.
[2025-10-23 16:53:10 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.56 GB.
[2025-10-23 16:53:10 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-23 16:53:10 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-23 16:53:11 TP2] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP2] Memory pool end. avail mem=43.39 GB
[2025-10-23 16:53:11 TP6] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP6] Memory pool end. avail mem=43.52 GB
[2025-10-23 16:53:11 TP0] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP0] Memory pool end. avail mem=43.81 GB
[2025-10-23 16:53:11 TP3] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP3] Memory pool end. avail mem=43.40 GB
[2025-10-23 16:53:11 TP4] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP4] Memory pool end. avail mem=43.45 GB
[2025-10-23 16:53:11 TP1] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP1] Memory pool end. avail mem=43.40 GB
[2025-10-23 16:53:11 TP7] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP7] Memory pool end. avail mem=43.53 GB
[2025-10-23 16:53:11 TP5] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-23 16:53:11 TP5] Memory pool end. avail mem=43.54 GB
[2025-10-23 16:53:12 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-23 16:53:13 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.61 GB
[2025-10-23 16:53:13 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-23 16:53:13 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.25 GB
[2025-10-23 16:53:13 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.20 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.97 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-23 16:53:13 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-23 16:53:13 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.32 GB
[2025-10-23 16:53:14 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-23 16:53:14 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-23 16:53:15 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:16 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:18 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:18 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:19 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:19 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:19 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:19 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:19 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:19 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:20 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:20 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-23 16:53:20 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:20 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:20 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.97 GB):   2%|         | 1/52 [00:10<08:34, 10.08s/it]Capturing batches (bs=496 avail_mem=42.31 GB):   2%|         | 1/52 [00:10<08:34, 10.08s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:24 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.31 GB):   4%|         | 2/52 [00:12<04:38,  5.57s/it]Capturing batches (bs=480 avail_mem=42.30 GB):   4%|         | 2/52 [00:12<04:38,  5.57s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:25 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:25 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:25 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:25 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:25 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:26 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:26 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:26 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.30 GB):   6%|         | 3/52 [00:13<02:56,  3.61s/it]Capturing batches (bs=464 avail_mem=42.30 GB):   6%|         | 3/52 [00:13<02:56,  3.61s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:27 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.30 GB):   8%|         | 4/52 [00:14<02:03,  2.58s/it]Capturing batches (bs=448 avail_mem=42.29 GB):   8%|         | 4/52 [00:14<02:03,  2.58s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:28 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.29 GB):  10%|         | 5/52 [00:15<01:34,  2.01s/it]Capturing batches (bs=432 avail_mem=42.29 GB):  10%|         | 5/52 [00:15<01:34,  2.01s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:29 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.29 GB):  12%|        | 6/52 [00:16<01:19,  1.72s/it]Capturing batches (bs=416 avail_mem=42.28 GB):  12%|        | 6/52 [00:16<01:19,  1.72s/it][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:30 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.28 GB):  13%|        | 7/52 [00:17<01:06,  1.48s/it]Capturing batches (bs=400 avail_mem=42.28 GB):  13%|        | 7/52 [00:17<01:06,  1.48s/it][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:31 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.28 GB):  15%|        | 8/52 [00:18<00:56,  1.28s/it]Capturing batches (bs=384 avail_mem=42.27 GB):  15%|        | 8/52 [00:18<00:56,  1.28s/it][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.27 GB):  17%|        | 9/52 [00:19<00:46,  1.07s/it]Capturing batches (bs=368 avail_mem=42.27 GB):  17%|        | 9/52 [00:19<00:46,  1.07s/it][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:32 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.27 GB):  19%|        | 10/52 [00:20<00:40,  1.03it/s]Capturing batches (bs=352 avail_mem=42.27 GB):  19%|        | 10/52 [00:20<00:40,  1.03it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:33 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.27 GB):  21%|        | 11/52 [00:20<00:36,  1.11it/s]Capturing batches (bs=336 avail_mem=42.26 GB):  21%|        | 11/52 [00:20<00:36,  1.11it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.26 GB):  23%|       | 12/52 [00:21<00:33,  1.20it/s]Capturing batches (bs=320 avail_mem=42.26 GB):  23%|       | 12/52 [00:21<00:33,  1.20it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:34 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.26 GB):  25%|       | 13/52 [00:22<00:28,  1.35it/s]Capturing batches (bs=304 avail_mem=42.25 GB):  25%|       | 13/52 [00:22<00:28,  1.35it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:35 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.25 GB):  27%|       | 14/52 [00:22<00:27,  1.39it/s]Capturing batches (bs=288 avail_mem=42.25 GB):  27%|       | 14/52 [00:22<00:27,  1.39it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.25 GB):  29%|       | 15/52 [00:22<00:21,  1.73it/s]Capturing batches (bs=272 avail_mem=42.24 GB):  29%|       | 15/52 [00:22<00:21,  1.73it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.24 GB):  31%|       | 16/52 [00:23<00:18,  1.90it/s]Capturing batches (bs=256 avail_mem=42.24 GB):  31%|       | 16/52 [00:23<00:18,  1.90it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:36 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:36 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.24 GB):  33%|      | 17/52 [00:23<00:17,  2.04it/s]Capturing batches (bs=248 avail_mem=42.23 GB):  33%|      | 17/52 [00:23<00:17,  2.04it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.23 GB):  35%|      | 18/52 [00:24<00:15,  2.16it/s]Capturing batches (bs=240 avail_mem=42.23 GB):  35%|      | 18/52 [00:24<00:15,  2.16it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:37 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.23 GB):  37%|      | 19/52 [00:24<00:14,  2.24it/s]Capturing batches (bs=232 avail_mem=42.22 GB):  37%|      | 19/52 [00:24<00:14,  2.24it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.22 GB):  38%|      | 20/52 [00:25<00:13,  2.31it/s]Capturing batches (bs=224 avail_mem=42.22 GB):  38%|      | 20/52 [00:25<00:13,  2.31it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.22 GB):  40%|      | 21/52 [00:25<00:13,  2.36it/s]Capturing batches (bs=216 avail_mem=42.21 GB):  40%|      | 21/52 [00:25<00:13,  2.36it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:38 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.21 GB):  42%|     | 22/52 [00:25<00:12,  2.38it/s]Capturing batches (bs=208 avail_mem=42.21 GB):  42%|     | 22/52 [00:25<00:12,  2.38it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.21 GB):  44%|     | 23/52 [00:26<00:10,  2.69it/s]Capturing batches (bs=200 avail_mem=42.20 GB):  44%|     | 23/52 [00:26<00:10,  2.69it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.20 GB):  46%|     | 24/52 [00:26<00:10,  2.63it/s]Capturing batches (bs=192 avail_mem=42.20 GB):  46%|     | 24/52 [00:26<00:10,  2.63it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:39 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.20 GB):  48%|     | 25/52 [00:26<00:09,  2.90it/s]Capturing batches (bs=184 avail_mem=42.20 GB):  48%|     | 25/52 [00:26<00:09,  2.90it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.20 GB):  50%|     | 26/52 [00:27<00:08,  3.12it/s]Capturing batches (bs=176 avail_mem=42.19 GB):  50%|     | 26/52 [00:27<00:08,  3.12it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.19 GB):  52%|    | 27/52 [00:27<00:07,  3.26it/s]Capturing batches (bs=168 avail_mem=42.19 GB):  52%|    | 27/52 [00:27<00:07,  3.26it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.19 GB):  54%|    | 28/52 [00:27<00:08,  2.99it/s]Capturing batches (bs=160 avail_mem=42.19 GB):  54%|    | 28/52 [00:27<00:08,  2.99it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:40 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.19 GB):  56%|    | 29/52 [00:27<00:07,  3.20it/s]Capturing batches (bs=152 avail_mem=42.18 GB):  56%|    | 29/52 [00:27<00:07,  3.20it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.18 GB):  58%|    | 30/52 [00:28<00:07,  2.95it/s]Capturing batches (bs=144 avail_mem=42.18 GB):  58%|    | 30/52 [00:28<00:07,  2.95it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.18 GB):  60%|    | 31/52 [00:28<00:06,  3.18it/s]Capturing batches (bs=136 avail_mem=42.17 GB):  60%|    | 31/52 [00:28<00:06,  3.18it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:41 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.17 GB):  62%|   | 32/52 [00:28<00:05,  3.36it/s]Capturing batches (bs=128 avail_mem=42.17 GB):  62%|   | 32/52 [00:28<00:05,  3.36it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:42 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:42 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.17 GB):  63%|   | 33/52 [00:29<00:05,  3.47it/s]Capturing batches (bs=120 avail_mem=42.17 GB):  63%|   | 33/52 [00:29<00:05,  3.47it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.17 GB):  65%|   | 34/52 [00:29<00:05,  3.09it/s]Capturing batches (bs=112 avail_mem=42.16 GB):  65%|   | 34/52 [00:29<00:05,  3.09it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:42 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.16 GB):  67%|   | 35/52 [00:29<00:05,  3.25it/s]Capturing batches (bs=104 avail_mem=42.16 GB):  67%|   | 35/52 [00:29<00:05,  3.25it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.16 GB):  69%|   | 36/52 [00:30<00:05,  2.97it/s]Capturing batches (bs=96 avail_mem=42.16 GB):  69%|   | 36/52 [00:30<00:05,  2.97it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.16 GB):  71%|   | 37/52 [00:30<00:04,  3.18it/s]Capturing batches (bs=88 avail_mem=42.15 GB):  71%|   | 37/52 [00:30<00:04,  3.18it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:43 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.15 GB):  73%|  | 38/52 [00:30<00:04,  2.93it/s]Capturing batches (bs=80 avail_mem=42.15 GB):  73%|  | 38/52 [00:30<00:04,  2.93it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.15 GB):  75%|  | 39/52 [00:31<00:04,  3.15it/s]Capturing batches (bs=72 avail_mem=42.14 GB):  75%|  | 39/52 [00:31<00:04,  3.15it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.14 GB):  77%|  | 40/52 [00:31<00:04,  2.92it/s]Capturing batches (bs=64 avail_mem=42.14 GB):  77%|  | 40/52 [00:31<00:04,  2.92it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:53:44 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:44 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.14 GB):  79%|  | 41/52 [00:31<00:03,  3.14it/s]Capturing batches (bs=56 avail_mem=42.13 GB):  79%|  | 41/52 [00:31<00:03,  3.14it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.13 GB):  81%|  | 42/52 [00:32<00:03,  2.90it/s]Capturing batches (bs=48 avail_mem=42.13 GB):  81%|  | 42/52 [00:32<00:03,  2.90it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.13 GB):  83%| | 43/52 [00:32<00:02,  3.12it/s]Capturing batches (bs=40 avail_mem=42.12 GB):  83%| | 43/52 [00:32<00:02,  3.12it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:45 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.12 GB):  85%| | 44/52 [00:32<00:02,  2.91it/s]Capturing batches (bs=32 avail_mem=42.12 GB):  85%| | 44/52 [00:32<00:02,  2.91it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.12 GB):  87%| | 45/52 [00:33<00:02,  3.13it/s]Capturing batches (bs=24 avail_mem=42.11 GB):  87%| | 45/52 [00:33<00:02,  3.13it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.11 GB):  88%| | 46/52 [00:33<00:02,  2.92it/s]Capturing batches (bs=16 avail_mem=42.11 GB):  88%| | 46/52 [00:33<00:02,  2.92it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:53:46 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:46 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.11 GB):  90%| | 47/52 [00:33<00:01,  3.16it/s]Capturing batches (bs=12 avail_mem=42.11 GB):  90%| | 47/52 [00:33<00:01,  3.16it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.11 GB):  92%|| 48/52 [00:34<00:01,  3.37it/s]Capturing batches (bs=8 avail_mem=42.10 GB):  92%|| 48/52 [00:34<00:01,  3.37it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.10 GB):  94%|| 49/52 [00:34<00:00,  3.53it/s]Capturing batches (bs=4 avail_mem=42.10 GB):  94%|| 49/52 [00:34<00:00,  3.53it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.10 GB):  96%|| 50/52 [00:34<00:00,  3.60it/s]Capturing batches (bs=2 avail_mem=42.10 GB):  96%|| 50/52 [00:34<00:00,  3.60it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:47 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.10 GB):  98%|| 51/52 [00:34<00:00,  3.71it/s]Capturing batches (bs=1 avail_mem=42.09 GB):  98%|| 51/52 [00:34<00:00,  3.71it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:48 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:35<00:00,  2.53it/s]Capturing batches (bs=1 avail_mem=42.09 GB): 100%|| 52/52 [00:35<00:00,  1.46it/s]
[2025-10-23 16:53:49 TP0] Registering 6396 cuda graph addresses
[2025-10-23 16:53:49 TP0] Capture cuda graph end. Time elapsed: 36.56 s. mem usage=1.52 GB. avail mem=42.09 GB.
[2025-10-23 16:53:49 TP4] Capture cuda graph end. Time elapsed: 36.42 s. mem usage=1.52 GB. avail mem=41.72 GB.
[2025-10-23 16:53:49 TP7] Capture cuda graph end. Time elapsed: 35.22 s. mem usage=1.52 GB. avail mem=41.80 GB.
[2025-10-23 16:53:49 TP5] Capture cuda graph end. Time elapsed: 35.15 s. mem usage=1.52 GB. avail mem=41.81 GB.
[2025-10-23 16:53:49 TP2] Capture cuda graph end. Time elapsed: 36.77 s. mem usage=1.52 GB. avail mem=41.66 GB.
[2025-10-23 16:53:49 TP1] Capture cuda graph end. Time elapsed: 36.36 s. mem usage=1.52 GB. avail mem=41.67 GB.
[2025-10-23 16:53:49 TP3] Capture cuda graph end. Time elapsed: 36.53 s. mem usage=1.52 GB. avail mem=41.68 GB.
[2025-10-23 16:53:49 TP6] Capture cuda graph end. Time elapsed: 36.18 s. mem usage=1.52 GB. avail mem=41.79 GB.
[2025-10-23 16:53:50 TP0] max_total_num_tokens=971748, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.09 GB
[2025-10-23 16:53:50] INFO:     Started server process [51]
[2025-10-23 16:53:50] INFO:     Waiting for application startup.
[2025-10-23 16:53:50] INFO:     Application startup complete.
[2025-10-23 16:53:50] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-23 16:53:51] INFO:     127.0.0.1:42446 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:53:51 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:52 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:53:53] INFO:     127.0.0.1:42464 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:53:54] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:53:54] The server is fired up and ready to roll!
[2025-10-23 16:54:00] INFO:     127.0.0.1:60238 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:54:00 TP0] Prefill batch. #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:00 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:01 TP0] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP0] Prefill batch. #new-seq: 23, #new-token: 1388, #cached-token: 15341, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:01 TP0] Prefill batch. #new-seq: 147, #new-token: 8914, #cached-token: 98345, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[2025-10-23 16:54:01 TP0] Prefill batch. #new-seq: 130, #new-token: 7638, #cached-token: 86996, token usage: 0.01, #running-req: 171, #queue-req: 0, 
[2025-10-23 16:54:04 TP0] Prefill batch. #new-seq: 277, #new-token: 16364, #cached-token: 185461, token usage: 0.02, #running-req: 301, #queue-req: 415, 
[2025-10-23 16:54:06 TP0] Prefill batch. #new-seq: 273, #new-token: 16368, #cached-token: 182829, token usage: 0.04, #running-req: 578, #queue-req: 468, 
[2025-10-23 16:54:07 TP0] Prefill batch. #new-seq: 173, #new-token: 10693, #cached-token: 115882, token usage: 0.05, #running-req: 851, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-23 16:54:08 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:08 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-23 16:54:11] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP0] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:11 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:12] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP0] Decode batch. #running-req: 1024, #token: 95343, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1461.23, #queue-req: 294, 
[2025-10-23 16:54:12 TP0] Prefill batch. #new-seq: 2, #new-token: 107, #cached-token: 1340, token usage: 0.10, #running-req: 1022, #queue-req: 292, 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP2] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP4] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP6] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP0] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP5] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP1] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP3] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP7] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:12 TP0] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:12 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13 TP0] Prefill batch. #new-seq: 2, #new-token: 178, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP4] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP2] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP6] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP5] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP0] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP3] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP1] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP7] [fused_moe] using default for (178, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP0] Prefill batch. #new-seq: 9, #new-token: 645, #cached-token: 6030, token usage: 0.11, #running-req: 1015, #queue-req: 280, 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP2] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP0] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP6] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP4] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP1] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP3] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP5] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP7] [fused_moe] using default for (645, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:13] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:13 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] Prefill batch. #new-seq: 5, #new-token: 295, #cached-token: 3352, token usage: 0.11, #running-req: 1019, #queue-req: 275, 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (295, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] Prefill batch. #new-seq: 10, #new-token: 537, #cached-token: 6701, token usage: 0.11, #running-req: 1014, #queue-req: 265, 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14 TP0] Prefill batch. #new-seq: 5, #new-token: 248, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 260, 
[2025-10-23 16:54:14] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14 TP0] Prefill batch. #new-seq: 5, #new-token: 249, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 255, 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (249, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14 TP0] Prefill batch. #new-seq: 2, #new-token: 97, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 253, 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (97, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:14] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:14 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] Prefill batch. #new-seq: 12, #new-token: 755, #cached-token: 8038, token usage: 0.11, #running-req: 1012, #queue-req: 241, 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15 TP0] Prefill batch. #new-seq: 9, #new-token: 627, #cached-token: 6031, token usage: 0.11, #running-req: 1015, #queue-req: 232, 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15 TP0] Prefill batch. #new-seq: 5, #new-token: 318, #cached-token: 3353, token usage: 0.11, #running-req: 1019, #queue-req: 227, 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP2] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP1] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP4] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP5] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP6] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP3] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP7] [fused_moe] using default for (318, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:36156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:15] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:15 TP0] Prefill batch. #new-seq: 13, #new-token: 798, #cached-token: 8707, token usage: 0.11, #running-req: 1011, #queue-req: 214, 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16 TP0] Prefill batch. #new-seq: 9, #new-token: 416, #cached-token: 6027, token usage: 0.12, #running-req: 1015, #queue-req: 205, 
[2025-10-23 16:54:16] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16 TP0] Prefill batch. #new-seq: 9, #new-token: 543, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 196, 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] Prefill batch. #new-seq: 7, #new-token: 391, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 189, 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (391, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16 TP0] Prefill batch. #new-seq: 7, #new-token: 406, #cached-token: 4689, token usage: 0.12, #running-req: 1017, #queue-req: 182, 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (406, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:16] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:16 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] Prefill batch. #new-seq: 11, #new-token: 673, #cached-token: 7370, token usage: 0.12, #running-req: 1013, #queue-req: 171, 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:38996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17 TP0] Prefill batch. #new-seq: 10, #new-token: 443, #cached-token: 6699, token usage: 0.12, #running-req: 1014, #queue-req: 161, 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (443, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:35268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17 TP0] Prefill batch. #new-seq: 7, #new-token: 381, #cached-token: 4687, token usage: 0.12, #running-req: 1017, #queue-req: 154, 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (381, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] Prefill batch. #new-seq: 8, #new-token: 525, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 146, 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:17 TP0] Prefill batch. #new-seq: 9, #new-token: 578, #cached-token: 6034, token usage: 0.12, #running-req: 1015, #queue-req: 137, 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:17 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18 TP0] Prefill batch. #new-seq: 7, #new-token: 370, #cached-token: 4689, token usage: 0.12, #running-req: 1017, #queue-req: 130, 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP2] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP6] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP4] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP1] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP5] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP3] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP7] [fused_moe] using default for (370, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18 TP0] Prefill batch. #new-seq: 9, #new-token: 605, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 121, 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] Prefill batch. #new-seq: 6, #new-token: 433, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 115, 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP2] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP6] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP5] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP4] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP1] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP3] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP7] [fused_moe] using default for (433, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18 TP0] Prefill batch. #new-seq: 12, #new-token: 837, #cached-token: 8043, token usage: 0.12, #running-req: 1012, #queue-req: 103, 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP2] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP0] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP1] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP4] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP5] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP6] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP3] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18 TP7] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:18] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:18] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19 TP0] Prefill batch. #new-seq: 11, #new-token: 671, #cached-token: 7373, token usage: 0.12, #running-req: 1013, #queue-req: 92, 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] Prefill batch. #new-seq: 14, #new-token: 775, #cached-token: 9378, token usage: 0.12, #running-req: 1010, #queue-req: 78, 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19 TP0] Prefill batch. #new-seq: 5, #new-token: 287, #cached-token: 3348, token usage: 0.12, #running-req: 1019, #queue-req: 73, 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] Prefill batch. #new-seq: 18, #new-token: 1289, #cached-token: 12055, token usage: 0.12, #running-req: 1006, #queue-req: 55, 
[2025-10-23 16:54:19] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:38102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:39728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:19] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:19 TP0] Prefill batch. #new-seq: 17, #new-token: 1043, #cached-token: 11387, token usage: 0.12, #running-req: 1007, #queue-req: 38, 
[2025-10-23 16:54:20] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:37630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20 TP0] Prefill batch. #new-seq: 10, #new-token: 588, #cached-token: 6702, token usage: 0.12, #running-req: 1014, #queue-req: 28, 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP5] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP1] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP4] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP2] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP6] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP3] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP7] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:37376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20 TP0] Prefill batch. #new-seq: 14, #new-token: 861, #cached-token: 9378, token usage: 0.12, #running-req: 1010, #queue-req: 14, 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP2] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP4] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP6] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP5] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP1] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP3] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP7] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20 TP0] Prefill batch. #new-seq: 10, #new-token: 583, #cached-token: 6700, token usage: 0.13, #running-req: 1014, #queue-req: 4, 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP2] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP6] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP4] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP1] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP5] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP3] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP7] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] Decode batch. #running-req: 1014, #token: 120681, token usage: 0.12, cuda graph: False, gen throughput (token/s): 4891.78, #queue-req: 4, 
[2025-10-23 16:54:20] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20 TP0] Prefill batch. #new-seq: 4, #new-token: 210, #cached-token: 2684, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP2] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP6] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP5] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP1] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP4] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP3] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP7] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:20] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:20 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:35182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:34936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:35142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:33842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:21] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:21 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (918, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:35950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:22] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:22 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (819, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:34082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (810, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:23] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP4] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP6] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP2] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP5] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP0] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP1] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP7] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:23 TP3] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:42150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:37690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:24] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP4] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP5] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP2] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP0] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP6] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP3] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP1] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:24 TP7] [fused_moe] using default for (683, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] Decode batch. #running-req: 619, #token: 93238, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6386.51, #queue-req: 0, 
[2025-10-23 16:54:25] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:25] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP4] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP0] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP5] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP1] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP2] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP6] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP3] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:25 TP7] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP4] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP2] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP6] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP5] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP0] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP1] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP3] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26 TP7] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:26] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:26] INFO:     127.0.0.1:42920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:27] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28 TP0] Decode batch. #running-req: 281, #token: 52513, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5949.87, #queue-req: 0, 
[2025-10-23 16:54:28] INFO:     127.0.0.1:38264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:35222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:28] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:29] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:33748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:38986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:34730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:37612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:40208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30 TP0] Decode batch. #running-req: 118, #token: 27555, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3893.78, #queue-req: 0, 
[2025-10-23 16:54:30] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:40800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:30] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:35796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:31] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32 TP0] Decode batch. #running-req: 32, #token: 9228, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1784.52, #queue-req: 0, 
[2025-10-23 16:54:32] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:42214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:41232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:43374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:42858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:32] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33 TP0] Decode batch. #running-req: 7, #token: 2984, token usage: 0.00, cuda graph: True, gen throughput (token/s): 708.23, #queue-req: 0, 
[2025-10-23 16:54:33] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:42292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:33 TP0] Decode batch. #running-req: 2, #token: 1416, token usage: 0.00, cuda graph: True, gen throughput (token/s): 198.64, #queue-req: 0, 
[2025-10-23 16:54:34] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:34 TP0] Decode batch. #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.04, #queue-req: 0, 
[2025-10-23 16:54:35 TP0] Decode batch. #running-req: 1, #token: 1144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.54, #queue-req: 0, 
[2025-10-23 16:54:36 TP0] Decode batch. #running-req: 1, #token: 1184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.51, #queue-req: 0, 
[2025-10-23 16:54:37 TP0] Decode batch. #running-req: 1, #token: 1224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.50, #queue-req: 0, 
[2025-10-23 16:54:37] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:38 TP0] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.48, #queue-req: 0, 
[2025-10-23 16:54:50] INFO:     127.0.0.1:35324 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:54:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:54:51] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 39, #new-token: 39, #cached-token: 28350, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 44, #new-token: 44, #cached-token: 31927, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37286, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37239, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 41509, token usage: 0.02, #running-req: 186, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 42094, token usage: 0.02, #running-req: 243, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:51 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 46652, token usage: 0.02, #running-req: 301, #queue-req: 0, 
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:51 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 45984, token usage: 0.03, #running-req: 365, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 54471, token usage: 0.03, #running-req: 428, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 64, #new-token: 64, #cached-token: 46265, token usage: 0.04, #running-req: 503, #queue-req: 0, 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 79, #new-token: 79, #cached-token: 57553, token usage: 0.04, #running-req: 567, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 49489, token usage: 0.04, #running-req: 646, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 84, #new-token: 84, #cached-token: 61227, token usage: 0.05, #running-req: 714, #queue-req: 0, 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (84, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] Prefill batch. #new-seq: 73, #new-token: 73, #cached-token: 53242, token usage: 0.05, #running-req: 798, #queue-req: 0, 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP2] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP4] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP7] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP6] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP5] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP0] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP3] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:52 TP1] [fused_moe] using default for (73, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP0] Prefill batch. #new-seq: 94, #new-token: 94, #cached-token: 68554, token usage: 0.06, #running-req: 871, #queue-req: 0, 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP2] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP1] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP3] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP0] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP7] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP5] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP4] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP6] [fused_moe] using default for (94, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP0] Prefill batch. #new-seq: 59, #new-token: 59, #cached-token: 43294, token usage: 0.06, #running-req: 965, #queue-req: 12, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:53 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:56] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:56 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 16:54:57] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:57 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 749, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 16:54:57] INFO:     127.0.0.1:38108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:57 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 779, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 16:54:58] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP0] Decode batch. #running-req: 1024, #token: 103128, token usage: 0.11, cuda graph: False, gen throughput (token/s): 1988.11, #queue-req: 292, 
[2025-10-23 16:54:58] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:39688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2162, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6645, token usage: 0.11, #running-req: 1015, #queue-req: 280, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:58] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:58 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2875, token usage: 0.11, #running-req: 1020, #queue-req: 276, 
[2025-10-23 16:54:59] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:37634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:39956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5099, token usage: 0.11, #running-req: 1017, #queue-req: 269, 
[2025-10-23 16:54:59] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4401, token usage: 0.11, #running-req: 1018, #queue-req: 263, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:54:59] INFO:     127.0.0.1:38320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:38916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2866, token usage: 0.11, #running-req: 1020, #queue-req: 259, 
[2025-10-23 16:54:59] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1438, token usage: 0.11, #running-req: 1022, #queue-req: 257, 
[2025-10-23 16:54:59] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:54:59] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7321, token usage: 0.11, #running-req: 1014, #queue-req: 247, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:00] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5934, token usage: 0.11, #running-req: 1016, #queue-req: 239, 
[2025-10-23 16:55:00] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4355, token usage: 0.11, #running-req: 1018, #queue-req: 233, 
[2025-10-23 16:55:00] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5885, token usage: 0.12, #running-req: 1016, #queue-req: 225, 
[2025-10-23 16:55:00] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:43142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8704, token usage: 0.12, #running-req: 1012, #queue-req: 213, 
[2025-10-23 16:55:00] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:36446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:00] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5710, token usage: 0.12, #running-req: 1016, #queue-req: 205, 
[2025-10-23 16:55:01] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7284, token usage: 0.12, #running-req: 1014, #queue-req: 195, 
[2025-10-23 16:55:01] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4352, token usage: 0.12, #running-req: 1018, #queue-req: 189, 
[2025-10-23 16:55:01] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:43470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5129, token usage: 0.12, #running-req: 1017, #queue-req: 182, 
[2025-10-23 16:55:01] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:39536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:01 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8693, token usage: 0.12, #running-req: 1012, #queue-req: 170, 
[2025-10-23 16:55:02] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5696, token usage: 0.12, #running-req: 1016, #queue-req: 162, 
[2025-10-23 16:55:02] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4358, token usage: 0.12, #running-req: 1018, #queue-req: 156, 
[2025-10-23 16:55:02] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5109, token usage: 0.12, #running-req: 1017, #queue-req: 149, 
[2025-10-23 16:55:02] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9516, token usage: 0.12, #running-req: 1011, #queue-req: 136, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:02] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:36680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:40670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:02 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9546, token usage: 0.12, #running-req: 1011, #queue-req: 123, 
[2025-10-23 16:55:03] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:41986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3707, token usage: 0.12, #running-req: 1019, #queue-req: 118, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6639, token usage: 0.12, #running-req: 1015, #queue-req: 109, 
[2025-10-23 16:55:03] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:41930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9549, token usage: 0.12, #running-req: 1011, #queue-req: 96, 
[2025-10-23 16:55:03] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:42288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10102, token usage: 0.12, #running-req: 1010, #queue-req: 82, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:03] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:03] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6544, token usage: 0.12, #running-req: 1015, #queue-req: 73, 
[2025-10-23 16:55:04] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:37266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:37890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11814, token usage: 0.12, #running-req: 1008, #queue-req: 57, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:55:04] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11038, token usage: 0.12, #running-req: 1009, #queue-req: 42, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:04] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6545, token usage: 0.13, #running-req: 1015, #queue-req: 33, 
[2025-10-23 16:55:04] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:37328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10189, token usage: 0.13, #running-req: 1010, #queue-req: 19, 
[2025-10-23 16:55:04] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:04] INFO:     127.0.0.1:41984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8053, token usage: 0.13, #running-req: 1013, #queue-req: 8, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5768, token usage: 0.13, #running-req: 1008, #queue-req: 0, 
[2025-10-23 16:55:05] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:38344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP5] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP4] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP7] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP6] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP1] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP0] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP3] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP2] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:40964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:05] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:05 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] Decode batch. #running-req: 963, #token: 119074, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5226.64, #queue-req: 0, 
[2025-10-23 16:55:06] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (916, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:06] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP5] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP1] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP3] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP2] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP0] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP4] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP6] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:06 TP7] [fused_moe] using default for (888, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:07] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:07 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:45096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:35508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:40796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:08] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:08 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (655, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:41850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:09] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP3] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP1] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP4] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP2] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP6] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP5] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP0] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:09 TP7] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP3] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP5] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP2] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP6] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP4] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP7] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP0] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP1] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:39264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:35726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP4] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP6] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP5] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP7] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP1] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP2] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP0] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP3] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:37530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:10] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10 TP0] Decode batch. #running-req: 523, #token: 80958, token usage: 0.08, cuda graph: False, gen throughput (token/s): 6604.82, #queue-req: 0, 
[2025-10-23 16:55:10] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:40792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:41204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:10] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:37664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:44734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:11] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:38028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:37516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:47002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:39674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:12] INFO:     127.0.0.1:44900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:35634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13 TP0] Decode batch. #running-req: 221, #token: 43558, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5691.53, #queue-req: 0, 
[2025-10-23 16:55:13] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:36972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:41588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:13] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:42504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:39328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:35620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:43482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:14] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:36172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:44584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15 TP0] Decode batch. #running-req: 80, #token: 18800, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3263.41, #queue-req: 0, 
[2025-10-23 16:55:15] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:36830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:41448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:45266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:15] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:41570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16 TP0] Decode batch. #running-req: 27, #token: 8169, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1389.46, #queue-req: 0, 
[2025-10-23 16:55:16] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:16] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:38826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:42066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17 TP0] Decode batch. #running-req: 8, #token: 3416, token usage: 0.00, cuda graph: True, gen throughput (token/s): 686.99, #queue-req: 0, 
[2025-10-23 16:55:17] INFO:     127.0.0.1:45914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:17] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:18] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:18 TP0] Decode batch. #running-req: 5, #token: 2718, token usage: 0.00, cuda graph: True, gen throughput (token/s): 293.41, #queue-req: 0, 
[2025-10-23 16:55:18] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:18] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:19 TP0] Decode batch. #running-req: 3, #token: 2054, token usage: 0.00, cuda graph: True, gen throughput (token/s): 178.52, #queue-req: 0, 
[2025-10-23 16:55:19 TP0] Decode batch. #running-req: 3, #token: 2174, token usage: 0.00, cuda graph: True, gen throughput (token/s): 147.13, #queue-req: 0, 
[2025-10-23 16:55:20] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:20 TP0] Decode batch. #running-req: 3, #token: 1733, token usage: 0.00, cuda graph: True, gen throughput (token/s): 146.74, #queue-req: 0, 
[2025-10-23 16:55:21 TP0] Decode batch. #running-req: 2, #token: 1813, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.23, #queue-req: 0, 
[2025-10-23 16:55:21] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:22] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:34] INFO:     127.0.0.1:49642 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:55:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:55:35] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 40, #new-token: 40, #cached-token: 29038, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 45, #new-token: 45, #cached-token: 32696, token usage: 0.01, #running-req: 41, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37192, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38809, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 45718, token usage: 0.02, #running-req: 190, #queue-req: 0, 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44380, token usage: 0.02, #running-req: 253, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 48066, token usage: 0.02, #running-req: 314, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:35 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 49024, token usage: 0.03, #running-req: 380, #queue-req: 0, 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 74, #new-token: 74, #cached-token: 53539, token usage: 0.03, #running-req: 447, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 74, #new-token: 74, #cached-token: 53861, token usage: 0.04, #running-req: 521, #queue-req: 0, 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 85, #new-token: 85, #cached-token: 61756, token usage: 0.04, #running-req: 595, #queue-req: 0, 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP4] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP5] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP7] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP6] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP2] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP1] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP3] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 54602, token usage: 0.05, #running-req: 680, #queue-req: 0, 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 90, #new-token: 90, #cached-token: 65797, token usage: 0.05, #running-req: 755, #queue-req: 0, 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP2] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP1] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP3] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP4] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP7] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP5] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP6] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] Prefill batch. #new-seq: 30, #new-token: 30, #cached-token: 21663, token usage: 0.05, #running-req: 845, #queue-req: 0, 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP1] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP3] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP2] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP0] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP4] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP7] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP5] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:36 TP6] [fused_moe] using default for (30, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP0] Prefill batch. #new-seq: 21, #new-token: 21, #cached-token: 15266, token usage: 0.06, #running-req: 875, #queue-req: 0, 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP4] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP1] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP3] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP0] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP5] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP2] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP7] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP6] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP0] Prefill batch. #new-seq: 48, #new-token: 48, #cached-token: 35049, token usage: 0.06, #running-req: 896, #queue-req: 0, 
[2025-10-23 16:55:37 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36589, token usage: 0.06, #running-req: 944, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:37 TP0] Prefill batch. #new-seq: 30, #new-token: 30, #cached-token: 22242, token usage: 0.06, #running-req: 994, #queue-req: 30, 
[2025-10-23 16:55:38 TP0] Decode batch. #running-req: 1024, #token: 71080, token usage: 0.07, cuda graph: False, gen throughput (token/s): 485.99, #queue-req: 295, 
[2025-10-23 16:55:40] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 16:55:41] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:41 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 16:55:41] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:41 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 16:55:42] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1433, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[2025-10-23 16:55:42] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:52076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8851, token usage: 0.11, #running-req: 1012, #queue-req: 278, 
[2025-10-23 16:55:42] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:42] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2190, token usage: 0.11, #running-req: 1021, #queue-req: 275, 
[2025-10-23 16:55:43] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7228, token usage: 0.11, #running-req: 1014, #queue-req: 265, 
[2025-10-23 16:55:43] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3593, token usage: 0.11, #running-req: 1019, #queue-req: 260, 
[2025-10-23 16:55:43] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2879, token usage: 0.11, #running-req: 1020, #queue-req: 256, 
[2025-10-23 16:55:43] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:43 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1448, token usage: 0.11, #running-req: 1022, #queue-req: 254, 
[2025-10-23 16:55:44] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5841, token usage: 0.11, #running-req: 1016, #queue-req: 246, 
[2025-10-23 16:55:44 TP0] Decode batch. #running-req: 1016, #token: 108338, token usage: 0.11, cuda graph: False, gen throughput (token/s): 7181.63, #queue-req: 246, 
[2025-10-23 16:55:44] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9509, token usage: 0.11, #running-req: 1011, #queue-req: 233, 
[2025-10-23 16:55:44] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4448, token usage: 0.11, #running-req: 1018, #queue-req: 227, 
[2025-10-23 16:55:44] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8750, token usage: 0.11, #running-req: 1012, #queue-req: 215, 
[2025-10-23 16:55:44] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:44] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5760, token usage: 0.12, #running-req: 1016, #queue-req: 207, 
[2025-10-23 16:55:45] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 198, 
[2025-10-23 16:55:45] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6562, token usage: 0.12, #running-req: 1015, #queue-req: 189, 
[2025-10-23 16:55:45] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4356, token usage: 0.12, #running-req: 1018, #queue-req: 183, 
[2025-10-23 16:55:45] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7980, token usage: 0.12, #running-req: 1013, #queue-req: 172, 
[2025-10-23 16:55:45] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:45] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6434, token usage: 0.12, #running-req: 1015, #queue-req: 163, 
[2025-10-23 16:55:46] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:54678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4391, token usage: 0.12, #running-req: 1018, #queue-req: 157, 
[2025-10-23 16:55:46] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5069, token usage: 0.12, #running-req: 1017, #queue-req: 150, 
[2025-10-23 16:55:46] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5157, token usage: 0.12, #running-req: 1017, #queue-req: 143, 
[2025-10-23 16:55:46] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:46 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5879, token usage: 0.12, #running-req: 1016, #queue-req: 135, 
[2025-10-23 16:55:46] INFO:     127.0.0.1:49812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10938, token usage: 0.12, #running-req: 1009, #queue-req: 120, 
[2025-10-23 16:55:47] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4437, token usage: 0.12, #running-req: 1018, #queue-req: 114, 
[2025-10-23 16:55:47] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7349, token usage: 0.12, #running-req: 1014, #queue-req: 104, 
[2025-10-23 16:55:47] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5928, token usage: 0.12, #running-req: 1016, #queue-req: 96, 
[2025-10-23 16:55:47] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:47 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12273, token usage: 0.12, #running-req: 1007, #queue-req: 79, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:47 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:48] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5846, token usage: 0.12, #running-req: 1016, #queue-req: 71, 
[2025-10-23 16:55:48] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10332, token usage: 0.12, #running-req: 1010, #queue-req: 57, 
[2025-10-23 16:55:48] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11030, token usage: 0.13, #running-req: 1009, #queue-req: 42, 
[2025-10-23 16:55:48] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5115, token usage: 0.13, #running-req: 1017, #queue-req: 35, 
[2025-10-23 16:55:48] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:48] INFO:     127.0.0.1:58776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6492, token usage: 0.13, #running-req: 1015, #queue-req: 26, 
[2025-10-23 16:55:49] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9524, token usage: 0.13, #running-req: 1011, #queue-req: 13, 
[2025-10-23 16:55:49] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9445, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[2025-10-23 16:55:49] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:49] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP2] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP4] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP6] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP0] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP5] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP1] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP3] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:49 TP7] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:50] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP4] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP2] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP6] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP0] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP5] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP1] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP3] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:50 TP7] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (882, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] Decode batch. #running-req: 873, #token: 113654, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5450.73, #queue-req: 0, 
[2025-10-23 16:55:51] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (847, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:51] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP4] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP6] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP2] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP5] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP0] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP1] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP3] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:51 TP7] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:52162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (751, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:52] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP4] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP0] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP5] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP2] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP6] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP1] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP3] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:52 TP7] [fused_moe] using default for (728, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:60668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (661, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP0] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP4] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP2] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP1] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP5] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP6] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP3] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53 TP7] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:53] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:53] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:52414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:54164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:54] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP0] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP4] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP1] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP5] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP3] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP7] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP2] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:54 TP6] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP4] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP1] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP5] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP3] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP7] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP0] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP2] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55 TP6] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:55:55] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:55] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56 TP0] Decode batch. #running-req: 458, #token: 74585, token usage: 0.08, cuda graph: True, gen throughput (token/s): 5654.22, #queue-req: 0, 
[2025-10-23 16:55:56] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:56] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:52182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:58502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:57] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58 TP0] Decode batch. #running-req: 194, #token: 39514, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5295.24, #queue-req: 0, 
[2025-10-23 16:55:58] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:58] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:55:59] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00 TP0] Decode batch. #running-req: 64, #token: 16915, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2872.55, #queue-req: 0, 
[2025-10-23 16:56:00] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:00] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:32854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01 TP0] Decode batch. #running-req: 23, #token: 7542, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1228.59, #queue-req: 0, 
[2025-10-23 16:56:01] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:01] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:59324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02 TP0] Decode batch. #running-req: 7, #token: 2403, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.59, #queue-req: 0, 
[2025-10-23 16:56:02] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:02] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:03 TP0] Decode batch. #running-req: 2, #token: 1529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 172.21, #queue-req: 0, 
[2025-10-23 16:56:03 TP0] Decode batch. #running-req: 2, #token: 1609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.13, #queue-req: 0, 
[2025-10-23 16:56:04 TP0] Decode batch. #running-req: 2, #token: 1689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.10, #queue-req: 0, 
[2025-10-23 16:56:05] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:05 TP0] Decode batch. #running-req: 1, #token: 1200, token usage: 0.00, cuda graph: True, gen throughput (token/s): 94.39, #queue-req: 0, 
[2025-10-23 16:56:06 TP0] Decode batch. #running-req: 1, #token: 1240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.45, #queue-req: 0, 
[2025-10-23 16:56:06] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:19] INFO:     127.0.0.1:42686 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:56:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:56:19] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 39, #new-token: 39, #cached-token: 28279, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33416, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 48, #new-token: 48, #cached-token: 35021, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 41057, token usage: 0.01, #running-req: 134, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39212, token usage: 0.02, #running-req: 190, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 45767, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 42305, token usage: 0.02, #running-req: 307, #queue-req: 0, 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 70, #new-token: 70, #cached-token: 51099, token usage: 0.03, #running-req: 365, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:20 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 51443, token usage: 0.03, #running-req: 435, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 50027, token usage: 0.04, #running-req: 506, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 76, #new-token: 76, #cached-token: 55369, token usage: 0.04, #running-req: 575, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5819, token usage: 0.04, #running-req: 651, #queue-req: 0, 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 24, #new-token: 24, #cached-token: 17429, token usage: 0.04, #running-req: 659, #queue-req: 0, 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33546, token usage: 0.04, #running-req: 683, #queue-req: 0, 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36352, token usage: 0.05, #running-req: 729, #queue-req: 0, 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45484, token usage: 0.05, #running-req: 779, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:21 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39052, token usage: 0.06, #running-req: 841, #queue-req: 0, 
[2025-10-23 16:56:22 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 50428, token usage: 0.06, #running-req: 895, #queue-req: 0, 
[2025-10-23 16:56:22 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 44148, token usage: 0.06, #running-req: 964, #queue-req: 0, 
[2025-10-23 16:56:24 TP0] Decode batch. #running-req: 1024, #token: 81286, token usage: 0.08, cuda graph: False, gen throughput (token/s): 1006.49, #queue-req: 295, 
[2025-10-23 16:56:25] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 759, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 16:56:26] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 746, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 16:56:26] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 766, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 16:56:27] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1485, token usage: 0.11, #running-req: 1022, #queue-req: 290, 
[2025-10-23 16:56:27] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5789, token usage: 0.11, #running-req: 1016, #queue-req: 282, 
[2025-10-23 16:56:27] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:27] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2917, token usage: 0.11, #running-req: 1020, #queue-req: 278, 
[2025-10-23 16:56:28] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5795, token usage: 0.11, #running-req: 1016, #queue-req: 270, 
[2025-10-23 16:56:28] INFO:     127.0.0.1:44322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:51554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2863, token usage: 0.11, #running-req: 1020, #queue-req: 266, 
[2025-10-23 16:56:28] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4353, token usage: 0.11, #running-req: 1018, #queue-req: 260, 
[2025-10-23 16:56:28] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3594, token usage: 0.11, #running-req: 1019, #queue-req: 255, 
[2025-10-23 16:56:28] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:46154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:28] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6574, token usage: 0.11, #running-req: 1015, #queue-req: 246, 
[2025-10-23 16:56:29] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:50702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6650, token usage: 0.11, #running-req: 1015, #queue-req: 237, 
[2025-10-23 16:56:29] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3641, token usage: 0.11, #running-req: 1019, #queue-req: 232, 
[2025-10-23 16:56:29] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:44948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8088, token usage: 0.12, #running-req: 1013, #queue-req: 221, 
[2025-10-23 16:56:29] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:48980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6503, token usage: 0.12, #running-req: 1015, #queue-req: 212, 
[2025-10-23 16:56:30] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6442, token usage: 0.12, #running-req: 1015, #queue-req: 203, 
[2025-10-23 16:56:30] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5123, token usage: 0.12, #running-req: 1017, #queue-req: 196, 
[2025-10-23 16:56:30] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5819, token usage: 0.12, #running-req: 1016, #queue-req: 188, 
[2025-10-23 16:56:30] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7987, token usage: 0.12, #running-req: 1013, #queue-req: 177, 
[2025-10-23 16:56:30] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:30] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6491, token usage: 0.12, #running-req: 1015, #queue-req: 168, 
[2025-10-23 16:56:31] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4299, token usage: 0.12, #running-req: 1018, #queue-req: 162, 
[2025-10-23 16:56:31 TP0] Decode batch. #running-req: 1018, #token: 115599, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6185.42, #queue-req: 162, 
[2025-10-23 16:56:31] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4352, token usage: 0.12, #running-req: 1018, #queue-req: 156, 
[2025-10-23 16:56:31] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5109, token usage: 0.12, #running-req: 1017, #queue-req: 149, 
[2025-10-23 16:56:31] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5948, token usage: 0.12, #running-req: 1016, #queue-req: 141, 
[2025-10-23 16:56:31] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:31] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8629, token usage: 0.12, #running-req: 1012, #queue-req: 129, 
[2025-10-23 16:56:32] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:47872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7488, token usage: 0.12, #running-req: 1014, #queue-req: 119, 
[2025-10-23 16:56:32] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5077, token usage: 0.12, #running-req: 1017, #queue-req: 112, 
[2025-10-23 16:56:32] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8161, token usage: 0.12, #running-req: 1013, #queue-req: 101, 
[2025-10-23 16:56:32] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:51100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10151, token usage: 0.12, #running-req: 1010, #queue-req: 87, 
[2025-10-23 16:56:32] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:32] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4338, token usage: 0.12, #running-req: 1018, #queue-req: 81, 
[2025-10-23 16:56:33] INFO:     127.0.0.1:42908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:43322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12458, token usage: 0.12, #running-req: 1007, #queue-req: 64, 
[2025-10-23 16:56:33] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10407, token usage: 0.13, #running-req: 1010, #queue-req: 50, 
[2025-10-23 16:56:33] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9393, token usage: 0.13, #running-req: 1011, #queue-req: 37, 
[2025-10-23 16:56:33] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:33 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9463, token usage: 0.13, #running-req: 1011, #queue-req: 24, 
[2025-10-23 16:56:34] INFO:     127.0.0.1:42984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8762, token usage: 0.13, #running-req: 1012, #queue-req: 12, 
[2025-10-23 16:56:34] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8735, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[2025-10-23 16:56:34] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP4] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP0] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP2] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP6] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP5] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP1] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP3] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP7] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:45940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:49638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:34] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP0] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP4] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP2] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP6] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP1] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP5] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP3] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:34 TP7] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:42872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:43278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:35] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP4] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP2] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP6] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP0] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP3] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP7] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP1] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:35 TP5] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:44676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP4] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP2] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP6] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP0] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP3] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP7] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP1] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP5] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP4] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP6] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP2] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP0] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP7] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP1] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP5] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP3] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:44034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP4] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP0] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP2] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP6] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP1] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP7] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP3] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36 TP5] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:36] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:36] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37 TP0] Decode batch. #running-req: 775, #token: 108917, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5957.92, #queue-req: 0, 
[2025-10-23 16:56:37] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:49386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:37] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:37] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:44856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:48420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:51854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:38] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:38 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:39] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:39 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:56:40] INFO:     127.0.0.1:45336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:40] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:42970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41 TP0] Decode batch. #running-req: 370, #token: 62425, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6114.82, #queue-req: 0, 
[2025-10-23 16:56:41] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:47856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:41] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:49452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:42] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43 TP0] Decode batch. #running-req: 157, #token: 34808, token usage: 0.04, cuda graph: True, gen throughput (token/s): 4598.07, #queue-req: 0, 
[2025-10-23 16:56:43] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:53530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:43] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:44] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45 TP0] Decode batch. #running-req: 55, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2422.29, #queue-req: 0, 
[2025-10-23 16:56:45] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:45] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46 TP0] Decode batch. #running-req: 20, #token: 6024, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1169.18, #queue-req: 0, 
[2025-10-23 16:56:46] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:53244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:46] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:47] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:47 TP0] Decode batch. #running-req: 4, #token: 2029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 449.92, #queue-req: 0, 
[2025-10-23 16:56:47] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:47] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:47 TP0] Decode batch. #running-req: 2, #token: 1469, token usage: 0.00, cuda graph: True, gen throughput (token/s): 145.94, #queue-req: 0, 
[2025-10-23 16:56:48] INFO:     127.0.0.1:52232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:56:48 TP0] Decode batch. #running-req: 1, #token: 1168, token usage: 0.00, cuda graph: True, gen throughput (token/s): 55.74, #queue-req: 0, 
[2025-10-23 16:56:49 TP0] Decode batch. #running-req: 1, #token: 1208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.46, #queue-req: 0, 
[2025-10-23 16:56:50 TP0] Decode batch. #running-req: 1, #token: 1248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.46, #queue-req: 0, 
[2025-10-23 16:56:51] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:04] INFO:     127.0.0.1:35014 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:57:04] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 41, #new-token: 41, #cached-token: 29667, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33498, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 38045, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38687, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 48711, token usage: 0.02, #running-req: 193, #queue-req: 0, 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39217, token usage: 0.02, #running-req: 260, #queue-req: 0, 
[2025-10-23 16:57:04 TP0] Prefill batch. #new-seq: 32, #new-token: 32, #cached-token: 23317, token usage: 0.02, #running-req: 314, #queue-req: 0, 
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:04 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-23 16:57:05 TP0] Decode batch. #running-req: 346, #token: 21929, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.16, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 21, #new-token: 21, #cached-token: 15346, token usage: 0.02, #running-req: 346, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 73, #new-token: 73, #cached-token: 53277, token usage: 0.03, #running-req: 367, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37088, token usage: 0.03, #running-req: 440, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 85, #new-token: 85, #cached-token: 61544, token usage: 0.04, #running-req: 491, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40798, token usage: 0.04, #running-req: 576, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 94, #new-token: 94, #cached-token: 68383, token usage: 0.05, #running-req: 632, #queue-req: 0, 
[2025-10-23 16:57:05 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 45896, token usage: 0.05, #running-req: 726, #queue-req: 0, 
[2025-10-23 16:57:06 TP0] Prefill batch. #new-seq: 105, #new-token: 105, #cached-token: 76454, token usage: 0.06, #running-req: 789, #queue-req: 0, 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP2] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP7] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP4] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP0] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP3] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP1] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP6] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP5] [fused_moe] using default for (105, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:06 TP0] Prefill batch. #new-seq: 69, #new-token: 69, #cached-token: 50458, token usage: 0.06, #running-req: 894, #queue-req: 0, 
[2025-10-23 16:57:06 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44695, token usage: 0.06, #running-req: 963, #queue-req: 50, 
[2025-10-23 16:57:09] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 742, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-23 16:57:10] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:10] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:10 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-23 16:57:10 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 790, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-23 16:57:11] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 731, token usage: 0.11, #running-req: 1023, #queue-req: 291, 
[2025-10-23 16:57:11 TP0] Decode batch. #running-req: 1023, #token: 101706, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6096.62, #queue-req: 291, 
[2025-10-23 16:57:11] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5180, token usage: 0.11, #running-req: 1017, #queue-req: 284, 
[2025-10-23 16:57:11] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:11] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2930, token usage: 0.11, #running-req: 1020, #queue-req: 280, 
[2025-10-23 16:57:12] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:35720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3733, token usage: 0.11, #running-req: 1019, #queue-req: 275, 
[2025-10-23 16:57:12] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1441, token usage: 0.11, #running-req: 1022, #queue-req: 273, 
[2025-10-23 16:57:12] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4322, token usage: 0.11, #running-req: 1018, #queue-req: 267, 
[2025-10-23 16:57:12] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2896, token usage: 0.11, #running-req: 1020, #queue-req: 263, 
[2025-10-23 16:57:12] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:37442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:12] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7191, token usage: 0.11, #running-req: 1014, #queue-req: 253, 
[2025-10-23 16:57:13] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3686, token usage: 0.11, #running-req: 1019, #queue-req: 248, 
[2025-10-23 16:57:13] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7384, token usage: 0.11, #running-req: 1014, #queue-req: 238, 
[2025-10-23 16:57:13] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5105, token usage: 0.11, #running-req: 1017, #queue-req: 231, 
[2025-10-23 16:57:13] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:13 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2921, token usage: 0.12, #running-req: 1020, #queue-req: 227, 
[2025-10-23 16:57:14] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8795, token usage: 0.12, #running-req: 1012, #queue-req: 215, 
[2025-10-23 16:57:14] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6422, token usage: 0.12, #running-req: 1015, #queue-req: 206, 
[2025-10-23 16:57:14] INFO:     127.0.0.1:36062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3558, token usage: 0.12, #running-req: 1019, #queue-req: 201, 
[2025-10-23 16:57:14] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8060, token usage: 0.12, #running-req: 1013, #queue-req: 190, 
[2025-10-23 16:57:14] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:14 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5815, token usage: 0.12, #running-req: 1016, #queue-req: 182, 
[2025-10-23 16:57:15] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:37202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:37316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10139, token usage: 0.12, #running-req: 1010, #queue-req: 168, 
[2025-10-23 16:57:15] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:40658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4297, token usage: 0.12, #running-req: 1018, #queue-req: 162, 
[2025-10-23 16:57:15] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:38578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5057, token usage: 0.12, #running-req: 1017, #queue-req: 155, 
[2025-10-23 16:57:15] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2948, token usage: 0.12, #running-req: 1020, #queue-req: 151, 
[2025-10-23 16:57:15] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:15] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8102, token usage: 0.12, #running-req: 1013, #queue-req: 140, 
[2025-10-23 16:57:16] INFO:     127.0.0.1:35614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:35792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6510, token usage: 0.12, #running-req: 1015, #queue-req: 131, 
[2025-10-23 16:57:16] INFO:     127.0.0.1:36344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6614, token usage: 0.12, #running-req: 1015, #queue-req: 122, 
[2025-10-23 16:57:16] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8809, token usage: 0.12, #running-req: 1012, #queue-req: 110, 
[2025-10-23 16:57:16] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11093, token usage: 0.12, #running-req: 1009, #queue-req: 95, 
[2025-10-23 16:57:16] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:40518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:16] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7215, token usage: 0.12, #running-req: 1014, #queue-req: 85, 
[2025-10-23 16:57:17] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:37140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:37832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5790, token usage: 0.12, #running-req: 1016, #queue-req: 77, 
[2025-10-23 16:57:17] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:38784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5887, token usage: 0.12, #running-req: 1016, #queue-req: 69, 
[2025-10-23 16:57:17] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:39270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10365, token usage: 0.13, #running-req: 1010, #queue-req: 55, 
[2025-10-23 16:57:17] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:41212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:17 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10953, token usage: 0.13, #running-req: 1009, #queue-req: 40, 
[2025-10-23 16:57:18] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9454, token usage: 0.13, #running-req: 1011, #queue-req: 27, 
[2025-10-23 16:57:18] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9511, token usage: 0.13, #running-req: 1011, #queue-req: 14, 
[2025-10-23 16:57:18] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:35882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5843, token usage: 0.13, #running-req: 1016, #queue-req: 6, 
[2025-10-23 16:57:18] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4320, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[2025-10-23 16:57:18] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:18] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:38954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:34058 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] Decode batch. #running-req: 969, #token: 119990, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5066.90, #queue-req: 0, 
[2025-10-23 16:57:19] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:39642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:19] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP2] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP4] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP6] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP3] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP0] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP7] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP1] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:19 TP5] [fused_moe] using default for (926, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:41228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (863, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:37434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:20] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP4] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP2] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP3] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP0] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP6] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP1] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP7] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:20 TP5] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:37672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:59654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:35808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP2] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP4] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP3] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP0] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP6] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP7] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP1] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP5] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP4] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP2] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP3] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP0] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP6] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP7] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP1] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP5] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:36940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:37688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP2] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP4] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP0] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP6] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP3] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP7] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP1] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21 TP5] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:21] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:21] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22] INFO:     127.0.0.1:36722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:40490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:22] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP4] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP6] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP2] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP0] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP3] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP7] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP1] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:22 TP5] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34414 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP3] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP2] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP4] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP0] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP1] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP7] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP6] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP5] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:23] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP2] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP4] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP1] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP0] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP3] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP6] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP5] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:23 TP7] [fused_moe] using default for (557, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP2] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP4] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP1] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP0] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP6] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP5] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP3] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP7] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-23 16:57:24 TP0] Decode batch. #running-req: 534, #token: 82793, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6414.52, #queue-req: 0, 
[2025-10-23 16:57:24] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:37422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:24] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:59938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:25] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:37154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:41032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26 TP0] Decode batch. #running-req: 226, #token: 44843, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5957.36, #queue-req: 0, 
[2025-10-23 16:57:26] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:26] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:37528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:39338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:27] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:40876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28 TP0] Decode batch. #running-req: 82, #token: 19595, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3290.19, #queue-req: 0, 
[2025-10-23 16:57:28] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:28] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:39746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:36664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:29 TP0] Decode batch. #running-req: 27, #token: 8167, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1501.15, #queue-req: 0, 
[2025-10-23 16:57:30] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:30 TP0] Decode batch. #running-req: 7, #token: 2929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 647.10, #queue-req: 0, 
[2025-10-23 16:57:31] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31] INFO:     127.0.0.1:35694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:31 TP0] Decode batch. #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 154.22, #queue-req: 0, 
[2025-10-23 16:57:31] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:53] INFO:     127.0.0.1:43836 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-23 16:57:59] INFO:     127.0.0.1:33408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:57:59 TP0] Prefill batch. #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:58:01] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:58:01] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:01 TP0] Prefill batch. #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-23 16:58:01 TP0] Prefill batch. #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 16:58:02 TP0] Prefill batch. #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 16:58:04 TP0] Decode batch. #running-req: 16, #token: 51282, token usage: 0.05, cuda graph: True, gen throughput (token/s): 3.55, #queue-req: 0, 
[2025-10-23 16:58:05 TP0] Decode batch. #running-req: 16, #token: 51922, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.74, #queue-req: 0, 
[2025-10-23 16:58:06 TP0] Decode batch. #running-req: 16, #token: 52562, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.91, #queue-req: 0, 
[2025-10-23 16:58:07 TP0] Decode batch. #running-req: 16, #token: 53202, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.11, #queue-req: 0, 
[2025-10-23 16:58:08 TP0] Decode batch. #running-req: 16, #token: 53842, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.02, #queue-req: 0, 
[2025-10-23 16:58:09 TP0] Decode batch. #running-req: 16, #token: 54482, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.51, #queue-req: 0, 
[2025-10-23 16:58:10 TP0] Decode batch. #running-req: 16, #token: 55122, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.90, #queue-req: 0, 
[2025-10-23 16:58:11 TP0] Decode batch. #running-req: 16, #token: 55762, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.53, #queue-req: 0, 
[2025-10-23 16:58:12 TP0] Decode batch. #running-req: 16, #token: 56402, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.06, #queue-req: 0, 
[2025-10-23 16:58:13 TP0] Decode batch. #running-req: 16, #token: 57042, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.81, #queue-req: 0, 
[2025-10-23 16:58:14 TP0] Decode batch. #running-req: 16, #token: 57682, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.41, #queue-req: 0, 
[2025-10-23 16:58:15 TP0] Decode batch. #running-req: 16, #token: 58322, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.98, #queue-req: 0, 
[2025-10-23 16:58:16 TP0] Decode batch. #running-req: 16, #token: 58962, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.96, #queue-req: 0, 
[2025-10-23 16:58:17 TP0] Decode batch. #running-req: 16, #token: 59602, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.95, #queue-req: 0, 
[2025-10-23 16:58:18 TP0] Decode batch. #running-req: 16, #token: 60242, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.07, #queue-req: 0, 
[2025-10-23 16:58:19 TP0] Decode batch. #running-req: 16, #token: 60882, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.90, #queue-req: 0, 
[2025-10-23 16:58:20 TP0] Decode batch. #running-req: 16, #token: 61522, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.35, #queue-req: 0, 
[2025-10-23 16:58:21 TP0] Decode batch. #running-req: 16, #token: 62162, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.24, #queue-req: 0, 
[2025-10-23 16:58:22 TP0] Decode batch. #running-req: 16, #token: 62802, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.31, #queue-req: 0, 
[2025-10-23 16:58:23 TP0] Decode batch. #running-req: 16, #token: 63442, token usage: 0.07, cuda graph: True, gen throughput (token/s): 610.84, #queue-req: 0, 
[2025-10-23 16:58:24] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24 TP0] Prefill batch. #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:58:24] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:24 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.00, #running-req: 1, #queue-req: 6, 
[2025-10-23 16:58:25] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:25] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:25] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:25] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:25 TP0] Prefill batch. #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 16:58:26 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 16:58:27 TP0] Decode batch. #running-req: 16, #token: 51276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.83, #queue-req: 0, 
[2025-10-23 16:58:28 TP0] Decode batch. #running-req: 16, #token: 51916, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.61, #queue-req: 0, 
[2025-10-23 16:58:29 TP0] Decode batch. #running-req: 16, #token: 52556, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.83, #queue-req: 0, 
[2025-10-23 16:58:30 TP0] Decode batch. #running-req: 16, #token: 53196, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.59, #queue-req: 0, 
[2025-10-23 16:58:31 TP0] Decode batch. #running-req: 16, #token: 53836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.87, #queue-req: 0, 
[2025-10-23 16:58:32 TP0] Decode batch. #running-req: 16, #token: 54476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.72, #queue-req: 0, 
[2025-10-23 16:58:33 TP0] Decode batch. #running-req: 16, #token: 55116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.64, #queue-req: 0, 
[2025-10-23 16:58:34 TP0] Decode batch. #running-req: 16, #token: 55756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.54, #queue-req: 0, 
[2025-10-23 16:58:35 TP0] Decode batch. #running-req: 16, #token: 56396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.13, #queue-req: 0, 
[2025-10-23 16:58:37 TP0] Decode batch. #running-req: 16, #token: 57036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.07, #queue-req: 0, 
[2025-10-23 16:58:38 TP0] Decode batch. #running-req: 16, #token: 57676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.03, #queue-req: 0, 
[2025-10-23 16:58:39 TP0] Decode batch. #running-req: 16, #token: 58316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.48, #queue-req: 0, 
[2025-10-23 16:58:40 TP0] Decode batch. #running-req: 16, #token: 58956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.06, #queue-req: 0, 
[2025-10-23 16:58:41 TP0] Decode batch. #running-req: 16, #token: 59596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.82, #queue-req: 0, 
[2025-10-23 16:58:42 TP0] Decode batch. #running-req: 16, #token: 60236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.37, #queue-req: 0, 
[2025-10-23 16:58:43 TP0] Decode batch. #running-req: 16, #token: 60876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.41, #queue-req: 0, 
[2025-10-23 16:58:44 TP0] Decode batch. #running-req: 16, #token: 61516, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.84, #queue-req: 0, 
[2025-10-23 16:58:45 TP0] Decode batch. #running-req: 16, #token: 62156, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.99, #queue-req: 0, 
[2025-10-23 16:58:46 TP0] Decode batch. #running-req: 16, #token: 62796, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.55, #queue-req: 0, 
[2025-10-23 16:58:47 TP0] Decode batch. #running-req: 16, #token: 63436, token usage: 0.07, cuda graph: True, gen throughput (token/s): 610.33, #queue-req: 0, 
[2025-10-23 16:58:48] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48 TP0] Prefill batch. #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:58:48] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:58:48 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 16:58:48 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 16:58:49 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 16:58:51 TP0] Decode batch. #running-req: 16, #token: 51278, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.14, #queue-req: 0, 
[2025-10-23 16:58:52 TP0] Decode batch. #running-req: 16, #token: 51918, token usage: 0.05, cuda graph: True, gen throughput (token/s): 628.06, #queue-req: 0, 
[2025-10-23 16:58:53 TP0] Decode batch. #running-req: 16, #token: 52558, token usage: 0.05, cuda graph: True, gen throughput (token/s): 628.13, #queue-req: 0, 
[2025-10-23 16:58:54 TP0] Decode batch. #running-req: 16, #token: 53198, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.66, #queue-req: 0, 
[2025-10-23 16:58:55 TP0] Decode batch. #running-req: 16, #token: 53838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 622.10, #queue-req: 0, 
[2025-10-23 16:58:56 TP0] Decode batch. #running-req: 16, #token: 54478, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.40, #queue-req: 0, 
[2025-10-23 16:58:57 TP0] Decode batch. #running-req: 16, #token: 55118, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.45, #queue-req: 0, 
[2025-10-23 16:58:58 TP0] Decode batch. #running-req: 16, #token: 55758, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.78, #queue-req: 0, 
[2025-10-23 16:58:59 TP0] Decode batch. #running-req: 16, #token: 56398, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.89, #queue-req: 0, 
[2025-10-23 16:59:00 TP0] Decode batch. #running-req: 16, #token: 57038, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.78, #queue-req: 0, 
[2025-10-23 16:59:01 TP0] Decode batch. #running-req: 16, #token: 57678, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.33, #queue-req: 0, 
[2025-10-23 16:59:02 TP0] Decode batch. #running-req: 16, #token: 58318, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.33, #queue-req: 0, 
[2025-10-23 16:59:03 TP0] Decode batch. #running-req: 16, #token: 58958, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.24, #queue-req: 0, 
[2025-10-23 16:59:04 TP0] Decode batch. #running-req: 16, #token: 59598, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.58, #queue-req: 0, 
[2025-10-23 16:59:05 TP0] Decode batch. #running-req: 16, #token: 60238, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.53, #queue-req: 0, 
[2025-10-23 16:59:06 TP0] Decode batch. #running-req: 16, #token: 60878, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.81, #queue-req: 0, 
[2025-10-23 16:59:07 TP0] Decode batch. #running-req: 16, #token: 61518, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.22, #queue-req: 0, 
[2025-10-23 16:59:08 TP0] Decode batch. #running-req: 16, #token: 62158, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.99, #queue-req: 0, 
[2025-10-23 16:59:09 TP0] Decode batch. #running-req: 16, #token: 62798, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.52, #queue-req: 0, 
[2025-10-23 16:59:10 TP0] Decode batch. #running-req: 16, #token: 63438, token usage: 0.07, cuda graph: True, gen throughput (token/s): 612.06, #queue-req: 0, 
[2025-10-23 16:59:11] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11 TP0] Prefill batch. #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:59:11] INFO:     127.0.0.1:42860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:11] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:12 TP0] Prefill batch. #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 16:59:12 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 16:59:13 TP0] Prefill batch. #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 16:59:14 TP0] Decode batch. #running-req: 16, #token: 51281, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.54, #queue-req: 0, 
[2025-10-23 16:59:15 TP0] Decode batch. #running-req: 16, #token: 51921, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.96, #queue-req: 0, 
[2025-10-23 16:59:16 TP0] Decode batch. #running-req: 16, #token: 52561, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.36, #queue-req: 0, 
[2025-10-23 16:59:17 TP0] Decode batch. #running-req: 16, #token: 53201, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.96, #queue-req: 0, 
[2025-10-23 16:59:18 TP0] Decode batch. #running-req: 16, #token: 53841, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.99, #queue-req: 0, 
[2025-10-23 16:59:19 TP0] Decode batch. #running-req: 16, #token: 54481, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.22, #queue-req: 0, 
[2025-10-23 16:59:20 TP0] Decode batch. #running-req: 16, #token: 55121, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.42, #queue-req: 0, 
[2025-10-23 16:59:21 TP0] Decode batch. #running-req: 16, #token: 55761, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.07, #queue-req: 0, 
[2025-10-23 16:59:23 TP0] Decode batch. #running-req: 16, #token: 56401, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.96, #queue-req: 0, 
[2025-10-23 16:59:24 TP0] Decode batch. #running-req: 16, #token: 57041, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.84, #queue-req: 0, 
[2025-10-23 16:59:25 TP0] Decode batch. #running-req: 16, #token: 57681, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.92, #queue-req: 0, 
[2025-10-23 16:59:26 TP0] Decode batch. #running-req: 16, #token: 58321, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.52, #queue-req: 0, 
[2025-10-23 16:59:27 TP0] Decode batch. #running-req: 16, #token: 58961, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.92, #queue-req: 0, 
[2025-10-23 16:59:28 TP0] Decode batch. #running-req: 16, #token: 59601, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.51, #queue-req: 0, 
[2025-10-23 16:59:29 TP0] Decode batch. #running-req: 16, #token: 60241, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.86, #queue-req: 0, 
[2025-10-23 16:59:30 TP0] Decode batch. #running-req: 16, #token: 60881, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.18, #queue-req: 0, 
[2025-10-23 16:59:31 TP0] Decode batch. #running-req: 16, #token: 61521, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.55, #queue-req: 0, 
[2025-10-23 16:59:32 TP0] Decode batch. #running-req: 16, #token: 62161, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.85, #queue-req: 0, 
[2025-10-23 16:59:33 TP0] Decode batch. #running-req: 16, #token: 62801, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.97, #queue-req: 0, 
[2025-10-23 16:59:34 TP0] Decode batch. #running-req: 16, #token: 63441, token usage: 0.07, cuda graph: True, gen throughput (token/s): 612.74, #queue-req: 0, 
[2025-10-23 16:59:35] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35 TP0] Prefill batch. #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:59:35] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:35 TP0] Prefill batch. #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 16:59:35 TP0] Prefill batch. #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 16:59:36 TP0] Prefill batch. #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 16:59:38 TP0] Decode batch. #running-req: 16, #token: 51276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.19, #queue-req: 0, 
[2025-10-23 16:59:39 TP0] Decode batch. #running-req: 16, #token: 51916, token usage: 0.05, cuda graph: True, gen throughput (token/s): 626.45, #queue-req: 0, 
[2025-10-23 16:59:40 TP0] Decode batch. #running-req: 16, #token: 52556, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.53, #queue-req: 0, 
[2025-10-23 16:59:41 TP0] Decode batch. #running-req: 16, #token: 53196, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.17, #queue-req: 0, 
[2025-10-23 16:59:42 TP0] Decode batch. #running-req: 16, #token: 53836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.57, #queue-req: 0, 
[2025-10-23 16:59:43 TP0] Decode batch. #running-req: 16, #token: 54476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.80, #queue-req: 0, 
[2025-10-23 16:59:44 TP0] Decode batch. #running-req: 16, #token: 55116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.42, #queue-req: 0, 
[2025-10-23 16:59:45 TP0] Decode batch. #running-req: 16, #token: 55756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.32, #queue-req: 0, 
[2025-10-23 16:59:46 TP0] Decode batch. #running-req: 16, #token: 56396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.81, #queue-req: 0, 
[2025-10-23 16:59:47 TP0] Decode batch. #running-req: 16, #token: 57036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.65, #queue-req: 0, 
[2025-10-23 16:59:48 TP0] Decode batch. #running-req: 16, #token: 57676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.00, #queue-req: 0, 
[2025-10-23 16:59:49 TP0] Decode batch. #running-req: 16, #token: 58316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.46, #queue-req: 0, 
[2025-10-23 16:59:50 TP0] Decode batch. #running-req: 16, #token: 58956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.78, #queue-req: 0, 
[2025-10-23 16:59:51 TP0] Decode batch. #running-req: 16, #token: 59596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.67, #queue-req: 0, 
[2025-10-23 16:59:52 TP0] Decode batch. #running-req: 16, #token: 60236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.54, #queue-req: 0, 
[2025-10-23 16:59:53 TP0] Decode batch. #running-req: 16, #token: 60876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.61, #queue-req: 0, 
[2025-10-23 16:59:54 TP0] Decode batch. #running-req: 16, #token: 61516, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.95, #queue-req: 0, 
[2025-10-23 16:59:55 TP0] Decode batch. #running-req: 16, #token: 62156, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.07, #queue-req: 0, 
[2025-10-23 16:59:56 TP0] Decode batch. #running-req: 16, #token: 62796, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.09, #queue-req: 0, 
[2025-10-23 16:59:57 TP0] Decode batch. #running-req: 16, #token: 63436, token usage: 0.07, cuda graph: True, gen throughput (token/s): 615.17, #queue-req: 0, 
[2025-10-23 16:59:58] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58 TP0] Prefill batch. #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 16:59:58] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:58] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 16:59:59 TP0] Prefill batch. #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 16:59:59 TP0] Prefill batch. #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 17:00:00 TP0] Prefill batch. #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 17:00:01 TP0] Decode batch. #running-req: 16, #token: 51276, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.85, #queue-req: 0, 
[2025-10-23 17:00:02 TP0] Decode batch. #running-req: 16, #token: 51916, token usage: 0.05, cuda graph: True, gen throughput (token/s): 625.10, #queue-req: 0, 
[2025-10-23 17:00:03 TP0] Decode batch. #running-req: 16, #token: 52556, token usage: 0.05, cuda graph: True, gen throughput (token/s): 619.24, #queue-req: 0, 
[2025-10-23 17:00:04 TP0] Decode batch. #running-req: 16, #token: 53196, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.57, #queue-req: 0, 
[2025-10-23 17:00:05 TP0] Decode batch. #running-req: 16, #token: 53836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.71, #queue-req: 0, 
[2025-10-23 17:00:06 TP0] Decode batch. #running-req: 16, #token: 54476, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.08, #queue-req: 0, 
[2025-10-23 17:00:07 TP0] Decode batch. #running-req: 16, #token: 55116, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.62, #queue-req: 0, 
[2025-10-23 17:00:08 TP0] Decode batch. #running-req: 16, #token: 55756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.21, #queue-req: 0, 
[2025-10-23 17:00:09 TP0] Decode batch. #running-req: 16, #token: 56396, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.37, #queue-req: 0, 
[2025-10-23 17:00:11 TP0] Decode batch. #running-req: 16, #token: 57036, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.71, #queue-req: 0, 
[2025-10-23 17:00:12 TP0] Decode batch. #running-req: 16, #token: 57676, token usage: 0.06, cuda graph: True, gen throughput (token/s): 618.01, #queue-req: 0, 
[2025-10-23 17:00:13 TP0] Decode batch. #running-req: 16, #token: 58316, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.35, #queue-req: 0, 
[2025-10-23 17:00:14 TP0] Decode batch. #running-req: 16, #token: 58956, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.22, #queue-req: 0, 
[2025-10-23 17:00:15 TP0] Decode batch. #running-req: 16, #token: 59596, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.46, #queue-req: 0, 
[2025-10-23 17:00:16 TP0] Decode batch. #running-req: 16, #token: 60236, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.91, #queue-req: 0, 
[2025-10-23 17:00:17 TP0] Decode batch. #running-req: 16, #token: 60876, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.51, #queue-req: 0, 
[2025-10-23 17:00:18 TP0] Decode batch. #running-req: 16, #token: 61516, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.34, #queue-req: 0, 
[2025-10-23 17:00:19 TP0] Decode batch. #running-req: 16, #token: 62156, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.69, #queue-req: 0, 
[2025-10-23 17:00:20 TP0] Decode batch. #running-req: 16, #token: 62796, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.12, #queue-req: 0, 
[2025-10-23 17:00:21 TP0] Decode batch. #running-req: 16, #token: 63436, token usage: 0.07, cuda graph: True, gen throughput (token/s): 614.66, #queue-req: 0, 
[2025-10-23 17:00:22] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22 TP0] Prefill batch. #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:00:22] INFO:     127.0.0.1:38736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:22 TP0] Prefill batch. #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 17:00:22 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 17:00:23 TP0] Prefill batch. #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 17:00:25 TP0] Decode batch. #running-req: 16, #token: 51278, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.80, #queue-req: 0, 
[2025-10-23 17:00:26 TP0] Decode batch. #running-req: 16, #token: 51918, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.48, #queue-req: 0, 
[2025-10-23 17:00:27 TP0] Decode batch. #running-req: 16, #token: 52558, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.00, #queue-req: 0, 
[2025-10-23 17:00:28 TP0] Decode batch. #running-req: 16, #token: 53198, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.47, #queue-req: 0, 
[2025-10-23 17:00:29 TP0] Decode batch. #running-req: 16, #token: 53838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.44, #queue-req: 0, 
[2025-10-23 17:00:30 TP0] Decode batch. #running-req: 16, #token: 54478, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.68, #queue-req: 0, 
[2025-10-23 17:00:31 TP0] Decode batch. #running-req: 16, #token: 55118, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.32, #queue-req: 0, 
[2025-10-23 17:00:32 TP0] Decode batch. #running-req: 16, #token: 55758, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.27, #queue-req: 0, 
[2025-10-23 17:00:33 TP0] Decode batch. #running-req: 16, #token: 56398, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.07, #queue-req: 0, 
[2025-10-23 17:00:34 TP0] Decode batch. #running-req: 16, #token: 57038, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.32, #queue-req: 0, 
[2025-10-23 17:00:35 TP0] Decode batch. #running-req: 16, #token: 57678, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.98, #queue-req: 0, 
[2025-10-23 17:00:36 TP0] Decode batch. #running-req: 16, #token: 58318, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.69, #queue-req: 0, 
[2025-10-23 17:00:37 TP0] Decode batch. #running-req: 16, #token: 58958, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.49, #queue-req: 0, 
[2025-10-23 17:00:38 TP0] Decode batch. #running-req: 16, #token: 59598, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.10, #queue-req: 0, 
[2025-10-23 17:00:39 TP0] Decode batch. #running-req: 16, #token: 60238, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.01, #queue-req: 0, 
[2025-10-23 17:00:40 TP0] Decode batch. #running-req: 16, #token: 60878, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.58, #queue-req: 0, 
[2025-10-23 17:00:41 TP0] Decode batch. #running-req: 16, #token: 61518, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.10, #queue-req: 0, 
[2025-10-23 17:00:42 TP0] Decode batch. #running-req: 16, #token: 62158, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.07, #queue-req: 0, 
[2025-10-23 17:00:44 TP0] Decode batch. #running-req: 16, #token: 62798, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.14, #queue-req: 0, 
[2025-10-23 17:00:45 TP0] Decode batch. #running-req: 16, #token: 63438, token usage: 0.07, cuda graph: True, gen throughput (token/s): 606.76, #queue-req: 0, 
[2025-10-23 17:00:45] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46 TP0] Prefill batch. #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:00:46] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:00:46 TP0] Prefill batch. #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-23 17:00:46 TP0] Prefill batch. #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-23 17:00:47 TP0] Prefill batch. #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-10-23 17:00:48 TP0] Decode batch. #running-req: 16, #token: 51279, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.35, #queue-req: 0, 
[2025-10-23 17:00:49 TP0] Decode batch. #running-req: 16, #token: 51919, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.77, #queue-req: 0, 
[2025-10-23 17:00:50 TP0] Decode batch. #running-req: 16, #token: 52559, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.63, #queue-req: 0, 
[2025-10-23 17:00:51 TP0] Decode batch. #running-req: 16, #token: 53199, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.36, #queue-req: 0, 
[2025-10-23 17:00:52 TP0] Decode batch. #running-req: 16, #token: 53839, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.61, #queue-req: 0, 
[2025-10-23 17:00:54 TP0] Decode batch. #running-req: 16, #token: 54479, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.03, #queue-req: 0, 
[2025-10-23 17:00:55 TP0] Decode batch. #running-req: 16, #token: 55119, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.60, #queue-req: 0, 
[2025-10-23 17:00:56 TP0] Decode batch. #running-req: 16, #token: 55759, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.16, #queue-req: 0, 
[2025-10-23 17:00:57 TP0] Decode batch. #running-req: 16, #token: 56399, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.63, #queue-req: 0, 
[2025-10-23 17:00:58 TP0] Decode batch. #running-req: 16, #token: 57039, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.17, #queue-req: 0, 
[2025-10-23 17:00:59 TP0] Decode batch. #running-req: 16, #token: 57679, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.64, #queue-req: 0, 
[2025-10-23 17:01:00 TP0] Decode batch. #running-req: 16, #token: 58319, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.28, #queue-req: 0, 
[2025-10-23 17:01:01 TP0] Decode batch. #running-req: 16, #token: 58959, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.53, #queue-req: 0, 
[2025-10-23 17:01:02 TP0] Decode batch. #running-req: 16, #token: 59599, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.85, #queue-req: 0, 
[2025-10-23 17:01:03 TP0] Decode batch. #running-req: 16, #token: 60239, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.80, #queue-req: 0, 
[2025-10-23 17:01:04 TP0] Decode batch. #running-req: 16, #token: 60879, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.58, #queue-req: 0, 
[2025-10-23 17:01:05 TP0] Decode batch. #running-req: 16, #token: 61519, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.77, #queue-req: 0, 
[2025-10-23 17:01:06 TP0] Decode batch. #running-req: 16, #token: 62159, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.61, #queue-req: 0, 
[2025-10-23 17:01:07 TP0] Decode batch. #running-req: 16, #token: 62799, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.66, #queue-req: 0, 
[2025-10-23 17:01:08 TP0] Decode batch. #running-req: 16, #token: 63439, token usage: 0.07, cuda graph: True, gen throughput (token/s): 607.66, #queue-req: 0, 
[2025-10-23 17:01:09] INFO:     127.0.0.1:55344 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-23 17:01:26] INFO:     127.0.0.1:59468 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-23 17:01:32] INFO:     127.0.0.1:59476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:32 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:01:32 TP0] Decode batch. #running-req: 1, #token: 3207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 23.60, #queue-req: 0, 
[2025-10-23 17:01:34] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:34] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:01:34] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:34] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:34 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-23 17:01:34 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 39.91, #queue-req: 0, 
[2025-10-23 17:01:35 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.03, #queue-req: 0, 
[2025-10-23 17:01:36 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-23 17:01:37 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-10-23 17:01:37 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:01:38 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.91, #queue-req: 0, 
[2025-10-23 17:01:39 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:01:40 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:01:41 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:01:42 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:01:42 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:01:43 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:01:44 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:01:45 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:01:46 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:01:47 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:01:47 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:01:48 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:01:49 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:01:50 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:01:50] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:50] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:50] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:50 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:01:50] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:01:50 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:01:51 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.09, #queue-req: 0, 
[2025-10-23 17:01:52 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.07, #queue-req: 0, 
[2025-10-23 17:01:52 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.02, #queue-req: 0, 
[2025-10-23 17:01:53 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:01:54 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-10-23 17:01:55 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:01:56 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-10-23 17:01:57 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:01:57 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:01:58 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:01:59 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:02:00 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:01 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:02 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:02 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:02:03 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:04 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:02:05 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:06 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:07 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:07] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:07] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:07] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:07 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:02:07] INFO:     127.0.0.1:34398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:07 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:02:08 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.08, #queue-req: 0, 
[2025-10-23 17:02:08 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-23 17:02:09 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:02:10 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-10-23 17:02:11 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:02:12 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-10-23 17:02:12 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:02:13 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:02:14 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:15 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:16 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:17 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:17 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:18 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:19 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:20 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:21 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:02:22 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:22 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:02:23 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:02:24] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:24] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:24] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:24 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:02:24] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:24 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:02:24 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.12, #queue-req: 0, 
[2025-10-23 17:02:25 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.09, #queue-req: 0, 
[2025-10-23 17:02:26 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.07, #queue-req: 0, 
[2025-10-23 17:02:27 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-10-23 17:02:27 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.96, #queue-req: 0, 
[2025-10-23 17:02:28 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.91, #queue-req: 0, 
[2025-10-23 17:02:29 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:02:30 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:31 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:32 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:32 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:33 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:34 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:35 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:36 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:37 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:37 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:02:38 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:02:39 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:40 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:02:40] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:40] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:40] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:40 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:02:40] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:41 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:02:41 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.20, #queue-req: 0, 
[2025-10-23 17:02:42 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-23 17:02:43 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-10-23 17:02:43 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.96, #queue-req: 0, 
[2025-10-23 17:02:44 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:02:45 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:02:46 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:02:47 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:02:47 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:48 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:02:49 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:50 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:02:51 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:02:52 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:02:52 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:53 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:02:54 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:55 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:02:56 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:02:57 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:02:57] INFO:     127.0.0.1:48376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:57] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:57] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:57 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:02:57] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:02:57 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:02:58 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.97, #queue-req: 0, 
[2025-10-23 17:02:58 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.09, #queue-req: 0, 
[2025-10-23 17:02:59 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.04, #queue-req: 0, 
[2025-10-23 17:03:00 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:03:01 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:03:02 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:03:02 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.91, #queue-req: 0, 
[2025-10-23 17:03:03 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:03:04 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:03:05 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:06 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:03:07 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:07 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:08 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:03:09 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:10 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:11 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:03:12 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:03:12 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:13 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:03:14] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:14] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:14] INFO:     127.0.0.1:36888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:14 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:03:14] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:14 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:03:14 TP0] Decode batch. #running-req: 4, #token: 12852, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.82, #queue-req: 0, 
[2025-10-23 17:03:15 TP0] Decode batch. #running-req: 4, #token: 13012, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.05, #queue-req: 0, 
[2025-10-23 17:03:16 TP0] Decode batch. #running-req: 4, #token: 13172, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.01, #queue-req: 0, 
[2025-10-23 17:03:17 TP0] Decode batch. #running-req: 4, #token: 13332, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.97, #queue-req: 0, 
[2025-10-23 17:03:18 TP0] Decode batch. #running-req: 4, #token: 13492, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.96, #queue-req: 0, 
[2025-10-23 17:03:18 TP0] Decode batch. #running-req: 4, #token: 13652, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:03:19 TP0] Decode batch. #running-req: 4, #token: 13812, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:03:20 TP0] Decode batch. #running-req: 4, #token: 13972, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:03:21 TP0] Decode batch. #running-req: 4, #token: 14132, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:03:22 TP0] Decode batch. #running-req: 4, #token: 14292, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:03:22 TP0] Decode batch. #running-req: 4, #token: 14452, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:23 TP0] Decode batch. #running-req: 4, #token: 14612, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:03:24 TP0] Decode batch. #running-req: 4, #token: 14772, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:25 TP0] Decode batch. #running-req: 4, #token: 14932, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:26 TP0] Decode batch. #running-req: 4, #token: 15092, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:27 TP0] Decode batch. #running-req: 4, #token: 15252, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:03:27 TP0] Decode batch. #running-req: 4, #token: 15412, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:28 TP0] Decode batch. #running-req: 4, #token: 15572, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:29 TP0] Decode batch. #running-req: 4, #token: 15732, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:03:30 TP0] Decode batch. #running-req: 4, #token: 15892, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:30] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:30] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:30] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:30 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:03:30] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:31 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:03:31 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.66, #queue-req: 0, 
[2025-10-23 17:03:32 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.15, #queue-req: 0, 
[2025-10-23 17:03:33 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.08, #queue-req: 0, 
[2025-10-23 17:03:33 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:03:34 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:03:35 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:03:36 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:37 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:37 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:03:38 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:39 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:03:40 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:41 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:42 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:42 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:03:43 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:03:44 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:03:45 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:03:46 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:03:47 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:03:47] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:47] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:47] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:47 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:03:47] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:03:47 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:03:48 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.99, #queue-req: 0, 
[2025-10-23 17:03:48 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.11, #queue-req: 0, 
[2025-10-23 17:03:49 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.12, #queue-req: 0, 
[2025-10-23 17:03:50 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.11, #queue-req: 0, 
[2025-10-23 17:03:51 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:03:52 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:03:53 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:03:53 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:03:54 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:03:55 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:03:56 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:03:57 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:03:57 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:03:58 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:03:59 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:04:00 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:04:01 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:04:02 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:04:02 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:04:03 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:04:04] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:04] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:04] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:04 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:04:04] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:04 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:04:04 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.04, #queue-req: 0, 
[2025-10-23 17:04:05 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.08, #queue-req: 0, 
[2025-10-23 17:04:06 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.02, #queue-req: 0, 
[2025-10-23 17:04:07 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:04:08 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:04:08 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:04:09 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:04:10 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:04:11 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:04:12 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:04:12 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:04:13 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:04:14 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:04:15 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:04:16 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:04:17 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:04:17 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:04:18 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:04:19 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-23 17:04:20 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:04:20] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:20] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:20] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:20 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:04:20] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:21 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:04:21 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.03, #queue-req: 0, 
[2025-10-23 17:04:22 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.05, #queue-req: 0, 
[2025-10-23 17:04:23 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-10-23 17:04:23 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-10-23 17:04:24 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-10-23 17:04:25 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:04:26 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:04:27 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:04:28 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:04:28 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:04:29 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-23 17:04:30 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 194.06, #queue-req: 0, 
[2025-10-23 17:04:31 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:04:32 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:04:32 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:04:33 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:04:34 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-23 17:04:35 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-23 17:04:36 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:04:37 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-23 17:04:37] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:37] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:37] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:37 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:04:37] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:37 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:04:38 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.04, #queue-req: 0, 
[2025-10-23 17:04:38 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.09, #queue-req: 0, 
[2025-10-23 17:04:39 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.04, #queue-req: 0, 
[2025-10-23 17:04:40 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-10-23 17:04:41 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.97, #queue-req: 0, 
[2025-10-23 17:04:42 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.94, #queue-req: 0, 
[2025-10-23 17:04:43 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:04:43 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:04:44 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:04:45 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:04:46 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:04:47 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:04:47 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:04:48 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:04:49 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:04:50 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:04:51 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:04:52 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:04:52 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-23 17:04:53 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:04:54] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:54] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:54] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:54 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:04:54] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:04:54 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:04:54 TP0] Decode batch. #running-req: 4, #token: 12853, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.11, #queue-req: 0, 
[2025-10-23 17:04:55 TP0] Decode batch. #running-req: 4, #token: 13013, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.03, #queue-req: 0, 
[2025-10-23 17:04:56 TP0] Decode batch. #running-req: 4, #token: 13173, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.06, #queue-req: 0, 
[2025-10-23 17:04:57 TP0] Decode batch. #running-req: 4, #token: 13333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.99, #queue-req: 0, 
[2025-10-23 17:04:58 TP0] Decode batch. #running-req: 4, #token: 13493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.97, #queue-req: 0, 
[2025-10-23 17:04:58 TP0] Decode batch. #running-req: 4, #token: 13653, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-10-23 17:04:59 TP0] Decode batch. #running-req: 4, #token: 13813, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:05:00 TP0] Decode batch. #running-req: 4, #token: 13973, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-10-23 17:05:01 TP0] Decode batch. #running-req: 4, #token: 14133, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:05:02 TP0] Decode batch. #running-req: 4, #token: 14293, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:05:03 TP0] Decode batch. #running-req: 4, #token: 14453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:05:03 TP0] Decode batch. #running-req: 4, #token: 14613, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:05:04 TP0] Decode batch. #running-req: 4, #token: 14773, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:05:05 TP0] Decode batch. #running-req: 4, #token: 14933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:05:06 TP0] Decode batch. #running-req: 4, #token: 15093, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:05:07 TP0] Decode batch. #running-req: 4, #token: 15253, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-23 17:05:07 TP0] Decode batch. #running-req: 4, #token: 15413, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-23 17:05:08 TP0] Decode batch. #running-req: 4, #token: 15573, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:05:09 TP0] Decode batch. #running-req: 4, #token: 15733, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:05:10 TP0] Decode batch. #running-req: 4, #token: 15893, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-23 17:05:10] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:10] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:11] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:11 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:05:11] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:11 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:05:11 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.16, #queue-req: 0, 
[2025-10-23 17:05:12 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.09, #queue-req: 0, 
[2025-10-23 17:05:13 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.03, #queue-req: 0, 
[2025-10-23 17:05:13 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.97, #queue-req: 0, 
[2025-10-23 17:05:14 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-10-23 17:05:15 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-10-23 17:05:16 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-10-23 17:05:17 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:05:18 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:05:18 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:05:19 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:05:20 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-10-23 17:05:21 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-10-23 17:05:22 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-23 17:05:22 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-23 17:05:23 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-10-23 17:05:24 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-10-23 17:05:25 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-10-23 17:05:26 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-10-23 17:05:27 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.91, #queue-req: 0, 
[2025-10-23 17:05:27] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:27] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:05:27] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:27] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:27 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-23 17:05:28 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.20, #queue-req: 0, 
[2025-10-23 17:05:28 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:05:29 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-23 17:05:30 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:05:31 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:05:32 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:05:33 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:05:33 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:05:34 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:05:35 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:05:36 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:05:37 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:05:38 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:05:38 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:05:39 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:05:40 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:05:41 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:05:42 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:05:43 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:05:43 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:05:44] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:44] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:44] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:44 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:05:44] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:05:44 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:05:44 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.58, #queue-req: 0, 
[2025-10-23 17:05:45 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:05:46 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:05:47 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:05:48 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:05:48 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:05:49 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:05:50 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:05:51 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:05:52 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:05:53 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:05:53 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:05:54 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:05:55 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:05:56 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-23 17:05:57 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:05:58 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:05:58 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-23 17:05:59 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:06:00 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-23 17:06:01] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:01] INFO:     127.0.0.1:35918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:01] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:01 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:06:01] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:01 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:06:01 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.96, #queue-req: 0, 
[2025-10-23 17:06:02 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-23 17:06:03 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-23 17:06:04 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:06:04 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:06:05 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:06:06 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:06:07 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:06:08 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:06:09 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:06:09 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:06:10 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:06:11 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:06:12 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:06:13 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:06:13 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:06:14 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:06:15 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:06:16 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:06:17 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:06:17] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:17] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:17] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:17 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:06:17] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:17 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:06:18 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.77, #queue-req: 0, 
[2025-10-23 17:06:19 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-23 17:06:19 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-23 17:06:20 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-23 17:06:21 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-23 17:06:22 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-23 17:06:23 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:06:24 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:06:24 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:06:25 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:06:26 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-23 17:06:27 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:06:28 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-23 17:06:29 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:06:29 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:06:30 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:06:31 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-23 17:06:32 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:06:33 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:06:33 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:06:34] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:34] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:34] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:34 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:06:34] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:34 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:06:34 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.91, #queue-req: 0, 
[2025-10-23 17:06:35 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-23 17:06:36 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-23 17:06:37 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-23 17:06:38 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-23 17:06:39 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-23 17:06:39 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-23 17:06:40 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:06:41 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-23 17:06:42 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-23 17:06:43 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:06:44 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:06:44 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:06:45 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:06:46 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:06:47 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:06:48 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:06:49 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:06:49 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:06:50 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:06:51] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:51] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:51] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:51 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:06:51] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:06:51 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:06:51 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.82, #queue-req: 0, 
[2025-10-23 17:06:52 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:06:53 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:06:54 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:06:54 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:06:55 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:06:56 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-23 17:06:57 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:06:58 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:06:59 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:06:59 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:07:00 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:07:01 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:07:02 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:07:03 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:07:04 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-23 17:07:04 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:07:05 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:07:06 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:07:07 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-23 17:07:07] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:07] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:07] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:07:07] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:08 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-23 17:07:08 TP0] Decode batch. #running-req: 4, #token: 12853, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0, 
[2025-10-23 17:07:09 TP0] Decode batch. #running-req: 4, #token: 13013, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:07:10 TP0] Decode batch. #running-req: 4, #token: 13173, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:07:10 TP0] Decode batch. #running-req: 4, #token: 13333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:07:11 TP0] Decode batch. #running-req: 4, #token: 13493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:07:12 TP0] Decode batch. #running-req: 4, #token: 13653, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:07:13 TP0] Decode batch. #running-req: 4, #token: 13813, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:07:14 TP0] Decode batch. #running-req: 4, #token: 13973, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:07:15 TP0] Decode batch. #running-req: 4, #token: 14133, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:07:15 TP0] Decode batch. #running-req: 4, #token: 14293, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:07:16 TP0] Decode batch. #running-req: 4, #token: 14453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:07:17 TP0] Decode batch. #running-req: 4, #token: 14613, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:07:18 TP0] Decode batch. #running-req: 4, #token: 14773, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:07:19 TP0] Decode batch. #running-req: 4, #token: 14933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:07:19 TP0] Decode batch. #running-req: 4, #token: 15093, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-23 17:07:20 TP0] Decode batch. #running-req: 4, #token: 15253, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:07:21 TP0] Decode batch. #running-req: 4, #token: 15413, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:07:22 TP0] Decode batch. #running-req: 4, #token: 15573, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:07:23 TP0] Decode batch. #running-req: 4, #token: 15733, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:07:24 TP0] Decode batch. #running-req: 4, #token: 15893, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:07:24] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:24] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:24] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:24 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:07:24] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:24 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:07:25 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.57, #queue-req: 0, 
[2025-10-23 17:07:25 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-23 17:07:26 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-23 17:07:27 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:07:28 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:07:29 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:07:30 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:07:30 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:07:31 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:07:32 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-23 17:07:33 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-23 17:07:34 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:07:35 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:07:35 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-23 17:07:36 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:07:37 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:07:38 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:07:39 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:07:39 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:07:40 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-23 17:07:41] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:41] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:41] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:41 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:07:41] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:41 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:07:41 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.17, #queue-req: 0, 
[2025-10-23 17:07:42 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-23 17:07:43 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:07:44 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:07:45 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:07:45 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:07:46 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:07:47 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:07:48 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:07:49 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:07:50 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:07:50 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:07:51 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:07:52 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:07:53 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-23 17:07:54 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:07:55 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:07:55 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:07:56 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:07:57 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:07:58] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:58] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:58] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:58 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:07:58] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:07:58 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:07:58 TP0] Decode batch. #running-req: 4, #token: 12853, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.81, #queue-req: 0, 
[2025-10-23 17:07:59 TP0] Decode batch. #running-req: 4, #token: 13013, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-23 17:08:00 TP0] Decode batch. #running-req: 4, #token: 13173, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-23 17:08:01 TP0] Decode batch. #running-req: 4, #token: 13333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-23 17:08:01 TP0] Decode batch. #running-req: 4, #token: 13493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:08:02 TP0] Decode batch. #running-req: 4, #token: 13653, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-23 17:08:03 TP0] Decode batch. #running-req: 4, #token: 13813, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:08:04 TP0] Decode batch. #running-req: 4, #token: 13973, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-23 17:08:05 TP0] Decode batch. #running-req: 4, #token: 14133, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:08:05 TP0] Decode batch. #running-req: 4, #token: 14293, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:08:06 TP0] Decode batch. #running-req: 4, #token: 14453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-23 17:08:07 TP0] Decode batch. #running-req: 4, #token: 14613, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-23 17:08:08 TP0] Decode batch. #running-req: 4, #token: 14773, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:08:09 TP0] Decode batch. #running-req: 4, #token: 14933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:08:10 TP0] Decode batch. #running-req: 4, #token: 15093, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:08:10 TP0] Decode batch. #running-req: 4, #token: 15253, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:08:11 TP0] Decode batch. #running-req: 4, #token: 15413, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:08:12 TP0] Decode batch. #running-req: 4, #token: 15573, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:08:13 TP0] Decode batch. #running-req: 4, #token: 15733, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:08:14 TP0] Decode batch. #running-req: 4, #token: 15893, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:08:14] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:14] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:14] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:14 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:08:14] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:14 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:08:15 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.64, #queue-req: 0, 
[2025-10-23 17:08:16 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:08:16 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:08:17 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:08:18 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-23 17:08:19 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:08:20 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:08:21 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-23 17:08:21 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:08:22 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:08:23 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-23 17:08:24 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-23 17:08:25 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-23 17:08:26 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-23 17:08:26 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-23 17:08:27 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-10-23 17:08:28 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-23 17:08:29 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.11, #queue-req: 0, 
[2025-10-23 17:08:30 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-23 17:08:30 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-23 17:08:31] INFO:     127.0.0.1:34528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:31] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:31] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:31 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:08:31] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:31 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:08:31 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.83, #queue-req: 0, 
[2025-10-23 17:08:32 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:08:33 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:08:34 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-23 17:08:35 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:08:36 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:08:36 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-23 17:08:37 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-23 17:08:38 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:08:39 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:08:40 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-23 17:08:41 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-23 17:08:41 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-23 17:08:42 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.10, #queue-req: 0, 
[2025-10-23 17:08:43 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-23 17:08:44 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.10, #queue-req: 0, 
[2025-10-23 17:08:45 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-23 17:08:46 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.04, #queue-req: 0, 
[2025-10-23 17:08:46 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-23 17:08:47 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.09, #queue-req: 0, 
[2025-10-23 17:08:48] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:48] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:48] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:48 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:08:48] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:08:48 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:08:48 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.52, #queue-req: 0, 
[2025-10-23 17:08:49 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-23 17:08:50 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-23 17:08:51 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-23 17:08:52 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:08:52 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:08:53 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-23 17:08:54 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:08:55 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:08:56 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:08:56 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:08:57 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:08:58 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-23 17:08:59 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:09:00 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:09:01 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-23 17:09:01 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-23 17:09:02 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-23 17:09:03 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-23 17:09:04 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-23 17:09:04] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:04] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:05] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:05 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:09:05] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:05 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:09:05 TP0] Decode batch. #running-req: 4, #token: 12853, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.54, #queue-req: 0, 
[2025-10-23 17:09:06 TP0] Decode batch. #running-req: 4, #token: 13013, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-23 17:09:07 TP0] Decode batch. #running-req: 4, #token: 13173, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-23 17:09:07 TP0] Decode batch. #running-req: 4, #token: 13333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:09:08 TP0] Decode batch. #running-req: 4, #token: 13493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:09:09 TP0] Decode batch. #running-req: 4, #token: 13653, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-23 17:09:10 TP0] Decode batch. #running-req: 4, #token: 13813, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:09:11 TP0] Decode batch. #running-req: 4, #token: 13973, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-23 17:09:12 TP0] Decode batch. #running-req: 4, #token: 14133, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-23 17:09:12 TP0] Decode batch. #running-req: 4, #token: 14293, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:09:13 TP0] Decode batch. #running-req: 4, #token: 14453, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-23 17:09:14 TP0] Decode batch. #running-req: 4, #token: 14613, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:09:15 TP0] Decode batch. #running-req: 4, #token: 14773, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-23 17:09:16 TP0] Decode batch. #running-req: 4, #token: 14933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:09:17 TP0] Decode batch. #running-req: 4, #token: 15093, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:09:17 TP0] Decode batch. #running-req: 4, #token: 15253, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-23 17:09:18 TP0] Decode batch. #running-req: 4, #token: 15413, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-23 17:09:19 TP0] Decode batch. #running-req: 4, #token: 15573, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-23 17:09:20 TP0] Decode batch. #running-req: 4, #token: 15733, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-23 17:09:21 TP0] Decode batch. #running-req: 4, #token: 15893, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-23 17:09:21] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:21] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:09:21] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:21] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:21 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-23 17:09:22 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.91, #queue-req: 0, 
[2025-10-23 17:09:22 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-23 17:09:23 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-23 17:09:24 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-23 17:09:25 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:09:26 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:09:27 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:09:27 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:09:28 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-23 17:09:29 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:09:30 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-23 17:09:31 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-23 17:09:32 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:09:32 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-23 17:09:33 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-23 17:09:34 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:09:35 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:09:36 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:09:37 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:09:37 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-23 17:09:38] INFO:     127.0.0.1:33500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:38] INFO:     127.0.0.1:33516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:38] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:38 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:09:38] INFO:     127.0.0.1:33526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:38 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:09:38 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.78, #queue-req: 0, 
[2025-10-23 17:09:39 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-23 17:09:40 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-23 17:09:41 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-23 17:09:42 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-23 17:09:42 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-23 17:09:43 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:09:44 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-23 17:09:45 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:09:46 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-23 17:09:47 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:09:47 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:09:48 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:09:49 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:09:50 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-23 17:09:51 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:09:52 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:09:52 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:09:53 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:09:54 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:09:55] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:55] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:55] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:55 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:09:55] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:09:55 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:09:55 TP0] Decode batch. #running-req: 4, #token: 12854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.85, #queue-req: 0, 
[2025-10-23 17:09:56 TP0] Decode batch. #running-req: 4, #token: 13014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-23 17:09:57 TP0] Decode batch. #running-req: 4, #token: 13174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-23 17:09:58 TP0] Decode batch. #running-req: 4, #token: 13334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-23 17:09:58 TP0] Decode batch. #running-req: 4, #token: 13494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-23 17:09:59 TP0] Decode batch. #running-req: 4, #token: 13654, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-23 17:10:00 TP0] Decode batch. #running-req: 4, #token: 13814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-23 17:10:01 TP0] Decode batch. #running-req: 4, #token: 13974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:10:02 TP0] Decode batch. #running-req: 4, #token: 14134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-23 17:10:03 TP0] Decode batch. #running-req: 4, #token: 14294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-23 17:10:03 TP0] Decode batch. #running-req: 4, #token: 14454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:10:04 TP0] Decode batch. #running-req: 4, #token: 14614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:10:05 TP0] Decode batch. #running-req: 4, #token: 14774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:10:06 TP0] Decode batch. #running-req: 4, #token: 14934, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:10:07 TP0] Decode batch. #running-req: 4, #token: 15094, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-23 17:10:07 TP0] Decode batch. #running-req: 4, #token: 15254, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-23 17:10:08 TP0] Decode batch. #running-req: 4, #token: 15414, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:10:09 TP0] Decode batch. #running-req: 4, #token: 15574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:10:10 TP0] Decode batch. #running-req: 4, #token: 15734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:10:11 TP0] Decode batch. #running-req: 4, #token: 15894, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-23 17:10:11] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:11] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:11] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:11 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:10:11] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:11 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-23 17:10:12 TP0] Decode batch. #running-req: 4, #token: 12852, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.75, #queue-req: 0, 
[2025-10-23 17:10:13 TP0] Decode batch. #running-req: 4, #token: 13012, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-23 17:10:13 TP0] Decode batch. #running-req: 4, #token: 13172, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-23 17:10:14 TP0] Decode batch. #running-req: 4, #token: 13332, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-23 17:10:15 TP0] Decode batch. #running-req: 4, #token: 13492, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-23 17:10:16 TP0] Decode batch. #running-req: 4, #token: 13652, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-23 17:10:17 TP0] Decode batch. #running-req: 4, #token: 13812, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-23 17:10:18 TP0] Decode batch. #running-req: 4, #token: 13972, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:10:18 TP0] Decode batch. #running-req: 4, #token: 14132, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-23 17:10:19 TP0] Decode batch. #running-req: 4, #token: 14292, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-23 17:10:20 TP0] Decode batch. #running-req: 4, #token: 14452, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-23 17:10:21 TP0] Decode batch. #running-req: 4, #token: 14612, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:10:22 TP0] Decode batch. #running-req: 4, #token: 14772, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:10:23 TP0] Decode batch. #running-req: 4, #token: 14932, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-23 17:10:23 TP0] Decode batch. #running-req: 4, #token: 15092, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-23 17:10:24 TP0] Decode batch. #running-req: 4, #token: 15252, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-23 17:10:25 TP0] Decode batch. #running-req: 4, #token: 15412, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-23 17:10:26 TP0] Decode batch. #running-req: 4, #token: 15572, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-23 17:10:27 TP0] Decode batch. #running-req: 4, #token: 15732, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-23 17:10:28 TP0] Decode batch. #running-req: 4, #token: 15892, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-23 17:10:28] INFO:     127.0.0.1:57172 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-23 17:10:45] INFO:     127.0.0.1:38830 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-23 17:10:50] INFO:     127.0.0.1:38842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:51 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:10:51 TP0] Decode batch. #running-req: 1, #token: 3215, token usage: 0.00, cuda graph: True, gen throughput (token/s): 5.17, #queue-req: 0, 
[2025-10-23 17:10:52] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:10:52 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:10:53 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.12, #queue-req: 0, 
[2025-10-23 17:10:54 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:10:54 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:10:55 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:10:56 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:10:57 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:10:58 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:10:59 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:10:59 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:00 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:01 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:02 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:03 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:03 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:04 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:05 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:06 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:07 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:08 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:08 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:09] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:11:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:11:09 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-10-23 17:11:10 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:11:11 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:12 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:11:12 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:11:13 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:14 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:11:15 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:16 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:17 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:17 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:18 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:19 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:20 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:21 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:21 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:22 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:23 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:24 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:25 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:11:25] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:11:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:11:26 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:11:26 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:11:27 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:28 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:29 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:30 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:31 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:31 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:32 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:33 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:34 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:35 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:35 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:36 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:37 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:38 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:39 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:40 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:40 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:11:41 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:11:42] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:11:42 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:11:42 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:11:43 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:11:44 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:11:45 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:45 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:11:46 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:47 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:48 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:11:49 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:49 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:50 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:11:51 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:52 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:53 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:54 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:54 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:11:55 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:11:56 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:57 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:11:58 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:11:58] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:11:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:11:59 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:11:59 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:12:00 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:01 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:12:02 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:03 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:03 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:04 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:05 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:06 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:07 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:08 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:08 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:09 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:10 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:11 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:12 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:12 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:13 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:14 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:14] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:12:14 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:12:15 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:12:16 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:12:17 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:12:17 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:18 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:12:19 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:20 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:21 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:21 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:22 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:23 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:24 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:25 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:26 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:26 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:27 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:28 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:29 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:30 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:30 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:31] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:12:31 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:12:31 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:12:32 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:12:33 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:12:34 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:35 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:35 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:36 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:37 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:38 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:39 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:40 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:40 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:41 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:42 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:43 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:44 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:44 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:45 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:46 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:12:47 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:12:47] INFO:     127.0.0.1:43600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:12:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:12:48 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:12:49 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:12:49 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:12:50 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:12:51 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:52 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:53 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:12:54 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:54 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:55 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:12:56 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:12:57 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:58 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:12:58 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:12:59 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:00 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:01 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:02 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:03 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:03 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:13:04] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:13:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:13:04 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-23 17:13:05 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:06 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:13:07 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:13:08 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:08 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:09 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:10 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:11 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:12 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:12 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:13 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:14 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:15 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:16 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:17 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:17 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:18 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:19 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-23 17:13:20 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:13:20] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:13:20 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:13:21 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:13:22 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:22 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:23 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:13:24 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:13:25 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:26 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:26 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:27 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:28 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:29 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:30 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:31 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:31 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:32 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:33 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:34 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:35 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:35 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:36 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:37] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:13:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:13:37 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:13:38 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:39 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:40 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:40 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:41 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:42 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:43 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:13:44 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:44 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:13:45 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:46 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:13:47 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:48 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:13:49 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:49 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:50 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:51 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:13:52 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:13:53 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:13:53] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:13:53 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:13:54 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:13:54 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:55 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:13:56 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:13:57 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:58 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:58 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:13:59 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:00 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:01 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:02 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:03 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:03 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:04 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:05 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:06 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:07 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:07 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:08 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:09 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:09] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:14:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:14:10 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-23 17:14:11 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:14:12 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:12 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:13 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:14 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:15 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:16 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:17 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:17 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:18 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:19 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:20 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:21 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:21 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:22 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:23 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:24 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:25 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:26 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:26] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:14:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:14:26 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:14:27 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:14:28 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:14:29 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:14:30 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:31 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:31 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:32 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:33 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:34 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:35 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:35 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:36 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:37 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:38 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:39 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 37.57, #queue-req: 0, 
[2025-10-23 17:14:40 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:41 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:41 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:42 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:43] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:14:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:14:43 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-23 17:14:44 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:14:45 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:14:46 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:14:46 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:14:47 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:48 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:49 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:50 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:14:50 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:51 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:52 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:14:53 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:54 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:55 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:14:55 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:14:56 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:57 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:14:58 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:14:59 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:14:59] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:14:59 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:15:00 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:15:00 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:01 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:02 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:03 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:04 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:04 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:05 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:06 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:07 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:08 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.31, #queue-req: 0, 
[2025-10-23 17:15:09 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:10 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:11 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:11 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:12 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:13 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:14 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:15 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:15:16 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:15:16] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:15:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:15:16 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:15:17 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:15:18 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:19 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:20 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:20 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:21 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:22 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:23 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:24 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:25 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:25 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:26 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:27 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:28 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:29 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:29 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:30 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:15:31 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:15:32 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:15:32] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:15:32 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:15:33 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-23 17:15:34 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:15:34 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:15:35 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:36 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:37 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:38 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:39 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:39 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:40 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:41 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:42 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:43 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:43 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:44 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:45 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:46 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:15:47 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:15:48 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:15:48 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:15:49] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:15:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:15:49 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:15:50 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:15:51 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:52 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:53 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:15:53 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:54 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:15:55 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:15:56 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:57 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:57 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:58 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:15:59 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:00 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:01 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:02 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:02 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:03 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:04 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:16:05 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:05] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:16:05 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:16:06 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-23 17:16:07 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:07 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:08 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:09 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:16:10 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:11 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:11 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:12 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:13 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:14 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:15 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:16 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:16 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:17 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:18 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:19 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:20 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:20 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:21 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:16:22] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:16:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:16:22 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:16:23 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:16:24 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:16:25 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:16:25 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:26 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:27 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:16:28 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:29 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:29 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:30 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:31 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:32 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:33 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:34 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:34 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:35 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:36 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:37 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:16:38 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:16:38] INFO:     127.0.0.1:40394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:16:38 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:16:39 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:16:39 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:16:40 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:41 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:42 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:16:43 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:43 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:44 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:45 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:46 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:16:47 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:48 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:48 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:49 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:50 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:16:51 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:52 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:16:52 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:53 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:16:54 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:16:54] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:16:54 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:16:55 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:16:56 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:16:57 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:16:57 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:16:58 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:16:59 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:17:00 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:01 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:02 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:02 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:03 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:04 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:05 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:06 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:06 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:07 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:08 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:09 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:10 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:11 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:17:11] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:17:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:17:11 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:17:12 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:17:13 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:17:14 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:17:15 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:16 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:16 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:17 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:18 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:19 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:20 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:20 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:21 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:22 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:23 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:24 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:25 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:25 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:26 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:27 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:17:27] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:17:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:17:28 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.89, #queue-req: 0, 
[2025-10-23 17:17:29 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:17:30 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:17:30 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:31 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:32 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:33 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:34 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:34 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:35 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:36 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:37 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:38 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:38 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:39 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:40 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:41 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:42 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:43 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:17:43 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:44] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:17:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:17:44 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:17:45 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:17:46 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:17:47 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:48 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:48 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:17:49 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:50 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:17:51 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:52 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:17:52 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:53 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:54 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:17:55 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:56 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:57 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:57 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:17:58 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:17:59 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:00 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:00] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:18:00 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:18:01 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:18:02 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:02 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:18:03 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:18:04 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:05 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:06 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:06 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:07 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:08 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:09 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:10 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:11 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:11 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:12 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:13 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:14 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:15 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:15 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:16 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:17] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:18:17 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:18:17 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:18:18 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:19 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:20 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:20 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.38, #queue-req: 0, 
[2025-10-23 17:18:21 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:22 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:23 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:24 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:25 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:25 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:26 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:27 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:28 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:29 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:29 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:30 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:31 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:32 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:33 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:33] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:18:33 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:18:34 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:18:34 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:35 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:36 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:37 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:38 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:39 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:39 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:40 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:41 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:42 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:43 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:43 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:44 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:45 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:46 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:18:47 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:48 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:18:48 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:18:49 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:18:50] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:18:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:18:50 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:18:51 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:52 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:18:52 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:18:53 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:18:54 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:18:55 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:56 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:18:57 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:57 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:18:58 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:18:59 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:00 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:01 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:01 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:02 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:03 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:04 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:05 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:06 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:06] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:19:06 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:19:06 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:19:07 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:19:08 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:19:09 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:19:10 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:19:11 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:11 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:12 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:13 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:14 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:15 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:15 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:16 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:17 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:18 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:19 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:20 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:20 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:21 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:22 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:22] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:19:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:19:23 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:19:24 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:19:25 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:19:25 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:19:26 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:19:27 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:19:28 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:19:29 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:29 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:30 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:31 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:32 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:33 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:34 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:34 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:35 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:36 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:37 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:38 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:38 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:19:39] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:19:39 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:19:39 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-10-23 17:19:40 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:19:41 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:19:42 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:19:43 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:19:43 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:44 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:45 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:46 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:19:47 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:19:48 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:48 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:19:49 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:50 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:51 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:19:52 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:52 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:19:53 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:19:54 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:19:55 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:19:55] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:19:55 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:19:56 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:19:57 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:19:57 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:19:58 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:19:59 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:00 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:20:01 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:01 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:02 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:03 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:04 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:05 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:06 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:06 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:07 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:08 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:09 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:10 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:20:10 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:11 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:12] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:20:12 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:20:12 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.93, #queue-req: 0, 
[2025-10-23 17:20:13 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:20:14 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:15 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:15 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:16 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:17 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:18 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:19 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:20 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:20 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:21 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:22 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:23 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:24 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:24 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:25 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:26 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:27 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:28 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:20:28] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:20:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:20:29 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-23 17:20:29 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:20:30 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:20:31 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:32 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:20:33 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:34 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:34 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:35 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:36 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:37 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:38 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:38 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:39 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:40 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:41 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:20:42 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:43 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:43 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:20:44 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:45] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:20:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:20:45 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:20:46 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:20:47 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:20:48 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:20:48 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:20:49 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:20:50 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:20:51 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:52 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:52 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:20:53 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:54 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:55 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:56 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:20:57 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:57 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:58 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:20:59 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:21:00 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:01 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:21:01] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:21:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:21:02 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-10-23 17:21:02 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:21:03 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:04 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:21:05 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:06 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:06 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:07 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:08 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:09 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:21:10 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:10 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:11 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:12 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:13 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:14 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:15 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:21:15 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:16 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:17 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:17] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:21:17 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:21:18 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:21:19 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:20 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:20 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:21:21 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:22 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:23 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:24 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:24 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:25 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:21:26 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:27 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:28 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:29 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:29 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:30 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:31 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:32 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:33 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:33 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:34] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:21:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:21:34 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-10-23 17:21:35 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:21:36 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:37 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:38 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:21:38 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:21:39 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:40 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:41 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:42 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:43 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:43 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:21:44 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:21:45 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:46 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:47 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:47 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:48 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:49 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:21:50 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:21:50] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:21:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:21:51 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:21:52 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:21:52 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:53 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:54 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:21:55 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:56 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:57 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:21:57 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:21:58 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:21:59 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:00 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:01 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:01 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:02 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:03 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:04 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:05 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:22:06 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:22:06 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:22:07] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:22:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:22:07 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-10-23 17:22:08 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:22:09 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:10 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:10 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:11 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:12 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:22:13 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:22:14 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:15 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:15 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:16 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:17 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:18 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:19 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:19 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:20 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:21 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:22 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:22:23 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:23] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:22:23 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:22:24 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:22:24 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:22:25 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:22:26 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:27 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:28 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:29 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:29 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:30 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:31 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:32 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:33 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:33 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:34 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:35 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:36 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:37 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:38 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:38 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:39 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:40] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:22:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:22:40 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.86, #queue-req: 0, 
[2025-10-23 17:22:41 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:22:42 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:22:43 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:43 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:44 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:45 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:46 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:22:47 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:47 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:48 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:49 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:22:50 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:51 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:22:52 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:52 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:53 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:54 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:22:55 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:22:56 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:22:56] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:22:56 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:22:57 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:22:57 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:58 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:22:59 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:00 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:01 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:01 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:02 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:03 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:04 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:05 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:06 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:06 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:07 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:08 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:09 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:10 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:23:10 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:11 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:12 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:12] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:23:12 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:23:13 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:23:14 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:15 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:15 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:16 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:17 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:18 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:19 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:19 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:20 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:21 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:22 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:23 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:24 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:24 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:25 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:26 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:27 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:28 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:28 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:23:29] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:23:29 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:23:29 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:23:30 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:23:31 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:23:32 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:33 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:23:33 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:34 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:35 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:36 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:37 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:38 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:23:38 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:39 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:40 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:41 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:42 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:42 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:43 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:44 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:45 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:23:45] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:23:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:23:46 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-10-23 17:23:47 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:47 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:23:48 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:49 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:23:50 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:51 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:52 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:52 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:23:53 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:54 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:55 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:23:56 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:56 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:57 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:23:58 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:23:59 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:00 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:01 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:01 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:24:02] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:24:02 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:24:02 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:24:03 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:04 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:05 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:06 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:06 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:07 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:08 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:24:09 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:10 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:10 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:11 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:12 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:13 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:14 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:15 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:15 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:16 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:17 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:18 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:18] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:24:18 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:24:19 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:24:20 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:20 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:21 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:22 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:24:23 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:24 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:24 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:25 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:26 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:27 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:28 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:28 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:29 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:30 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:31 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:32 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:33 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:33 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:24:34 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:24:35] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:24:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:24:35 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-23 17:24:36 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:24:37 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:38 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:38 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:24:39 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:40 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:41 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:42 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:42 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:43 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:44 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:45 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:46 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:47 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:47 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:24:48 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:49 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:50 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:24:51 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:24:51] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:24:51 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:24:52 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-23 17:24:52 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:53 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:54 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:55 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:56 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:56 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:24:57 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:24:58 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:24:59 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:00 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:01 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:01 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:02 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:03 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:04 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:05 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:05 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:06 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:25:07 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:25:07] INFO:     127.0.0.1:49336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:25:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:25:08 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-10-23 17:25:09 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-10-23 17:25:10 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:10 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:11 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:12 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:13 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:14 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:15 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:15 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:16 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:17 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:18 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:19 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:19 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:20 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:21 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:22 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:23 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:24 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:24] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:25:24 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:25:24 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-10-23 17:25:25 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:26 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:25:27 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:28 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:28 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:29 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:30 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:31 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:32 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:33 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:33 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:34 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:35 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:36 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:37 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:37 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:38 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:39 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:40 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:40] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:25:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:25:41 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-10-23 17:25:42 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:42 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:25:43 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-10-23 17:25:44 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-10-23 17:25:45 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:46 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-10-23 17:25:47 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:47 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:48 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-23 17:25:49 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-10-23 17:25:50 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:51 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:51 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:52 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:53 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:54 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:55 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-10-23 17:25:56 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:25:56 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:57] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:25:57 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:25:57 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-10-23 17:25:58 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-23 17:25:59 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:26:00 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:26:01 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-23 17:26:01 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.30, #queue-req: 0, 
[2025-10-23 17:26:02 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:26:03 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:26:04 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:26:05 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:26:05 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:26:06 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:07 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:26:08 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:09 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-23 17:26:10 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:10 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:11 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:12 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:13 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:13] INFO:     127.0.0.1:33550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:26:13 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:26:14 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-10-23 17:26:15 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:26:15 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:26:16 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:26:17 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:26:18 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:26:19 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:26:19 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:26:20 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:26:21 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:26:22 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:26:23 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:26:24 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:26:24 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:26:25 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:26:26 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:26:27 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:26:28 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:26:28 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:26:29 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:26:30] INFO:     127.0.0.1:40956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:26:30 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:26:30 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:26:31 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:26:32 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:26:33 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:26:33 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:34 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:26:35 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:36 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:37 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:38 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:38 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:39 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:26:40 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:41 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:42 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:42 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:43 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:44 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:45 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:46 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:46] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:26:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:26:47 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:26:47 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:26:48 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:26:49 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:26:50 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:51 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:52 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:26:52 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:26:53 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:26:54 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:26:55 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:26:56 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:26:57 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:26:57 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:26:58 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:26:59 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:00 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:01 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:01 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:02 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:27:03] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:27:03 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:27:03 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-23 17:27:04 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:27:05 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:27:06 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:27:06 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:27:07 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:08 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:09 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:10 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:11 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:11 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:12 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:13 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:14 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:15 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:15 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:16 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:17 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:18 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:19 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:19] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:27:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:27:20 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-23 17:27:20 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:27:21 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:27:22 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:27:23 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:24 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:25 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:25 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:26 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:27 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:28 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:29 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:29 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:30 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:31 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:32 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:33 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:34 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:34 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:35 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:36] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:27:36 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:27:36 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:27:37 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:27:38 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:27:39 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:27:39 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:40 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:41 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:42 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:43 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:44 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:44 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:45 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:46 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:47 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:48 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:48 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:49 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:50 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:51 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:27:52 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:27:52] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:27:52 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:27:53 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:27:53 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:27:54 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:55 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:56 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:57 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:27:58 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:27:58 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:27:59 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:00 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:01 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:02 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:02 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:03 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:04 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:05 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:06 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:07 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:07 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:08 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:09] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:28:09 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:28:09 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:28:10 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:11 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:12 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:12 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:13 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:14 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:15 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:16 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:17 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:17 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:18 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:19 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:20 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:21 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:21 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:22 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:23 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:24 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:28:25 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:25] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:28:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:28:26 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:28:26 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:28:27 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:28:28 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:28:29 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:30 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:31 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:31 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:32 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:33 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:34 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:35 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:35 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:36 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:37 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:38 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:39 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:28:40 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:40 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:28:41 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:28:42] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:28:42 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:28:42 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:28:43 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:28:44 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:45 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:28:45 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:46 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:28:47 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:48 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:49 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:49 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:50 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:28:51 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:52 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:53 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:54 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:54 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:55 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:56 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:28:57 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:58 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:28:58] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:28:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:28:59 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:28:59 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:29:00 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:29:01 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:29:02 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:29:03 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:29:04 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:04 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:05 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:06 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:07 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:08 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:08 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:09 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:10 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:11 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:12 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:13 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:13 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:14 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:29:15] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:29:15 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:29:15 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:29:16 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:17 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:29:18 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:18 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:19 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:20 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:21 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:22 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:22 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:23 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:24 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:25 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:29:26 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:27 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:27 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:29:28 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:29 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:29:30 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:29:31 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:29:31] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:29:31 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:29:32 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:29:32 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:29:33 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:29:34 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:29:35 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:36 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:29:36 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:29:37 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:29:38 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:39 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:40 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:41 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:41 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:42 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:43 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:44 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:45 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:29:45 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:46 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:47 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:29:47] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:29:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:29:48 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:29:49 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:50 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:29:50 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:51 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:52 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:29:53 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:29:54 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:55 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:55 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:56 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:29:57 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:58 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:59 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:29:59 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:00 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:01 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:02 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:03 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:04 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:04] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:30:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:30:05 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:30:05 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:06 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:07 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:08 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:09 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:09 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:10 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:11 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:12 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:13 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:14 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:14 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:15 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:16 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:17 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:18 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:18 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:19 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:30:20 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:20] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:30:20 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:30:21 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:30:22 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:30:23 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:23 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:24 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:30:25 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:26 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:30:27 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:28 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:28 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:29 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:30 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:31 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:32 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:32 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:33 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:34 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:35 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:36 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:37 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:37] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:30:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:30:37 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:30:38 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:39 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:30:40 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:41 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:42 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:42 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:30:43 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:44 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:45 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:46 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:46 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:47 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:48 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:49 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:50 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:51 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:51 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:30:52 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:53 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:53] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:30:53 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:30:54 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:30:55 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:56 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:56 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:30:57 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:30:58 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:30:59 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:00 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:00 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:01 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:02 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:03 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:04 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:31:05 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:05 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:06 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:31:07 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:08 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:31:09 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:09 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:31:10] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:31:10 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:31:10 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:31:11 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-23 17:31:12 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:31:13 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:14 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:31:14 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:15 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:16 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:17 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:18 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:19 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:19 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:20 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:21 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:22 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:23 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:23 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:24 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:25 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:26 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:26] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:31:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:31:27 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-23 17:31:28 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:28 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:29 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:31:30 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:31 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:31:32 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:33 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:31:33 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:34 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:35 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:36 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:37 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:38 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:38 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:39 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:40 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:41 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:42 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:42 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:43] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:31:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:31:43 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-23 17:31:44 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:31:45 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:46 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:31:47 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:31:47 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:31:48 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:31:49 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:31:50 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:51 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:52 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:52 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:53 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:54 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:55 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:56 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:31:56 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:31:57 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:58 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:31:59 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:31:59] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:31:59 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:32:00 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:32:01 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:01 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:02 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:03 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:04 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:05 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:06 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:06 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:07 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:08 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:09 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:10 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:10 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:11 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:12 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:13 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:14 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:15 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:32:15 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:16] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:32:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:32:16 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-23 17:32:17 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:18 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:32:19 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:20 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:20 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:21 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:22 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:23 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:24 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:24 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:25 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:26 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:27 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:28 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:29 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:29 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:30 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:31 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:32 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:32] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:32:32 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:32:33 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:32:34 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:34 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:35 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:36 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:37 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:38 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:39 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:39 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:40 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:41 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:42 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:43 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:43 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:44 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:32:45 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:46 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:47 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:32:48 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:32:48 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:32:49] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:32:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:32:49 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:32:50 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:51 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:52 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:53 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:32:53 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:54 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:32:55 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:56 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:57 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:32:57 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:58 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:32:59 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:00 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:01 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:02 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:02 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:33:03 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:04 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:05 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:05] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:33:05 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:33:06 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:33:07 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:07 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:08 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:09 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:10 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:11 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:12 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:33:12 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:13 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:14 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:15 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:16 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:16 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:17 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:18 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:19 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:20 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:21 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:21 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:22] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:33:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:33:22 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:33:23 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:24 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:25 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:33:26 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:26 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:27 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:28 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:29 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:30 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:30 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:31 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:32 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:33 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:34 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:35 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:35 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:36 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:37 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:38 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:33:38] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:33:38 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:33:39 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:33:40 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:33:40 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:41 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:33:42 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:43 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:33:44 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:44 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:45 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:33:46 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:47 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:33:48 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:49 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:49 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:50 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:33:51 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:52 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:53 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:54 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:33:54 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:33:55] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:33:55 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:33:55 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-23 17:33:56 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:33:57 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:33:58 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:33:59 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:33:59 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:00 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:01 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:02 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:03 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:03 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:04 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:05 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:06 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:07 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:34:08 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:08 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:09 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:10 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:11 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:34:11] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:34:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:34:12 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:34:13 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:34:13 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:14 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:34:15 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:16 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:17 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:17 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:18 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:19 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:20 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:21 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:22 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:22 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:23 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:24 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:25 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:26 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:26 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:27 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:34:28] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:34:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:34:28 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:34:29 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:30 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:31 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:34:31 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:32 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:33 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:34 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:35 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:36 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:36 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:37 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:38 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:39 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:40 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:41 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:34:41 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:42 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:34:43 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:44 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:34:44] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:34:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:34:45 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:34:46 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:34:46 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:34:47 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:48 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:49 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:34:50 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:34:50 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:34:51 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:52 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:34:53 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:54 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:55 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:55 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:56 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:57 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:58 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:34:59 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:34:59 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:00 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:35:01] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:35:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:35:01 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:35:02 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:35:03 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:04 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:35:04 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:05 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:06 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:07 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:08 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:09 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:09 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:10 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:11 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:12 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:13 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:13 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:14 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:15 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:16 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:35:17 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:17] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:35:17 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:35:18 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-23 17:35:18 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:35:19 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:35:20 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:21 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:22 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:23 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:23 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:24 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:25 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:26 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:35:27 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:35:27 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:28 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:29 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:30 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:31 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:32 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:35:32 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:33 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:34] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:35:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:35:34 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:35:35 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:35:36 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:35:37 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:37 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:35:38 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:39 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:40 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:41 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:41 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:42 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:43 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:44 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:45 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:46 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:46 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:47 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:48 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:49 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:50 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:50] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:35:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:35:51 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-23 17:35:51 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:35:52 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:53 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:54 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:55 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:35:56 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:35:56 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:35:57 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:35:58 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:35:59 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:00 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:00 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:01 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:02 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:36:03 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:04 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:05 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:05 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:06 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:07] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:36:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:36:07 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-23 17:36:08 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:36:09 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:36:10 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:36:10 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:36:11 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:12 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:13 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:14 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:14 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:15 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:16 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:17 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:18 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:19 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:19 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:20 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:21 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:22 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:23 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:23] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:36:23 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:36:24 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.79, #queue-req: 0, 
[2025-10-23 17:36:24 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:36:25 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:36:26 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:36:27 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:36:28 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:28 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:29 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:30 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:31 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:32 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:33 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:33 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:34 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:35 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:36 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:37 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:37 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:38 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:39 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:36:39] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:36:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:36:40 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.83, #queue-req: 0, 
[2025-10-23 17:36:41 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:36:42 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:36:42 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:43 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:44 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:45 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:36:46 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:47 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:47 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:48 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:49 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:50 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:36:51 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:51 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:52 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:53 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:54 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:36:55 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:36:56 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:36:56] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:36:56 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:36:56 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-23 17:36:57 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:58 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:36:59 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:37:00 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:37:01 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:37:01 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:02 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:03 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:37:04 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:05 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:37:06 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:06 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:37:07 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:37:08 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:09 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:10 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:10 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:11 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:37:12 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:37:12] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:37:12 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:37:13 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.77, #queue-req: 0, 
[2025-10-23 17:37:14 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:15 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:37:15 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:16 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:17 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:37:18 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:19 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:20 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:20 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:21 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:22 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:23 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:24 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:24 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:25 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:26 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:27 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:28 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:29 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:29] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:37:29 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:37:29 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:37:30 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:31 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:32 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:33 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:34 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:37:34 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:35 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:36 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:37 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:38 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:38 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:39 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:40 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:41 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:42 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:43 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:43 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:44 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:45 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:45] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:37:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:37:46 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-23 17:37:47 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:48 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:48 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:49 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:37:50 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:51 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:52 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:53 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:53 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:37:54 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:37:55 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:37:56 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:57 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:57 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:37:58 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:37:59 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:00 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:01 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:02 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:38:02] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:38:02 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:38:02 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-23 17:38:03 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:38:04 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:38:05 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:38:06 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:38:07 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:38:07 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:38:08 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:38:09 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:10 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:11 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:11 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:12 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:13 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:14 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:15 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:16 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:17 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.24, #queue-req: 0, 
[2025-10-23 17:38:18 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:18 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:19] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:38:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:38:19 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-23 17:38:20 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:21 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:22 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:23 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:23 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:24 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:25 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:26 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:27 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:28 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:28 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:29 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:30 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:31 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:32 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:33 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-23 17:38:33 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:34 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:38:35 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:38:35] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:38:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:38:36 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-23 17:38:37 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:38:38 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:38:38 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:38:39 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:40 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:41 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:42 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:42 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:43 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:44 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:45 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:46 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:47 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:47 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:48 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:49 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:50 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:51 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:38:51 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:38:52] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:38:52 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:38:52 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-23 17:38:53 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:38:54 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:38:55 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:56 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:38:56 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:57 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:58 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:38:59 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:00 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:01 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:01 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:02 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:03 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:04 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:05 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:06 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:06 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:07 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:08 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:08] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:39:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:39:09 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-23 17:39:10 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:11 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:11 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:12 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:13 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:14 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:15 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:15 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:16 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:17 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:18 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:19 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:20 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:20 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:21 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:22 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:23 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:24 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 37.62, #queue-req: 0, 
[2025-10-23 17:39:25 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:25] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:39:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:39:26 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-23 17:39:26 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:27 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:28 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:29 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:30 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:31 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:31 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:32 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:33 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:34 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:35 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:35 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:36 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:37 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:38 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:39 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:40 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:40 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:39:41 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:42] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:39:42 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:39:42 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-23 17:39:43 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:39:44 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:39:45 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:45 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:46 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:47 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:48 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:49 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:49 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:50 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:39:51 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:52 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:53 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:54 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:54 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:39:55 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:56 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:39:57 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:39:58 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:39:58] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:39:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:39:59 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:39:59 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:40:00 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:40:01 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:02 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:40:03 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:40:04 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:04 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:05 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:06 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:07 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:08 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:08 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:09 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:10 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:11 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:12 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:13 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:13 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:40:14 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:40:15] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:40:15 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:40:15 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-23 17:40:16 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:40:17 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:40:18 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:40:18 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:19 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:20 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:21 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:22 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:22 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:23 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:24 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:25 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:26 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:27 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:27 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:28 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:29 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:30 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:40:31 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:31] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:40:31 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:40:32 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:40:32 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:40:33 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:40:34 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:35 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:36 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:37 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:37 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:38 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:39 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:40 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:41 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:41 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:42 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:43 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:44 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:40:45 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:46 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:46 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:47 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:40:48] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:40:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:40:48 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:40:49 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:40:50 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:40:51 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:40:51 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:52 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:53 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:54 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:40:55 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:55 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:40:56 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:57 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:58 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:40:59 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:00 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:00 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:01 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:02 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:03 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:04 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:04] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:41:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:41:05 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:41:05 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:06 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:07 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:08 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:41:09 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:09 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:10 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:41:11 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:12 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:13 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:14 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:14 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:15 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:16 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:17 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:18 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:19 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:19 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:20 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:21] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:41:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:41:21 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:41:22 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:23 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:24 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:24 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:25 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:26 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:27 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:41:28 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:28 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:29 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:30 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:31 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:32 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:33 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:33 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:34 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:35 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:41:36 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:41:37 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:41:37] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:41:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:41:38 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:41:38 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:41:39 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:41:40 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:41:41 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:41:42 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:42 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:43 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:44 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:41:45 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:46 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:47 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:47 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:48 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:41:49 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:50 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:51 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:41:51 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:41:52 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:41:53 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.52, #queue-req: 0, 
[2025-10-23 17:41:53] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:41:54 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:41:54 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:41:55 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:41:56 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:41:56 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:41:57 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:41:58 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:41:59 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:00 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:01 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:01 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:02 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:03 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:04 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.52, #queue-req: 0, 
[2025-10-23 17:42:05 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:42:06 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:06 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:07 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:08 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:09 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:10 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:10] INFO:     127.0.0.1:60710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:42:10 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:42:11 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:42:11 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:12 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:13 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:14 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-23 17:42:15 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:15 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:16 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:17 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:18 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:19 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:20 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:20 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:21 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:22 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:23 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:24 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:24 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:25 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:26 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:26] INFO:     127.0.0.1:34332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:42:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:42:27 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.75, #queue-req: 0, 
[2025-10-23 17:42:28 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:29 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:29 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:30 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:31 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:32 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:33 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:34 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:34 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:35 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:36 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:37 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:38 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:38 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:39 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:40 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:42:41 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:42 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:42:43 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:43] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:42:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:42:43 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:42:44 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:42:45 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:42:46 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:42:47 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:42:48 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:48 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:49 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:50 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:51 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:52 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:52 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:53 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:42:54 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:55 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:42:56 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:57 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:42:57 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:42:58 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:42:59 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:42:59] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:42:59 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:43:00 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-23 17:43:01 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:43:02 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:43:02 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:43:03 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:04 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:05 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:06 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:07 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:07 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:08 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:09 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:10 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:11 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:11 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:12 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:43:13 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:43:14 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:15 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:43:16 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:43:16] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:43:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:43:16 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:43:17 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:43:18 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:43:19 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:43:20 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:43:21 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:21 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:22 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:23 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:43:24 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:25 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:25 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:43:26 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:27 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:28 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:43:29 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:43:30 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:30 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:43:31 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:32 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:32] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:43:32 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:43:33 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.81, #queue-req: 0, 
[2025-10-23 17:43:34 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:43:35 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:43:35 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-23 17:43:36 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-23 17:43:37 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:43:38 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:39 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:43:39 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:43:40 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:43:41 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:43:42 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:43 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:44 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:44 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:45 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:46 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:47 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:43:48 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:43:48 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:43:49] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:43:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:43:49 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-23 17:43:50 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-23 17:43:51 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:43:52 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-23 17:43:53 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:53 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:54 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:55 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:43:56 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:57 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:43:58 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:43:58 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:43:59 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:00 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:44:01 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:02 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:02 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:03 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:04 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:05 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:05] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:44:05 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:44:06 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.02, #queue-req: 0, 
[2025-10-23 17:44:07 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:44:07 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:44:08 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-23 17:44:09 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:10 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-23 17:44:11 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:44:12 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:12 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-23 17:44:13 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:14 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:15 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:16 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:16 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-23 17:44:17 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-23 17:44:18 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:19 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:20 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:21 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:44:21 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-23 17:44:22] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:44:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:44:22 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-23 17:44:23 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:24 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-23 17:44:25 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:44:26 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:44:26 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:44:27 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:44:28 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:29 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:44:30 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:44:31 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:31 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-23 17:44:32 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:33 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:34 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:35 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:35 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:36 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:37 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:38 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:38] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:44:38 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:44:39 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-23 17:44:40 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:40 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:41 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:42 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:44:43 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:44 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:45 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:45 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:46 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:47 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:48 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:49 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:49 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:50 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:44:51 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:52 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:44:53 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:44:54 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:44:54 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:44:55] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:44:55 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:44:55 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-23 17:44:56 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:57 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:58 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:44:59 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:44:59 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:00 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:01 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:02 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:03 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:04 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:04 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:05 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:06 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:45:07 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:08 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:45:08 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:45:09 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:10 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:11 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:11] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:45:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:45:12 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-23 17:45:13 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-23 17:45:13 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:14 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:15 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:16 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:17 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:18 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:18 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:19 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:20 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:21 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:22 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:22 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:23 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:24 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:25 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:26 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:27 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:27 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:45:28] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:45:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:45:28 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-23 17:45:29 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:30 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:31 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:32 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:32 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:33 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:34 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:35 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:36 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-23 17:45:36 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:37 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:38 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:39 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:40 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:41 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:41 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:42 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:43 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:44 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:44] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-23 17:45:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-23 17:45:45 TP0] Decode batch. #running-req: 1, #token: 3223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-23 17:45:46 TP0] Decode batch. #running-req: 1, #token: 3263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:46 TP0] Decode batch. #running-req: 1, #token: 3303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:47 TP0] Decode batch. #running-req: 1, #token: 3343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:48 TP0] Decode batch. #running-req: 1, #token: 3383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:49 TP0] Decode batch. #running-req: 1, #token: 3423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-23 17:45:50 TP0] Decode batch. #running-req: 1, #token: 3463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:51 TP0] Decode batch. #running-req: 1, #token: 3503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-23 17:45:51 TP0] Decode batch. #running-req: 1, #token: 3543, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:52 TP0] Decode batch. #running-req: 1, #token: 3583, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:53 TP0] Decode batch. #running-req: 1, #token: 3623, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:54 TP0] Decode batch. #running-req: 1, #token: 3663, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:55 TP0] Decode batch. #running-req: 1, #token: 3703, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-23 17:45:55 TP0] Decode batch. #running-req: 1, #token: 3743, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:56 TP0] Decode batch. #running-req: 1, #token: 3783, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:57 TP0] Decode batch. #running-req: 1, #token: 3823, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-23 17:45:58 TP0] Decode batch. #running-req: 1, #token: 3863, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:45:59 TP0] Decode batch. #running-req: 1, #token: 3903, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:46:00 TP0] Decode batch. #running-req: 1, #token: 3943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:46:00 TP0] Decode batch. #running-req: 1, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-23 17:46:01] INFO:     127.0.0.1:44196 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-23 17:46:08] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-23 17:46:11] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
