INFO 10-31 10:50:42 __init__.py:179] Automatically detected platform rocm.
WARNING 10-31 10:50:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-31 10:50:42] WARNING server_args.py:1144: Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-31 10:50:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=143143465, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=-1, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-31 10:50:43] Using default HuggingFace chat template with detected content format: string
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-31 10:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
[2025-10-31 10:50:52 TP4] Process 287 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-31 10:50:52 TP6] Process 289 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-10-31 10:50:52 TP0] Process 283 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
INFO 10-31 10:50:52 __init__.py:179] Automatically detected platform rocm.
[2025-10-31 10:50:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-31 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:53 TP2] Process 285 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-10-31 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:53 TP5] Process 288 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-31 10:50:53 TP3] Process 286 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-10-31 10:50:53 TP6] Init torch distributed begin.
[2025-10-31 10:50:53 TP4] Init torch distributed begin.
[2025-10-31 10:50:53 TP0] Init torch distributed begin.
INFO 10-31 10:50:53 __init__.py:179] Automatically detected platform rocm.
INFO 10-31 10:50:53 __init__.py:179] Automatically detected platform rocm.
INFO 10-31 10:50:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-31 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-31 10:50:53 TP2] Init torch distributed begin.
[2025-10-31 10:50:53 TP5] Init torch distributed begin.
[2025-10-31 10:50:53 TP1] Process 284 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-31 10:50:53 TP7] Process 290 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-10-31 10:50:53 TP3] Init torch distributed begin.
[2025-10-31 10:50:53 TP7] Init torch distributed begin.
[2025-10-31 10:50:53 TP1] Init torch distributed begin.
[2025-10-31 10:50:53 TP0] sglang is using nccl==2.21.5
[2025-10-31 10:50:55 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-10-31 10:50:55 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-10-31 10:50:55 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-10-31 10:50:55 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-10-31 10:50:55 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-10-31 10:50:55 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-10-31 10:50:55 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-10-31 10:50:55 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-10-31 10:50:57 TP6] Load weight begin. avail mem=187.31 GB
[2025-10-31 10:50:57 TP4] Load weight begin. avail mem=187.25 GB
[2025-10-31 10:50:57 TP3] Load weight begin. avail mem=187.20 GB
[2025-10-31 10:50:57 TP7] Load weight begin. avail mem=187.32 GB
[2025-10-31 10:50:57 TP1] Load weight begin. avail mem=187.19 GB
[2025-10-31 10:50:57 TP5] Load weight begin. avail mem=187.33 GB
[2025-10-31 10:50:57 TP2] Load weight begin. avail mem=187.19 GB
[2025-10-31 10:50:57 TP0] Load weight begin. avail mem=187.61 GB
[2025-10-31 10:50:57 TP0] Detected fp8 checkpoint.
[2025-10-31 10:50:57 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:35,  4.61it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:46,  3.45it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:46,  3.43it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<00:46,  3.41it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:01<00:48,  3.24it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:45,  3.45it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:37,  4.11it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:02<00:33,  4.57it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:02<00:56,  2.71it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:02<00:46,  3.29it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<00:37,  4.06it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:03<00:31,  4.81it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:03<00:20,  7.23it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:13, 10.94it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:13, 10.60it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:03<00:11, 12.19it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:03<00:10, 13.52it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:03<00:08, 15.78it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:10, 12.48it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:04<00:08, 15.23it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:05<00:16,  7.66it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:05<00:13,  9.60it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:05<00:11, 10.75it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:05<00:10, 11.96it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:05<00:10, 11.33it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:05<00:08, 13.89it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:06<00:07, 15.64it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:06<00:07, 14.47it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:06<00:06, 16.15it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:06<00:07, 14.40it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:06<00:06, 16.33it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:06<00:06, 14.90it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:07<00:13,  7.16it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:07<00:10,  9.38it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:07<00:08, 11.46it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:08<00:07, 12.34it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:08<00:07, 11.88it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:08<00:05, 14.82it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:08<00:06, 13.78it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:08<00:05, 15.83it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:08<00:04, 16.68it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:08<00:04, 17.33it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:08<00:04, 17.23it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:09<00:06, 10.95it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:09<00:05, 13.17it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:09<00:04, 14.25it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:10<00:07,  8.88it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:10<00:12,  5.19it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:11<00:14,  4.34it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:11<00:14,  4.19it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:12<00:17,  3.40it/s]
Loading safetensors checkpoint shards:  64% Completed | 104/163 [00:12<00:18,  3.20it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:13<00:28,  2.03it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:13<00:24,  2.32it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:14<00:23,  2.43it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:14<00:21,  2.57it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:14<00:10,  4.94it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:14<00:06,  7.62it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:14<00:04, 10.27it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:14<00:03, 12.81it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:15<00:02, 15.03it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:15<00:02, 17.44it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:16<00:05,  6.61it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:16<00:06,  5.25it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:17<00:06,  4.32it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:17<00:06,  4.39it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:18<00:07,  3.95it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:18<00:08,  3.11it/s]
Loading safetensors checkpoint shards:  84% Completed | 137/163 [00:19<00:08,  3.24it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:19<00:07,  3.17it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:19<00:09,  2.57it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:20<00:08,  2.78it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:20<00:08,  2.74it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:20<00:06,  3.01it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:21<00:06,  3.26it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:21<00:03,  5.19it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:21<00:02,  7.23it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:21<00:01,  9.38it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:21<00:01, 11.35it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:22<00:02,  4.14it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:22<00:01,  5.04it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:22<00:00,  6.56it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:23<00:00,  8.94it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:23<00:00, 10.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:23<00:00,  7.01it/s]

[2025-10-31 10:51:45 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-31 10:51:45 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-10-31 10:51:45 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-10-31 10:51:46 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-10-31 10:51:48 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-31 10:51:48 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-10-31 10:51:48 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-10-31 10:51:49 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-31 10:51:49 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-31 10:51:49 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP7] Memory pool end. avail mem=43.50 GB
[2025-10-31 10:51:49 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP4] Memory pool end. avail mem=43.42 GB
[2025-10-31 10:51:49 TP1] Memory pool end. avail mem=43.37 GB
[2025-10-31 10:51:49 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP3] Memory pool end. avail mem=43.37 GB
[2025-10-31 10:51:49 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP6] Memory pool end. avail mem=43.49 GB
[2025-10-31 10:51:49 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP2] Memory pool end. avail mem=43.36 GB
[2025-10-31 10:51:49 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-10-31 10:51:49 TP5] Memory pool end. avail mem=43.51 GB
[2025-10-31 10:51:49 TP0] Memory pool end. avail mem=43.78 GB
[2025-10-31 10:51:50 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-10-31 10:51:50 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-31 10:51:50 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
[2025-10-31 10:51:50 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-10-31 10:51:50 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-10-31 10:51:50 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-10-31 10:51:50 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[2025-10-31 10:51:50 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-10-31 10:51:50 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-31 10:51:53 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:53 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:54 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-31 10:51:55 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:51:55 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:55 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:04<04:02,  4.76s/it]Capturing batches (bs=496 avail_mem=42.27 GB):   2%|         | 1/52 [00:04<04:02,  4.76s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.27 GB):   4%|         | 2/52 [00:06<02:34,  3.09s/it]Capturing batches (bs=480 avail_mem=42.25 GB):   4%|         | 2/52 [00:06<02:34,  3.09s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:57 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.25 GB):   6%|         | 3/52 [00:07<01:31,  1.87s/it]Capturing batches (bs=464 avail_mem=42.25 GB):   6%|         | 3/52 [00:07<01:31,  1.87s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.25 GB):   8%|         | 4/52 [00:07<01:02,  1.29s/it]Capturing batches (bs=448 avail_mem=42.24 GB):   8%|         | 4/52 [00:07<01:02,  1.29s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:58 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.24 GB):  10%|         | 5/52 [00:07<00:45,  1.02it/s]Capturing batches (bs=432 avail_mem=42.24 GB):  10%|         | 5/52 [00:07<00:45,  1.02it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.24 GB):  12%|        | 6/52 [00:08<00:36,  1.27it/s]Capturing batches (bs=416 avail_mem=42.23 GB):  12%|        | 6/52 [00:08<00:36,  1.27it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:51:59 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.23 GB):  13%|        | 7/52 [00:08<00:30,  1.50it/s]Capturing batches (bs=400 avail_mem=42.23 GB):  13%|        | 7/52 [00:08<00:30,  1.50it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.23 GB):  15%|        | 8/52 [00:09<00:25,  1.69it/s]Capturing batches (bs=384 avail_mem=42.22 GB):  15%|        | 8/52 [00:09<00:25,  1.69it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.22 GB):  17%|        | 9/52 [00:09<00:21,  2.04it/s]Capturing batches (bs=368 avail_mem=42.22 GB):  17%|        | 9/52 [00:09<00:21,  2.04it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:00 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.22 GB):  19%|        | 10/52 [00:09<00:19,  2.13it/s]Capturing batches (bs=352 avail_mem=42.22 GB):  19%|        | 10/52 [00:09<00:19,  2.13it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.22 GB):  21%|        | 11/52 [00:10<00:18,  2.21it/s]Capturing batches (bs=336 avail_mem=42.21 GB):  21%|        | 11/52 [00:10<00:18,  2.21it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.21 GB):  23%|       | 12/52 [00:10<00:17,  2.26it/s]Capturing batches (bs=320 avail_mem=42.21 GB):  23%|       | 12/52 [00:10<00:17,  2.26it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:01 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.21 GB):  25%|       | 13/52 [00:10<00:15,  2.56it/s]Capturing batches (bs=304 avail_mem=42.20 GB):  25%|       | 13/52 [00:10<00:15,  2.56it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.20 GB):  27%|       | 14/52 [00:11<00:15,  2.50it/s]Capturing batches (bs=288 avail_mem=42.20 GB):  27%|       | 14/52 [00:11<00:15,  2.50it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.20 GB):  29%|       | 15/52 [00:11<00:13,  2.77it/s]Capturing batches (bs=272 avail_mem=42.20 GB):  29%|       | 15/52 [00:11<00:13,  2.77it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:02 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.20 GB):  31%|       | 16/52 [00:12<00:13,  2.61it/s]Capturing batches (bs=256 avail_mem=42.19 GB):  31%|       | 16/52 [00:12<00:13,  2.61it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:03 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.19 GB):  33%|      | 17/52 [00:12<00:13,  2.53it/s]Capturing batches (bs=248 avail_mem=42.18 GB):  33%|      | 17/52 [00:12<00:13,  2.53it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:03 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.18 GB):  35%|      | 18/52 [00:12<00:13,  2.49it/s]Capturing batches (bs=240 avail_mem=42.18 GB):  35%|      | 18/52 [00:12<00:13,  2.49it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.18 GB):  37%|      | 19/52 [00:13<00:13,  2.46it/s]Capturing batches (bs=232 avail_mem=42.17 GB):  37%|      | 19/52 [00:13<00:13,  2.46it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:04 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.17 GB):  38%|      | 20/52 [00:13<00:13,  2.43it/s]Capturing batches (bs=224 avail_mem=42.17 GB):  38%|      | 20/52 [00:13<00:13,  2.43it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.17 GB):  40%|      | 21/52 [00:14<00:12,  2.41it/s]Capturing batches (bs=216 avail_mem=42.16 GB):  40%|      | 21/52 [00:14<00:12,  2.41it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.16 GB):  42%|     | 22/52 [00:14<00:12,  2.40it/s]Capturing batches (bs=208 avail_mem=42.16 GB):  42%|     | 22/52 [00:14<00:12,  2.40it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:05 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.16 GB):  44%|     | 23/52 [00:14<00:10,  2.66it/s]Capturing batches (bs=200 avail_mem=42.15 GB):  44%|     | 23/52 [00:14<00:10,  2.66it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.15 GB):  46%|     | 24/52 [00:15<00:10,  2.56it/s]Capturing batches (bs=192 avail_mem=42.15 GB):  46%|     | 24/52 [00:15<00:10,  2.56it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.15 GB):  48%|     | 25/52 [00:15<00:09,  2.82it/s]Capturing batches (bs=184 avail_mem=42.15 GB):  48%|     | 25/52 [00:15<00:09,  2.82it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:06 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.15 GB):  50%|     | 26/52 [00:15<00:08,  3.04it/s]Capturing batches (bs=176 avail_mem=42.14 GB):  50%|     | 26/52 [00:15<00:08,  3.04it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.14 GB):  52%|    | 27/52 [00:16<00:07,  3.20it/s]Capturing batches (bs=168 avail_mem=42.14 GB):  52%|    | 27/52 [00:16<00:07,  3.20it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.14 GB):  54%|    | 28/52 [00:16<00:08,  2.90it/s]Capturing batches (bs=160 avail_mem=42.14 GB):  54%|    | 28/52 [00:16<00:08,  2.90it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:07 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.14 GB):  56%|    | 29/52 [00:16<00:07,  3.08it/s]Capturing batches (bs=152 avail_mem=42.13 GB):  56%|    | 29/52 [00:16<00:07,  3.08it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.13 GB):  58%|    | 30/52 [00:17<00:07,  2.82it/s]Capturing batches (bs=144 avail_mem=42.13 GB):  58%|    | 30/52 [00:17<00:07,  2.82it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.13 GB):  60%|    | 31/52 [00:17<00:06,  3.04it/s]Capturing batches (bs=136 avail_mem=42.12 GB):  60%|    | 31/52 [00:17<00:06,  3.04it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:08 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.12 GB):  62%|   | 32/52 [00:17<00:06,  3.21it/s]Capturing batches (bs=128 avail_mem=42.12 GB):  62%|   | 32/52 [00:17<00:06,  3.21it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:09 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.12 GB):  63%|   | 33/52 [00:18<00:05,  3.31it/s]Capturing batches (bs=120 avail_mem=42.12 GB):  63%|   | 33/52 [00:18<00:05,  3.31it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.12 GB):  65%|   | 34/52 [00:18<00:06,  2.95it/s]Capturing batches (bs=112 avail_mem=42.12 GB):  65%|   | 34/52 [00:18<00:06,  2.95it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:09 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.12 GB):  67%|   | 35/52 [00:18<00:05,  3.12it/s]Capturing batches (bs=104 avail_mem=42.11 GB):  67%|   | 35/52 [00:18<00:05,  3.12it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.11 GB):  69%|   | 36/52 [00:19<00:05,  2.86it/s]Capturing batches (bs=96 avail_mem=42.11 GB):  69%|   | 36/52 [00:19<00:05,  2.86it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.11 GB):  71%|   | 37/52 [00:19<00:04,  3.06it/s]Capturing batches (bs=88 avail_mem=42.10 GB):  71%|   | 37/52 [00:19<00:04,  3.06it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:10 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.10 GB):  73%|  | 38/52 [00:19<00:04,  2.83it/s]Capturing batches (bs=80 avail_mem=42.10 GB):  73%|  | 38/52 [00:19<00:04,  2.83it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.10 GB):  75%|  | 39/52 [00:20<00:04,  3.04it/s]Capturing batches (bs=72 avail_mem=42.10 GB):  75%|  | 39/52 [00:20<00:04,  3.04it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.10 GB):  77%|  | 40/52 [00:20<00:04,  2.78it/s]Capturing batches (bs=64 avail_mem=42.09 GB):  77%|  | 40/52 [00:20<00:04,  2.78it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-31 10:52:11 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:11 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.09 GB):  79%|  | 41/52 [00:20<00:03,  3.00it/s]Capturing batches (bs=56 avail_mem=42.08 GB):  79%|  | 41/52 [00:20<00:03,  3.00it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.08 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s]Capturing batches (bs=48 avail_mem=42.08 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.08 GB):  83%| | 43/52 [00:21<00:02,  3.01it/s]Capturing batches (bs=40 avail_mem=42.07 GB):  83%| | 43/52 [00:21<00:02,  3.01it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:52:12 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.07 GB):  85%| | 44/52 [00:21<00:02,  2.78it/s]Capturing batches (bs=32 avail_mem=42.07 GB):  85%| | 44/52 [00:21<00:02,  2.78it/s][rank3]:W1031 10:52:18.902000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:18.917000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:18.922000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1031 10:52:18.986000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:18.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:19.000000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:19.001000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:19.005000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:19.007000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:52:19.074000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:19.083000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:19.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:19.116000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:19.132000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:19.137000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1031 10:52:19.199000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:52:19.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1031 10:52:19.213000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:19.218000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:52:19.261000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1031 10:52:19.282000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:52:19.343000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:52:19.412000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:19.476000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1031 10:52:20.819000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:20.832000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:20.835000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:20.909000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:20.915000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:20.921000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:52:21.108000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:21.180000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:22.403000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:22.488000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:22.536000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:22.586000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:22.606000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:22.611000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:22 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:52:22.681000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:22.709000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:22 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:52:22.778000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:22.791000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:22 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:52:22.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:22.866000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:22.871000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:22.876000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:22.942000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:22.946000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:22.951000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:52:22.958000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:22.964000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:23 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:52:23.033000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:23.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:23.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:23.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:23 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:23 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:23 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:52:23.116000 286 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank4]:W1031 10:52:23.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:23 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:52:23.242000 288 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1031 10:52:23.307000 290 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank0]:W1031 10:52:23.515000 283 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank6]:W1031 10:52:23.588000 289 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank1]:W1031 10:52:23.596000 284 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank2]:W1031 10:52:23.606000 285 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank4]:W1031 10:52:23.692000 287 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_7 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1597 seconds and 0.4944 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_3 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3530 seconds and 0.5153 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_11 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2715 seconds and 0.4972 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_5 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_13 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1525 seconds and 0.5130 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_7 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_13 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_5 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2738 seconds and 0.4945 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_15 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2288 seconds and 0.5160 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_7 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2453 seconds and 0.5578 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_15 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_19 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2124 seconds and 0.4692 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1031 10:52:34.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:34.450000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:34.582000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:34.636000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:34.715000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:52:34.933000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:34.949000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:52:34.951000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:35.124000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:35.144000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:35.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:52:35.185000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:52:35.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:52:35.737000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:52:35.739000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:36.605000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1031 10:52:46.272000 286 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:52:46.780000 285 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank1]:W1031 10:52:47.151000 284 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1031 10:52:49.316000 283 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0074 ms 94.0% 
  triton_bmm_41 0.0077 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_33 0.0086 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0087 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0088 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0094 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0094 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.8284 seconds and 0.5238 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 92.3% 
  triton_bmm_41 0.0077 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0086 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0094 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0096 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 9.6827 seconds and 0.5511 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0072 ms 97.2% 
  triton_bmm_41 0.0078 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0082 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0087 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0089 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0091 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0094 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0095 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.3911 seconds and 0.5402 seconds precompiling for 27 choices
[rank2]:W1031 10:52:58.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:58.243000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:58.248000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:58.257000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:52:58.317000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:58.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:52:58.354000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:58 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:52:58.413000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:52:58.418000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:58 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:52:58 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0069 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0073 ms 95.1% 
  triton_bmm_41 0.0074 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0081 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_49 0.0095 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0096 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 9.0405 seconds and 0.5232 seconds precompiling for 27 choices
[rank0]:W1031 10:53:00.058000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:00.132000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:00.229000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:00 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:02 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:02 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:02 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:53:03.944000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:04.019000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:04 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:53:04.097000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:04.129000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:04.173000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:04 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:53:04.271000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:04 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:53:04.510000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:04.584000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:04.681000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:04 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:53:05.177000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:05.251000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:05.348000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:05 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:53:06.877000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:07.005000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:07.109000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:07.233000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:07.393000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:07.509000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:07.549000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:07 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:53:07.777000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:07 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:53:08.053000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:08 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:53:08.789000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:08.968000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:09.033000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:09.042000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:09.088000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:09.141000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:09.163000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:09 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:53:09.262000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1031 10:53:09.313000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:09 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:53:09.647000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:09 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:53:09.730000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:09.837000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:09 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1031 10:53:11.358000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:11.432000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:11.529000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:53:11 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_55 0.0335 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0491 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0686 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0687 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0697 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0713 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5027 seconds and 0.3776 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_55 0.0320 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0656 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0683 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0713 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5900 seconds and 0.3849 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_55 0.0334 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0397 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0488 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0623 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0681 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0686 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0714 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5737 seconds and 0.3744 seconds precompiling for 27 choices
[rank1]:W1031 10:53:16.772000 284 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank1]:W1031 10:53:16.816000 284 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:53:16 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 10:53:16.987000 286 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1031 10:53:17.018000 286 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:53:17 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:53:17.674000 285 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 10:53:17.705000 285 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:53:17 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_55 0.0312 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0419 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0526 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0626 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_74 0.0710 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0720 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0729 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.5946 seconds and 0.3758 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1031 10:53:18.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:18.518000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:18.553000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:18.750000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:18.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:18.982000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:19.018000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:19.214000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:19.226000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:19.262000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:19.399000 283 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 10:53:19.430000 283 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[rank2]:W1031 10:53:19.458000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:19.470000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:19.505000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:53:19 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1031 10:53:19.695000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:19.702000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1031 10:53:19.738000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:19.930000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:19.935000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:19.970000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:20.166000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:20.170000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:20.206000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:20.398000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:20.402000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:20.442000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:20.634000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:20.639000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:20.750000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:20.874000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:20.914000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:20.930000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:20.986000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:21.110000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:21.149000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:21.166000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:21.218000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:21.346000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:21.381000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:21.398000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:21.450000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:21.618000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:21.634000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:21.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:21.686000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:21.853000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:21.866000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:21.878000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:21.918000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:22.086000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:22.098000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:22.114000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:22.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:22.334000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:22.343000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:22.363000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:22.395000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:22.566000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:22.578000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:22.598000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:22.626000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:22.809000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:22.822000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:22.834000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:22.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:23.058000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:23.070000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:23.094000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:23.102000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:23.294000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:23.306000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:23.330000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:23.337000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:23.526000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:23.542000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:23.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:23.570000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:23.757000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:23.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:23.794000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:23.801000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:23.990000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:24.006000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:24.026000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:24.034000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:24.226000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:24.242000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:24.270000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:24.338000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:24.482000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:24.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:24.527000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:24.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:24.718000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:24.741000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:24.762000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:24.814000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:24.961000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:24.985000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:25.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:25.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:25.221000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:25.241000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:25.262000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:25.281000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:25.458000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:25.474000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:25.498000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:25.518000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:25.693000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:25.710000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:25.734000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:25.754000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:25.929000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:25.946000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:25.970000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:25.986000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:26.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:26.178000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:26.206000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:26.218000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:26.401000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:26.410000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:26.442000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:26.450000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:26.646000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:26.682000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:26.687000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:26.710000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:26.882000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:26.918000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:26.922000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:26.945000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:27.118000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:27.154000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:27.159000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:27.182000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:27.349000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:27.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:27.395000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:27.413000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:27.594000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:27.630000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:27.635000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:27.646000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:27.830000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:27.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:27.870000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:27.881000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:28.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:28.102000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:28.122000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:28.178000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:28.342000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:28.362000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:28.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:28.418000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:28.582000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:28.598000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:28.610000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:28.654000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:28.818000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:28.833000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:28.846000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:28.890000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:29.073000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:29.081000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:29.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:29.142000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:29.309000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:29.317000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:29.373000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:29.378000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:29.546000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:29.554000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:29.610000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:29.615000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:29.781000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:29.790000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:29.850000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:29.854000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:30.017000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:30.026000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:30.086000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:30.090000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:30.254000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:30.262000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:30.323000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:30.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:30.501000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:30.562000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:30.567000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:30.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:30.738000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:30.802000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:30.808000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:30.812000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:30.986000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:31.042000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:31.046000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:31.051000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:31.234000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:31.290000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:31.296000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:31.300000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:31.469000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:31.525000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:31.534000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:31.538000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:31.706000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:31.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:31.770000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:53:31.775000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:53:31.942000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:31.997000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:32.007000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:32.234000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:32.247000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:32.473000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:32.488000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:32.726000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:53:32.739000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:32.979000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:33.221000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:33.461000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:33.702000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:33.949000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:34.193000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:53:34.429000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_81 0.1041 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1142 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1150 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1167 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1221 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1569 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1571 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6152 seconds and 0.3347 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1047 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1131 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1152 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1154 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1167 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1217 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1572 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1573 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6232 seconds and 0.3490 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_81 0.1036 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1057 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1114 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1138 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1138 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1153 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1199 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1566 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1567 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5653 seconds and 0.3326 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_81 0.1037 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1108 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1150 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1150 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1158 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1187 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1565 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1577 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5345 seconds and 0.3375 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1031 10:54:51.548000 290 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0074 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0093 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0094 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_37 0.0095 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 10.8562 seconds and 0.0001 seconds precompiling for 27 choices
[rank7]:W1031 10:55:03.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:03.460000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:03.561000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:55:03 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:55:08 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:55:09.272000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:09.347000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:09.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:55:09 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:55:12.601000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:12.881000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:13.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:55:13 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:55:14.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:14.637000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:14.737000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:55:14 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0331 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0491 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0652 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0689 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0716 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5972 seconds and 0.0001 seconds precompiling for 27 choices
[rank7]:W1031 10:55:26.208000 290 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 10:55:26.239000 290 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:55:26 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1031 10:55:27.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:27.973000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:28.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:28.437000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:28.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:28.901000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:29.133000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:29.421000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:29.653000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:29.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:30.117000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:30.349000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:30.581000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:30.817000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:31.049000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:31.281000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:31.513000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:31.745000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:31.977000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:32.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:32.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:32.737000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:32.973000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:33.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:33.437000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:33.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:33.901000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:34.133000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:34.365000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:34.597000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:34.833000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:35.069000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:35.301000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:35.533000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:35.765000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:35.997000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:36.229000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:36.533000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:36.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:37.005000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:37.241000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:37.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:37.705000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:37.937000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:38.169000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:38.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:38.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:38.867000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:39.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:39.333000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:39.569000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:39.805000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:40.037000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:40.273000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:40.585000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:40.825000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:55:41.061000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_81 0.1017 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1025 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1109 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1112 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1113 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1123 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1179 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_98 0.1464 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1464 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.4502 seconds and 0.0001 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:55:53.433000 289 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1031 10:55:54.290000 287 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank5]:W1031 10:55:54.408000 288 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0068 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0075 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0090 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0093 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 10.7590 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0068 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0074 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0078 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0086 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0087 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 10.7708 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0069 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0073 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0087 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0089 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0089 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0095 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 10.7236 seconds and 0.0001 seconds precompiling for 27 choices
[rank6]:W1031 10:56:05.172000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:05.246000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:05.343000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:05 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:56:06.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:06.109000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:06.119000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:06.194000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:06.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:06 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:56:06.292000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:06 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:09 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:10 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:10 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:56:10.468000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:10.541000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:10.638000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:10 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:56:11.187000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:11.260000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:11.289000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:11.358000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:11.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:11 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:56:11.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:11 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:56:13.377000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:13.605000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:13.836000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:14 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:56:14.789000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:14.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:15.021000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:15.149000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:15.297000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:15.328000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:15.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:15.433000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:15.500000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:15 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:15 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:15 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:56:16.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:16.971000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:17.034000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:17.069000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:17.109000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:17 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:56:17.206000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:56:17 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0288 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0453 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0488 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0579 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0677 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0682 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0705 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5393 seconds and 0.0000 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0303 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0487 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0578 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0677 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0681 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0683 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.4244 seconds and 0.0001 seconds precompiling for 27 choices
[rank6]:W1031 10:56:26.858000 289 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 10:56:26.889000 289 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0293 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0492 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0585 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0683 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0688 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.4332 seconds and 0.0000 seconds precompiling for 27 choices
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:56:27 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:56:28.389000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:28.467000 288 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 10:56:28.500000 288 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[rank4]:W1031 10:56:28.605000 287 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank6]:W1031 10:56:28.629000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:28.637000 287 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:56:28 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:56:28 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:56:28.865000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1031 10:56:29.101000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:29.333000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:29.569000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:29.805000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:30.009000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:30.037000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:30.137000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:30.245000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:30.269000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:30.369000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:30.477000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:30.501000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:30.601000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:30.713000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:30.790000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:30.837000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:30.949000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:31.021000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:31.069000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:31.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:31.253000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:31.301000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:31.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:31.485000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:31.533000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:31.649000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:31.721000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:31.765000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:31.881000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:31.953000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:31.997000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:32.173000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:32.189000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:32.289000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:32.415000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:32.423000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:32.523000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:32.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:32.658000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:32.758000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:32.890000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:32.894000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:32.990000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:33.126000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:33.130000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:33.222000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:33.362000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:33.366000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:33.454000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:33.595000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:33.602000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:33.685000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:33.827000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:33.834000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:33.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:34.063000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:34.070000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:34.150000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:34.306000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:34.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:34.386000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:34.547000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:34.595000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:34.622000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:34.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:34.829000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:34.854000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:35.018000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:35.063000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:35.090000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:35.254000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:35.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:35.322000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:35.490000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:35.534000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:35.555000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:35.777000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:35.789000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:35.853000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:36.015000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:36.027000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:36.091000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:36.250000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:36.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:36.326000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:36.486000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:36.498000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:36.562000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:36.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:36.733000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:36.797000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:36.951000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:36.966000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:37.031000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:37.186000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:37.206000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:37.270000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:37.422000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:37.446000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:37.510000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:37.658000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:37.682000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:37.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:37.893000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:37.919000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:37.979000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:38.158000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:38.199000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:38.218000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:38.398000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:38.438000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:38.454000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:38.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:38.676000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:38.691000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:38.869000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:38.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:38.925000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:39.107000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:39.147000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:39.159000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:39.342000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:39.382000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:39.394000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:39.622000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:39.654000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:39.703000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:39.861000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:39.893000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:39.937000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:40.099000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:40.130000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:40.175000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:40.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:40.366000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:40.410000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:40.570000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:40.602000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:40.646000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:40.806000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:40.838000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:40.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:41.042000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:41.074000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:41.118000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:41.277000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:41.309000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:41.353000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:41.513000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:41.545000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:41.589000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:56:41.749000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:41.781000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:41.821000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:42.017000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:42.053000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:42.253000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:42.286000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:42.493000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:42.525000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:42.731000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:42.763000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:42.970000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:43.002000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:43.210000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:43.238000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:56:43.449000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:56:43.473000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1142 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1149 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1164 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1203 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1567 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1569 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5313 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1041 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1148 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1159 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1160 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1166 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1216 ms 50.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1565 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1566 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5000 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_81 0.1038 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1058 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1116 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1144 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1161 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1198 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1561 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1566 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.5952 seconds and 0.0001 seconds precompiling for 27 choices
Capturing batches (bs=32 avail_mem=42.07 GB):  87%| | 45/52 [05:05<09:57, 85.30s/it]Capturing batches (bs=24 avail_mem=41.45 GB):  87%| | 45/52 [05:05<09:57, 85.30s/it][rank3]:W1031 10:57:00.737000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:00.762000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:00.766000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:00.779000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:00.811000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:00.837000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:00.841000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:00.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:00.882000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:00.914000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:00.940000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:00.945000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:00.956000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:00.959000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:00.978000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:01.006000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:01.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:01.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:01.080000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:01.157000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:01.183000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:01.439000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:01.513000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:01.617000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:02.412000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:02.433000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:02.444000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:02.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:02.578000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:02.656000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:02.688000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:03.109000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:03.762000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:03.837000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:03.935000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:04.193000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:04.223000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:04.269000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:04.300000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:04.367000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:04.396000 286 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1031 10:57:04.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:04.513000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:04.604000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:04.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:04.705000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:04.713000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:04.797000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:04.812000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:04.855000 285 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank7]:W1031 10:57:04.873000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:04.893000 283 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank7]:W1031 10:57:04.979000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:05.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:05.176000 284 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank5]:W1031 10:57:05.235000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:05.292000 289 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank5]:W1031 10:57:05.344000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:05.466000 290 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank4]:W1031 10:57:05.705000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:05.781000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:05.835000 288 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank4]:W1031 10:57:05.879000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:06.353000 287 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_129 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.8% 
  triton_bmm_126 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_127 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_113 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2903 seconds and 0.3684 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_106 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_110 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_127 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2734 seconds and 0.3765 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_123 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_111 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_129 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3188 seconds and 0.3803 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_119 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_107 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_110 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_109 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2782 seconds and 0.3727 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_119 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_111 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_127 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3257 seconds and 0.3695 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_117 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_107 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_127 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_115 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_123 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3400 seconds and 0.3708 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_107 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_111 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_127 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_123 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3255 seconds and 0.3738 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_107 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 99.4% 
  triton_bmm_117 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_121 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_123 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_127 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2954 seconds and 0.3674 seconds precompiling for 27 choices
[rank3]:W1031 10:57:15.520000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:15.813000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:15.949000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:16.017000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:16.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:16.312000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:16.444000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:16.467000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:16.555000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:16.960000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:17.071000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:17.094000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:17.559000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:17.569000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:17.601000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:18.064000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:18.355000 285 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank3]:W1031 10:57:18.363000 286 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank0]:W1031 10:57:18.597000 283 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank1]:W1031 10:57:19.140000 284 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank6]:W1031 10:57:19.931000 289 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank7]:W1031 10:57:20.114000 290 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1031 10:57:20.266000 288 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank4]:W1031 10:57:20.612000 287 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0078 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0085 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0086 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0087 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0093 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.1999 seconds and 0.5255 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0092 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2341 seconds and 0.5321 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0069 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0084 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0091 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0093 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0096 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0096 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2167 seconds and 0.5318 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0087 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0089 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0091 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0095 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0095 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2466 seconds and 0.5227 seconds precompiling for 27 choices
[rank3]:W1031 10:57:25.074000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:25.148000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:25.172000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:25.195000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:25.247000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:25.295000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:25.295000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:25.370000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:25.418000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0069 ms 100.0% 
  triton_bmm_133 0.0069 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0085 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0087 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2582 seconds and 0.5226 seconds precompiling for 27 choices
[rank1]:W1031 10:57:25.864000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:25.939000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_133 0.0068 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0089 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2715 seconds and 0.5195 seconds precompiling for 27 choices
[rank1]:W1031 10:57:25.988000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0068 ms 100.0% 
  triton_bmm_133 0.0069 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0074 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0080 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0084 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0087 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0088 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0095 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2816 seconds and 0.5392 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 85.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0089 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0093 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0929 seconds and 0.5429 seconds precompiling for 27 choices
[rank6]:W1031 10:57:26.647000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:26.722000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:26.770000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:26.855000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:26.932000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:26.982000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:27.057000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:27.132000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:27.180000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:27.185000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:27.261000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:27.309000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:30.378000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:30.453000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:30.562000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:31.261000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:31.337000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:31.437000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:31.440000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:31.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:31.614000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:31.900000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:31.943000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:31.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:32.017000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:32.084000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:32.117000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:32.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:32.450000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:32.550000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:33.220000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:33.296000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:33.311000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:33.395000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:33.552000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:33.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:33.936000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:34.011000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:34.109000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:34.570000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:34.778000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:34.806000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:35.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:35.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:35.215000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:35.247000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:35.259000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:35.322000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:35.420000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:35.459000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:35.546000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:35.699000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:35.795000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:35.979000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:36.043000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:36.227000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:36.470000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:36.707000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:36.854000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:36.884000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:36.950000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:36.960000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:37.057000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:37.089000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:37.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:37.154000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:37.165000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:37.195000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:37.229000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:37.264000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:37.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:37.348000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:37.498000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:37.577000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:37.682000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:38.073000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:38.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:38.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:38.808000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:38.827000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:38.884000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:38.904000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:38.983000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:39.003000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_159 0.0327 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0488 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0620 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0694 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0711 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4946 seconds and 0.3762 seconds precompiling for 27 choices
[rank3]:W1031 10:57:43.095000 286 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 10:57:43.118000 286 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:43 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0316 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0613 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0679 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0693 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0710 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6419 seconds and 0.3810 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_159 0.0309 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0420 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0454 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0533 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0643 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_170 0.0710 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0715 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_181 0.0722 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_179 0.0741 ms 12.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5559 seconds and 0.3916 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0313 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0403 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0495 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0601 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0690 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0698 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0700 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6303 seconds and 0.3777 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0295 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0587 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0680 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0687 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0694 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0707 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6656 seconds and 0.3779 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0290 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0404 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0489 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0592 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0683 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0686 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0695 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0708 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5625 seconds and 0.3742 seconds precompiling for 27 choices
[rank2]:W1031 10:57:44.788000 285 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 10:57:44.811000 285 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:44 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 10:57:44.898000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:45.025000 284 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank0]:W1031 10:57:45.033000 283 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank1]:W1031 10:57:45.049000 284 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[rank0]:W1031 10:57:45.056000 283 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:45 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:45 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 10:57:45.144000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0322 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0402 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_181 0.0681 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0688 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0692 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0709 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5717 seconds and 0.3996 seconds precompiling for 27 choices
[rank3]:W1031 10:57:45.383000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0288 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0406 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0496 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0591 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0680 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0688 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0693 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6266 seconds and 0.4127 seconds precompiling for 27 choices
[rank6]:W1031 10:57:45.427000 289 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 10:57:45.450000 289 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:45 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 10:57:45.635000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:45.888000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:46.130000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:46.142000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:46.173000 288 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 10:57:46.195000 288 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:46 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 10:57:46.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:46.384000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:46.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:46.560000 287 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank4]:W1031 10:57:46.583000 287 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[rank3]:W1031 10:57:46.614000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:46.631000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:46.637000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:46 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1031 10:57:46.787000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:46.814000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:46.854000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:46.874000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:46.881000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:46.934000 290 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 10:57:46.957000 290 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:57:47 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1031 10:57:47.031000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:47.058000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:47.094000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:47.118000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:47.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:47.271000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:47.298000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:47.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:47.370000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:47.376000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:47.515000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:47.538000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:47.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:47.615000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:47.621000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:47.754000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:47.779000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:47.823000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:47.859000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:47.866000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:47.911000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:47.971000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:47.998000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:48.031000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:48.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:48.098000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:48.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:48.155000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:48.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:48.238000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:48.254000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:48.271000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:48.303000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:48.339000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:48.347000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:48.395000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:48.462000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:48.482000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:48.498000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:48.511000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:48.543000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:48.580000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:48.587000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:48.639000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:48.706000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:48.722000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:48.742000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:48.751000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:48.783000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:48.818000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:48.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:48.882000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:48.950000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:48.962000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:48.987000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:48.992000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:49.023000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:49.058000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:49.067000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:49.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:49.195000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:49.202000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:49.231000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:49.237000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:49.267000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:49.298000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:49.307000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:49.367000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:49.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:49.447000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:49.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:49.483000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:49.511000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:49.538000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:49.547000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:49.608000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:49.684000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:49.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:49.715000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:49.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:49.750000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:49.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:49.787000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:49.848000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:49.923000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:49.931000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:49.963000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:49.972000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:49.990000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:50.022000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:50.030000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:50.088000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:50.165000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:50.171000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:50.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:50.217000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:50.270000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:50.278000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:50.327000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:50.370000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:50.408000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:50.413000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:50.459000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:50.464000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:50.516000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:50.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:50.567000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:50.619000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:50.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:50.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:50.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:50.718000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:50.766000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:50.807000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:50.866000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:50.892000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:50.898000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:50.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:50.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:50.962000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:51.014000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:51.047000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:51.110000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:51.132000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:51.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:51.162000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:51.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:51.206000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:51.263000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:51.287000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:51.356000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:51.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:51.412000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:51.427000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:51.503000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:51.530000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:51.596000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:51.603000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:51.635000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:51.654000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:51.664000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:51.672000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:51.748000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:51.771000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:51.842000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:51.847000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:51.879000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:51.902000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:51.911000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:51.918000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:51.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:52.011000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:52.086000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:52.091000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:52.124000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:52.151000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:52.160000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:52.165000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:52.236000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:52.336000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:52.342000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:52.371000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:52.378000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:52.399000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:52.407000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:52.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:52.483000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:52.583000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:52.589000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:52.614000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:52.622000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:52.643000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:52.651000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:52.659000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:52.728000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:52.826000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:52.832000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:52.859000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:52.867000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:52.887000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:52.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:52.902000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:52.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:53.086000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:53.091000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:53.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:53.146000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:53.158000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:53.228000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:53.259000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:53.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:53.331000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:53.339000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:53.363000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:53.395000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:53.403000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:53.475000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:53.507000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:53.539000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:53.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:53.583000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:53.606000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:53.638000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:53.647000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:53.724000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:53.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:53.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:53.819000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:53.826000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:53.851000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:53.883000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:53.891000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:53.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:54.003000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:54.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:54.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:54.070000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:54.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:54.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:54.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:54.219000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:54.247000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:54.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:54.306000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:54.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:54.339000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:54.378000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:54.386000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:54.463000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:54.491000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:54.523000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:54.550000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:54.558000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:54.579000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:54.626000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:54.634000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:54.707000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:54.735000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:54.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:54.794000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:54.802000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:54.819000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:54.875000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:54.882000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:54.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:54.979000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:55.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:55.051000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:55.058000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:55.064000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:55.134000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:55.139000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:55.200000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:55.223000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:55.255000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:55.294000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:55.302000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:55.308000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:55.382000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:55.387000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:55.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:55.467000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:55.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:55.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:55.556000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:55.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:55.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:55.635000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:55.691000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:55.711000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:55.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:55.791000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:55.802000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:55.807000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:55.878000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:55.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:55.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:55.955000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:55.988000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:56.031000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:56.047000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:56.052000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:56.126000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:56.131000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:56.179000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:56.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:56.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:56.271000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:56.294000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:56.299000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:56.374000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:56.379000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:56.423000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:56.443000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:56.476000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:56.511000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:56.538000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:56.543000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:56.622000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:56.628000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:56.668000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:56.688000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:56.720000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:56.756000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:56.790000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:56.874000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:56.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:56.914000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:56.932000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:56.939000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:56.964000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:56.995000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:57.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:57.138000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:57.143000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:57.159000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:57.175000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:57.186000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:57.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:57.236000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:57.295000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:57.390000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:57.404000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:57.419000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:57.447000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:57.453000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:57.475000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:57.554000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:57.560000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:57.642000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:57.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:57.663000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:57.694000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:57.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:57.715000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:57.798000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:57.810000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:57.891000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:57.912000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:57.944000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:57.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:57.964000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:58.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:58.067000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:58.164000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:58.199000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:58.204000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:58.211000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:58.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:58.288000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:58.294000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:58.318000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:58.411000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:58.451000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:58.457000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:58.462000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:58.468000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:58.532000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:58.546000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:58.571000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:58.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:58.702000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:58.710000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:58.715000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:58.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:58.798000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:58.823000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:58.859000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:58.911000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:58.951000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:58.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:57:58.965000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:59.032000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:59.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:59.074000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:59.107000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:59.159000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:59.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:59.213000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:59.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:59.302000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:59.327000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:59.351000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:59.407000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:59.452000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:59.462000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:59.528000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:59.554000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:59.579000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:59.600000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:59.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:57:59.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:57:59.815000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:57:59.824000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:57:59.839000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:57:59.852000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:57:59.865000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:57:59.963000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:00.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:00.070000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:00.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:00.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:00.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:00.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:00.214000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:00.291000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:00.322000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:00.340000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:00.350000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:00.356000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:00.372000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:00.462000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:00.539000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:00.570000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:00.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:00.603000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:00.620000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:00.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:00.791000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:00.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:00.855000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:00.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:00.958000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:01.097000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:01.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:01.121000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:01.356000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:01.363000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:01.377000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:01.620000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:01.627000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:01.637000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:01.876000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:01.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:01.896000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:02.148000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:02.155000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:02.403000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1032 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1055 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1142 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1149 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1158 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1210 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1561 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1563 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6904 seconds and 0.3622 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_185 0.1024 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1056 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1113 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1133 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1140 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1149 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1190 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1559 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_194 0.1559 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.5874 seconds and 0.3593 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_185 0.1025 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1058 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1094 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1140 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1146 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1153 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1181 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1557 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1569 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5894 seconds and 0.3682 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_185 0.1027 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1059 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1137 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1146 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1156 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1160 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1207 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1563 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1565 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6187 seconds and 0.3487 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_185 0.1027 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1058 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1136 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1146 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1155 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1160 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1206 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1564 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1565 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6159 seconds and 0.3576 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_185 0.1025 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1056 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1114 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1152 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1155 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1159 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1197 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1561 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1562 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6268 seconds and 0.3478 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_185 0.1026 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1056 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1150 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1159 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.1159 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_204 0.1164 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1225 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1562 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1563 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6625 seconds and 0.3502 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1000 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1025 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1109 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1116 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1120 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1128 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1169 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1464 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1465 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6289 seconds and 0.3560 seconds precompiling for 27 choices
Capturing batches (bs=24 avail_mem=41.45 GB):  88%| | 46/52 [06:20<08:13, 82.21s/it]Capturing batches (bs=16 avail_mem=40.85 GB):  88%| | 46/52 [06:20<08:13, 82.21s/it][rank7]:W1031 10:58:15.859000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:15.935000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:16.038000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:16.042000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:16.113000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:16.217000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:16.584000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:16.607000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:16.651000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:16.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:16.665000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:16.683000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:16.726000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:16.741000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:16.766000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:16.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:16.831000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:16.857000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:16.916000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:16.991000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:17.067000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:17.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:17.142000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:17.250000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:17.563000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:17.753000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:18.281000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:18.321000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:18.333000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:18.388000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:18.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:18.765000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:19.535000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:19.613000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:19.713000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:19 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:58:20.160000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:20.168000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:20.175000 285 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank7]:W1031 10:58:20.238000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:20.245000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:20.284000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:20.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:20.347000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:20.350000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:20.364000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:20.380000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:20.457000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:20.465000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:20.482000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:20.545000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:20.584000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:20.645000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:20.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:58:20.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:20.811000 287 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank7]:W1031 10:58:20.811000 290 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:20 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:58:20.940000 288 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank3]:W1031 10:58:20.944000 286 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank0]:W1031 10:58:21.127000 283 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank6]:W1031 10:58:21.235000 289 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank1]:W1031 10:58:21.590000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:21.666000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:21.765000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:21 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:58:22.222000 284 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_226 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_224 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_225 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_230 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_215 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_219 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_222 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8185 seconds and 0.1353 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_225 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_215 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_220 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8668 seconds and 0.1381 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_211 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_219 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_218 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_215 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_225 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8884 seconds and 0.1521 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_214 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0063 ms 100.0% 
  triton_bmm_210 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_219 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_215 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8178 seconds and 0.1489 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_225 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_218 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_214 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8490 seconds and 0.1572 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_226 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_215 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_209 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_213 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_210 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_229 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_218 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8102 seconds and 0.1526 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_209 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_215 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_229 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_213 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_218 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8391 seconds and 0.1462 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_215 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_209 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_211 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_213 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_219 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_224 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_225 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8282 seconds and 0.1495 seconds precompiling for 25 choices
[rank2]:W1031 10:58:30.604000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:31.023000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:31.092000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:31.101000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:31.512000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:31.588000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:31.695000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:31.820000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:32.135000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:32.149000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:32.193000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:32.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:32.429000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:32.638000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:32.670000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:32.934000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:33.870000 286 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank7]:W1031 10:58:33.987000 290 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1031 10:58:34.116000 285 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank6]:W1031 10:58:34.976000 289 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank1]:W1031 10:58:35.572000 284 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank4]:W1031 10:58:35.877000 287 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank0]:W1031 10:58:35.949000 283 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank5]:W1031 10:58:36.827000 288 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.8% 
  triton_bmm_245 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0080 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0080 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0084 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7944 seconds and 0.4386 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0068 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0073 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0085 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7719 seconds and 0.4472 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0085 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7435 seconds and 0.4257 seconds precompiling for 25 choices
[rank3]:W1031 10:58:40.193000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:40.245000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:40.268000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.8% 
  triton_bmm_234 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_244 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_236 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8416 seconds and 0.3921 seconds precompiling for 25 choices
[rank2]:W1031 10:58:40.322000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:40.366000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:40.421000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:40 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:40 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:58:40.602000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:40.678000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:40.776000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:40 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_234 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0071 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0078 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0079 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0084 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0084 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0086 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8952 seconds and 0.4038 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 97.6% 
  triton_bmm_245 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0083 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8343 seconds and 0.4241 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.2% 
  triton_bmm_244 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0077 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0077 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0086 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8213 seconds and 0.4114 seconds precompiling for 25 choices
[rank6]:W1031 10:58:41.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:41.342000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:41.442000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:41 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:58:41.818000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:41.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:41.994000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:42 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_234 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0070 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0071 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0074 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0083 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0083 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_237 0.0085 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7420 seconds and 0.4154 seconds precompiling for 25 choices
[rank4]:W1031 10:58:42.182000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:42.259000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:42.261000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:42.338000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:42.359000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:42 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:42.447000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:42 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:58:42.942000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:43.020000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:43.119000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:43 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:44 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:44 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:44 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:45 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:58:45.816000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:45 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:58:45.891000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:45.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:45 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:58:46.118000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:46 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1031 10:58:46.204000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:46.253000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:46.288000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:46 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:46.362000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:46.411000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:46 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:46 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:58:47.008000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:47.082000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:47.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:47.139000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:47 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:58:47.214000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:47.264000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:47 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:58:47.401000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:47.476000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:47.524000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:47 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:47 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:48.129000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:48.215000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:48.263000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:48 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1031 10:58:48.671000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:48.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:48.918000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:48.988000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:49.020000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:49.037000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:49 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:49.169000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:49.175000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:49.267000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:58:49.418000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:49.523000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:49 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:49.670000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:49 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:50 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:58:50.430000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:50.609000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:50.686000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:50.782000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:50.857000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:50.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:50.936000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:50.955000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:50.987000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:58:51.108000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:51.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:51.214000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:51.235000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:51.295000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:51.314000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:51.374000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:51.452000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1031 10:58:51.478000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:51.488000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:51.711000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:51.796000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:51 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:51.972000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:52.052000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:52.309000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:52 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:58:52.550000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:58:52.627000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:52 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1031 10:58:52.729000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:52.735000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:52 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1031 10:58:52.811000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:52.909000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:52 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1031 10:58:53.099000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:53.178000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:58:53.283000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:53 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1031 10:58:53.594000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:53.672000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:58:53.779000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:53 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1031 10:58:54.378000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:54.454000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:58:54.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-31 10:58:54 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_268 0.0309 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0313 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0319 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0338 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0465 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0469 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0504 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0505 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_262 0.0551 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0606 seconds and 0.2205 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_259 0.0307 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0311 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0313 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0321 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0468 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0491 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0505 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0506 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0547 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0154 seconds and 0.2197 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_268 0.0307 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0308 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0340 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0343 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0467 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0467 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_277 0.0514 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0570 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0834 seconds and 0.2196 seconds precompiling for 25 choices
[rank7]:W1031 10:58:57.908000 290 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 10:58:57.930000 290 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:58:58 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 10:58:58.235000 285 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 10:58:58.257000 285 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:58:58 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0303 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0308 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0308 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0321 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0464 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0517 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0541 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0833 seconds and 0.2305 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0099 ms 100.0% 
  triton_mm_259 0.0300 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0305 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0306 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0334 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0463 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0463 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0516 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0516 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0554 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0725 seconds and 0.2273 seconds precompiling for 25 choices
[rank3]:W1031 10:58:58.898000 286 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0307 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0308 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0308 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0332 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0463 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0463 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0504 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0505 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_262 0.0541 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0776 seconds and 0.2175 seconds precompiling for 25 choices
[rank3]:W1031 10:58:59.119000 286 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:58:59 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_259 0.0321 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0325 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0326 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_268 0.0334 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_266 0.0495 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0497 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_267 0.0499 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0499 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0589 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0522 seconds and 0.2164 seconds precompiling for 25 choices
[rank7]:W1031 10:58:59.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:58:59.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:58:59.787000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:59.947000 284 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank7]:W1031 10:58:59.948000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:58:59.970000 284 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:59:00 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 10:59:00.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:00.047000 289 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 10:59:00.070000 289 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:59:00 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 10:59:00.196000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_259 0.0296 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0308 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0309 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0313 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0463 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0463 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0516 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0543 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0821 seconds and 0.2472 seconds precompiling for 25 choices
[rank2]:W1031 10:59:00.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:00.453000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:00.456000 287 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank3]:W1031 10:59:00.467000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:00.479000 287 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:59:00 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 10:59:00.555000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:00.708000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:00.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:00.764000 283 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 10:59:00.788000 283 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[rank2]:W1031 10:59:00.807000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:59:00 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 10:59:00.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:00.983000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:01.063000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:01.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:01.234000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:01.308000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:01.316000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:01.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:01.459000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:01.495000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:01.553000 288 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 10:59:01.575000 288 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[rank1]:W1031 10:59:01.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:01.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 10:59:01 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1031 10:59:01.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:01.711000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:01.744000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:01.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:01.829000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:01.838000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:01.921000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:01.965000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:01.991000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:02.079000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:02.086000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:02.092000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:02.134000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:02.172000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:02.220000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:02.239000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:02.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:02.341000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:02.347000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:02.387000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:02.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:02.467000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:02.488000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:02.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:02.587000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:02.600000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:02.635000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:02.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:02.719000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:02.740000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:02.828000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:02.835000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:02.853000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:02.883000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:02.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:02.971000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:02.988000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:03.076000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:03.083000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:03.104000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:03.115000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:03.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:03.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:03.223000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:03.252000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:03.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:03.331000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:03.352000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:03.372000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:03.383000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:03.411000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:03.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:03.500000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:03.572000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:03.579000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:03.599000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:03.627000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:03.632000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:03.663000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:03.727000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:03.760000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:03.820000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:03.827000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:03.848000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:03.879000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:03.886000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:03.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:03.981000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:04.007000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:04.067000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:04.076000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:04.095000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:04.126000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:04.140000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:04.160000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:04.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:04.255000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:04.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:04.324000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:04.347000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:04.378000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:04.392000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:04.408000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:04.480000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:04.507000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:04.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:04.572000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:04.599000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:04.631000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:04.640000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:04.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:04.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:04.759000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:04.810000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:04.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:04.851000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:04.882000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:04.892000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:04.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:04.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:05.007000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:05.063000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:05.069000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:05.103000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:05.134000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:05.144000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:05.152000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:05.224000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:05.259000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:05.315000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:05.321000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:05.358000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:05.386000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:05.396000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:05.403000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:05.472000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:05.519000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:05.572000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:05.577000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:05.615000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:05.638000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:05.648000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:05.655000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:05.720000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:05.771000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:05.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:05.826000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:05.874000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:05.900000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:05.907000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:05.912000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:05.968000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:06.024000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:06.067000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:06.080000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:06.128000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:06.151000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:06.157000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:06.167000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:06.220000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:06.280000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:06.319000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:06.332000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:06.380000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:06.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:06.416000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:06.421000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:06.472000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:06.527000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:06.568000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:06.583000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:06.631000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:06.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:06.671000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:06.676000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:06.724000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:06.775000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:06.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:06.834000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:06.883000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:06.916000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:06.923000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:06.930000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:06.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:07.032000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:07.063000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:07.088000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:07.136000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:07.167000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:07.173000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:07.183000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:07.228000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:07.296000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:07.315000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:07.352000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:07.400000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:07.426000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:07.433000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:07.438000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:07.543000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:07.573000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:07.603000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:07.655000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:07.680000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:07.688000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:07.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:07.705000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:07.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:07.828000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:07.854000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:07.907000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:07.932000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:07.940000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:07.953000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:07.958000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:08.056000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:08.079000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:08.108000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:08.160000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:08.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:08.191000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:08.205000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:08.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:08.308000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:08.331000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:08.360000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:08.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:08.449000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:08.462000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:08.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:08.584000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:08.619000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:08.643000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:08.688000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:08.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:08.716000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:08.723000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:08.799000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:08.836000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:08.874000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:08.899000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:08.960000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:08.974000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:08.978000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:09.053000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:09.092000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:09.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:09.155000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:09.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:09.216000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:09.233000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:09.243000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:09.316000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:09.344000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:09.415000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:09.422000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:09.469000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:09.485000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:09.510000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:09.579000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:09.623000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:09.675000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:09.682000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:09.728000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:09.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:09.766000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:09.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:09.831000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:09.879000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:09.931000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:09.940000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:09.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:10.000000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:10.022000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:10.076000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:10.083000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:10.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:10.188000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:10.195000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:10.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:10.253000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:10.328000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:10.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:10.387000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:10.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:10.454000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:10.488000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:10.504000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:10.514000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:10.580000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:10.595000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:10.647000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:10.703000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:10.710000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:10.740000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:10.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:10.770000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:10.832000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:10.847000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:10.903000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:10.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:10.966000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:10.992000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:11.008000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:11.026000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:11.084000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:11.099000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:11.165000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:11.215000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:11.222000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:11.244000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:11.260000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:11.294000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:11.336000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:11.363000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:11.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:11.471000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:11.478000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:11.495000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:11.512000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:11.550000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:11.588000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:11.615000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:11.679000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:11.727000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:11.734000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:11.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:11.806000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:11.848000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:11.868000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:11.935000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:11.984000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:11.993000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:11.998000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:12.024000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:12.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:12.103000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:12.123000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:12.187000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:12.240000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:12.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:12.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:12.276000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:12.330000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:12.356000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:12.375000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:12.439000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:12.496000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:12.504000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:12.511000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:12.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:12.588000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:12.607000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:12.628000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:12.692000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:12.751000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:12.756000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:12.768000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:12.779000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:12.843000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:12.859000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:12.880000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:12.944000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:13.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:13.017000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:13.024000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:13.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:13.099000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:13.111000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:13.132000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:13.195000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:13.272000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:13.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:13.285000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:13.293000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:13.354000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:13.364000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:13.383000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:13.447000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:13.528000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:13.534000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:13.543000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:13.550000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:13.612000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:13.618000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:13.636000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:13.704000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:13.783000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:13.788000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:13.803000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:13.812000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:13.871000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:13.879000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:13.900000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:13.956000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:14.042000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:14.048000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:14.068000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:14.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:14.131000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:14.148000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:14.207000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:14.300000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:14.306000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:14.323000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:14.376000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:14.394000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:14.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:14.471000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:14.556000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:14.564000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:14.631000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:14.656000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:14.669000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:14.732000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:14.819000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:14.825000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:14.891000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:14.919000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:14.928000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:14.992000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:15.079000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:15.085000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:15.151000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:15.180000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:15.251000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:15.340000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:15.347000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:15.412000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:15.438000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:15.506000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:15.600000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:15.606000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:15.664000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:15.695000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:15.763000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:15.859000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:15.866000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:15.919000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:15.955000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:16.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:16.175000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:16.222000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:16.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:16.471000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:16.632000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:16.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:16.899000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:17.170000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:17.428000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:17.683000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_283 0.1005 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1006 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1014 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1014 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1054 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1056 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1090 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1091 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_303 0.1391 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0265 seconds and 0.1942 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_282 0.1036 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1047 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1092 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1092 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1126 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1126 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1476 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0788 seconds and 0.1826 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_283 0.1037 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1038 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1048 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1098 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1098 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1134 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1134 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1484 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0844 seconds and 0.1885 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_283 0.1038 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1038 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1048 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1049 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1098 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1098 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1135 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1136 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1485 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0485 seconds and 0.2050 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_282 0.1037 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1038 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1046 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1099 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1100 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1135 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1136 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1483 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0276 seconds and 0.2018 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_282 0.1036 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1048 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1098 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1099 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1130 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1131 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1482 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0308 seconds and 0.1932 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_283 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1036 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1049 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1096 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1098 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1130 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1485 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0436 seconds and 0.1813 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_282 0.1037 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1037 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1104 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1106 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1136 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1138 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_286 0.1484 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0529 seconds and 0.1871 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=40.85 GB):  90%| | 47/52 [07:34<06:39, 79.85s/it]Capturing batches (bs=12 avail_mem=40.24 GB):  90%| | 47/52 [07:34<06:39, 79.85s/it][rank3]:W1031 10:59:30.114000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:30.202000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:30.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:30.283000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:30.308000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:30.344000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:30.379000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:30.388000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:30.391000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:30.419000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:30.456000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:30.468000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:30.525000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:30.564000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:30.574000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:30.836000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:30.913000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:30.948000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:30.956000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:31.022000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:31.026000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:31.036000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:31.133000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:31.145000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:31.829000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:31.894000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:32.041000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:32.093000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:32.100000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:32.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:32.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:32.669000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:33.583000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:33.660000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:33.724000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:33.742000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:33.759000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:33.782000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:33.797000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:33.801000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:33.834000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:33.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:33.875000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:33.900000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:33.935000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:33.964000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:33.976000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:34.442000 287 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank7]:W1031 10:59:34.471000 290 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank2]:W1031 10:59:34.502000 285 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank1]:W1031 10:59:34.527000 284 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank6]:W1031 10:59:34.540000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:34.546000 286 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank6]:W1031 10:59:34.628000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:34.740000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:34.751000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:34.832000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:34.933000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:35.125000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:35.202000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:35.205000 289 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank5]:W1031 10:59:35.310000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:35.393000 283 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank5]:W1031 10:59:35.790000 288 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_325 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_314 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_319 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_322 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8561 seconds and 0.1590 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_317 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_319 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_321 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_322 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_326 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_327 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_316 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8418 seconds and 0.1550 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_310 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_325 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_304 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8214 seconds and 0.1508 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_324 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_325 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_327 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_307 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_320 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_326 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8446 seconds and 0.1566 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_310 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0063 ms 99.4% 
  triton_bmm_322 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_317 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8477 seconds and 0.1351 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_314 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_320 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_326 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_310 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_323 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_319 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8678 seconds and 0.1569 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_308 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_305 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_306 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_320 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_324 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_307 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_314 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7728 seconds and 0.1276 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_317 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_319 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_307 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_309 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_323 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_320 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_310 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8922 seconds and 0.1474 seconds precompiling for 25 choices
[rank1]:W1031 10:59:44.885000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:45.209000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:45.220000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:45.310000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:45.377000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:45.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:45.705000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:45.718000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:45.806000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:45.812000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:45.986000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:46.315000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:46.426000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:46.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:46.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:46.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:47.763000 284 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank0]:W1031 10:59:48.279000 283 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank3]:W1031 10:59:48.629000 286 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank6]:W1031 10:59:48.905000 289 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank4]:W1031 10:59:48.934000 287 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank2]:W1031 10:59:49.285000 285 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank7]:W1031 10:59:50.549000 290 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1031 10:59:52.756000 288 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0074 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0085 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8427 seconds and 0.4241 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0066 ms 97.6% 
  triton_bmm_341 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_338 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0082 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0085 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7881 seconds and 0.4356 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.8% 
  triton_bmm_340 0.0069 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8337 seconds and 0.4086 seconds precompiling for 25 choices
[rank1]:W1031 10:59:53.991000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:54.068000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:54.128000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_331 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8124 seconds and 0.4107 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_330 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_341 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8263 seconds and 0.3964 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0066 ms 98.8% 
  triton_bmm_340 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_335 0.0080 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7838 seconds and 0.4366 seconds precompiling for 25 choices
[rank3]:W1031 10:59:54.956000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:55.019000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:55.033000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 10:59:55.081000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:55.096000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 10:59:55.145000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:55.219000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:55.295000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 10:59:55.344000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:55.636000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:55.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 10:59:55.761000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_330 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0074 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0074 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8003 seconds and 0.4002 seconds precompiling for 25 choices
[rank2]:W1031 10:59:56.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:56.096000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 10:59:56.144000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:57.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:57.224000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 10:59:57.273000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_340 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0070 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8011 seconds and 0.3975 seconds precompiling for 25 choices
[rank5]:W1031 10:59:58.902000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:58.980000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 10:59:59.029000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:59.398000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:59.475000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 10:59:59.578000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:00.971000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:00.987000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:01.060000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:01.073000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:01.161000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:01.174000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:01.270000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:01.347000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:01.449000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:01.730000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:01.807000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:01.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:02.158000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:02.236000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:02.338000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:03.054000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:03.182000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:03.260000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:03.310000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:03.359000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:03.569000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:03.956000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:04.211000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:04.339000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:04.467000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:04.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:04.627000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:04.848000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:04.886000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:05.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:05.143000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:05.181000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:05.258000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:05.327000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:05.358000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:05.407000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:05.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:05.528000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:05.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:05.591000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:05.792000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:05.946000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:06.031000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:06.059000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:06.152000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:06.468000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:06.547000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:06.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:06.654000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:06.751000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:06.828000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:06.830000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:06.928000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:07.092000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:07.220000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:07.299000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:07.405000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:07.729000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:07.807000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:07.914000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:08.788000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:08.883000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:08.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:09.043000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:09.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:09.298000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:10.794000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:10.871000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:10.973000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0100 ms 100.0% 
  triton_mm_364 0.0305 ms 32.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0307 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0347 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0352 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0460 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0461 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0515 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0574 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0713 seconds and 0.2323 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0101 ms 100.0% 
  triton_mm_355 0.0325 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0327 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0328 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_365 0.0335 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0499 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0500 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_363 0.0506 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0512 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0606 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0597 seconds and 0.2198 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_364 0.0306 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0306 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0345 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0352 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0462 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0465 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0515 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0516 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0563 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9845 seconds and 0.2274 seconds precompiling for 25 choices
[rank1]:W1031 11:00:12.572000 284 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank1]:W1031 11:00:12.595000 284 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_355 0.0305 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0305 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0311 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0334 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0462 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0463 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0509 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0539 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0796 seconds and 0.2128 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:12 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_355 0.0305 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0307 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0309 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0323 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0463 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0463 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0517 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0542 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0641 seconds and 0.2323 seconds precompiling for 25 choices
[rank0]:W1031 11:00:13.262000 283 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 11:00:13.285000 283 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:13 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 11:00:13.397000 286 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 11:00:13.429000 286 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:13 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_365 0.0305 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0328 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0345 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0348 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0507 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0515 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0515 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_362 0.0523 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_374 0.0621 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0281 seconds and 0.2308 seconds precompiling for 25 choices
[rank4]:W1031 11:00:13.902000 287 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank1]:W1031 11:00:13.923000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:13.926000 287 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:13 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:00:14.180000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:14.376000 289 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 11:00:14.398000 289 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[rank1]:W1031 11:00:14.444000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:14 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:00:14.645000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:14.700000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_364 0.0306 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0308 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0320 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0338 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0470 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0471 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0509 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0509 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0563 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1081 seconds and 0.2114 seconds precompiling for 25 choices
[rank2]:W1031 11:00:14.864000 285 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 11:00:14.886000 285 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[rank3]:W1031 11:00:14.904000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:14.910000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:14 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:00:14.956000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:15.160000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:15.166000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:15.216000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:15.254000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:15.416000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:15.423000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:15.476000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:15.511000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:15.668000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:15.675000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:15.715000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:15.732000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:15.767000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:15.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:15.942000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:15.971000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:15.988000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:16.023000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:16.070000 290 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 11:00:16.093000 290 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:16 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 11:00:16.180000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:16.199000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:16.209000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:16.227000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:16.244000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:16.279000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:16.444000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:16.468000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:16.475000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:16.484000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:16.500000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:16.535000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:16.699000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:16.723000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:16.731000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:16.740000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:16.755000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:16.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:16.955000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:16.982000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:16.995000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:17.003000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:17.021000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:17.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:17.211000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:17.242000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:17.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:17.262000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:17.279000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:17.308000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:17.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:17.467000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:17.502000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:17.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:17.522000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:17.539000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:17.565000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:17.708000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:17.723000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:17.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:17.775000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:17.784000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:17.799000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:17.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:17.970000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:17.979000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:18.023000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:18.035000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:18.043000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:18.059000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:18.080000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:18.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:18.236000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:18.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:18.301000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:18.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:18.316000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:18.335000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_355 0.0301 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0304 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0306 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0323 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0464 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0514 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0546 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0961 seconds and 0.0001 seconds precompiling for 25 choices
[rank7]:W1031 11:00:18.487000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:18.494000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:18.544000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:18.561000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:18.569000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:18.575000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:18.591000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:18.748000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:18.759000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:18.811000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:18.827000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:18.834000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:18.841000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:18.850000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:19.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:19.020000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:19.078000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:19.088000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:19.095000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:19.105000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:19.273000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:19.283000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:19.343000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:19.351000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:19.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:19.366000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:19.443000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:19.534000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:19.543000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:19.611000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:19.617000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:19.626000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:19.633000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:19.707000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:19.708000 288 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 11:00:19.731000 288 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[rank7]:W1031 11:00:19.797000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:00:19 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1031 11:00:19.803000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:19.881000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:19.888000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:19.896000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:19.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:20.053000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:20.063000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:20.143000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:20.151000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:20.157000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:20.204000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:20.241000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:20.312000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:20.408000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:20.421000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:20.472000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:20.504000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:20.580000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:20.652000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:20.672000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:20.688000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:20.735000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:20.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:20.767000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:20.845000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:20.927000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:20.934000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:20.959000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:20.998000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:21.006000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:21.027000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:21.089000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:21.106000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:21.188000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:21.224000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:21.264000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:21.270000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:21.292000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:21.356000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:21.368000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:21.448000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:21.488000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:21.519000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:21.528000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:21.534000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:21.552000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:21.615000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:21.628000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:21.708000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:21.748000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:21.785000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:21.791000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:21.798000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:21.811000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:21.877000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:21.889000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:21.972000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:22.049000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:22.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:22.066000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:22.075000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:22.137000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:22.149000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:22.237000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:22.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:22.323000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:22.330000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:22.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:22.349000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:22.392000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:22.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:22.496000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:22.575000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:22.583000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:22.592000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:22.600000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:22.609000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:22.652000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:22.664000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:22.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:22.841000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:22.849000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:22.855000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:22.862000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:22.869000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:22.919000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:23.031000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:23.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:23.115000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:23.130000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:23.137000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:23.143000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:23.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:23.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:23.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:23.371000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:23.377000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:23.396000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:23.403000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:23.412000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:23.444000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:23.492000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:23.552000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:23.635000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:23.640000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:23.656000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:23.664000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:23.673000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:23.703000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:23.752000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:23.816000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:23.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:23.905000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:23.916000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:23.924000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:23.936000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:23.964000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:24.016000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:24.080000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:24.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:24.168000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:24.176000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:24.184000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:24.200000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:24.224000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:24.276000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:24.340000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:24.427000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:24.433000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:24.440000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:24.447000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:24.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:24.484000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:24.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:24.600000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:24.691000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:24.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:24.703000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:24.712000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:24.736000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:24.748000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:24.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:24.868000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:24.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:24.961000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:24.968000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:24.975000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:25.001000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:25.012000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:25.060000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:25.128000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:25.219000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:25.225000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:25.232000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:25.240000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:25.260000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:25.276000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:25.324000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:25.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:25.483000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:25.488000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:25.508000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:25.515000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:25.536000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:25.543000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:25.588000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:25.649000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:25.747000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:25.752000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:25.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:25.779000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:25.797000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:25.804000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:25.848000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:25.909000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:26.012000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:26.017000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:26.036000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:26.043000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:26.057000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:26.116000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:26.169000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:26.279000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:26.285000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:26.300000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:26.307000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:26.316000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:26.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:26.391000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:26.429000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:26.543000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:26.549000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:26.564000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:26.571000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:26.580000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:26.644000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:26.655000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:26.688000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:26.807000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:26.812000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:26.828000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:26.836000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:26.843000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:26.908000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:26.919000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:26.948000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:27.071000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:27.076000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:27.092000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:27.100000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:27.107000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:27.172000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:27.183000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:27.204000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:27.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:27.341000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:27.356000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:27.364000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:27.371000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:27.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:27.451000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:27.464000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:27.599000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:27.605000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:27.632000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:27.639000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:27.647000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:27.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:27.717000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:27.724000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:27.863000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:27.868000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:27.896000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:27.904000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:27.911000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:27.964000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:27.984000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:27.991000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:28.127000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:28.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:28.160000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:28.167000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:28.175000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:28.228000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:28.244000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:28.256000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:28.391000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:28.396000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:28.432000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:28.440000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:28.449000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:28.492000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:28.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:28.520000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:28.655000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:28.660000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:28.696000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:28.704000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:28.713000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:28.757000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:28.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:28.784000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:28.919000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:28.924000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:28.960000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:28.969000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:28.977000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:29.020000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:29.027000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:29.052000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:29.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:29.188000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:29.224000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:29.236000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:29.284000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:29.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:29.315000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:29.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:29.453000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:29.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:29.497000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:29.545000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:29.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:29.584000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:29.719000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:29.725000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:29.753000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:29.761000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:29.805000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:29.824000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:29.852000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:29.984000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:29.992000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:30.033000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:30.093000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:30.124000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:30.252000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:30.260000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:30.304000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:30.360000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:30.395000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:30.528000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:30.569000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:30.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:30.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:30.800000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:30.848000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:30.904000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:30.935000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:31.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:31.173000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:31.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:31.386000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:31.443000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:31.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:31.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:31.734000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:31.972000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:32.000000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:32.235000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:32.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:32.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:32.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:32.799000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:33.055000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:33.318000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:33.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:33.855000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:34.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:34.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:34.615000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:34.866000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:35.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_378 0.1046 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1049 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1093 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1094 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1131 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1132 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1452 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0896 seconds and 0.1869 seconds precompiling for 25 choices
[rank5]:W1031 11:00:35.372000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:35.631000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_379 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1048 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1049 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1093 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1126 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1454 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0868 seconds and 0.1984 seconds precompiling for 25 choices
[rank5]:W1031 11:00:35.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_379 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1047 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1093 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1095 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1133 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1453 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1090 seconds and 0.1915 seconds precompiling for 25 choices
[rank5]:W1031 11:00:36.144000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_379 0.1044 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1092 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1126 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1128 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1450 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1110 seconds and 0.1801 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_378 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1096 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1098 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1130 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1131 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1452 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0933 seconds and 0.2218 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_378 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1095 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1096 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1124 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1124 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1444 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0387 seconds and 0.1908 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_378 0.1012 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1012 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_379 0.1012 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_389 0.1013 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1064 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1065 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1090 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1091 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1374 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0101 seconds and 0.2170 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_378 0.1045 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1045 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_379 0.1046 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1046 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1093 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1144 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1145 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1466 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0076 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=40.24 GB):  92%|| 48/52 [08:55<05:19, 79.99s/it]Capturing batches (bs=8 avail_mem=39.66 GB):  92%|| 48/52 [08:55<05:19, 79.99s/it] [rank3]:W1031 11:00:50.618000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:50.661000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:50.696000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:50.738000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:50.740000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:50.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:50.804000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:50.818000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:50.844000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:50.849000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:50.909000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:50.924000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:50.926000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:50.958000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:50.987000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:51.003000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:51.012000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:51.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:51.095000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:51.111000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:51.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:51.197000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:51.233000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:51.344000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:52.365000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:52.381000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:52.461000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:52.483000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:52.644000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:52.653000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:52.750000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:52.877000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:54.041000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:54.121000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:54.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:54.254000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:54.328000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:54.333000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:54.408000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:54.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:54.437000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:54.506000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:00:54.509000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:54.542000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:54.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:54.621000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:00:54.696000 290 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank3]:W1031 11:00:54.723000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:00:54.909000 283 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank2]:W1031 11:00:54.984000 285 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1031 11:00:55.046000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:00:55.084000 288 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1031 11:00:55.132000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:00:55.200000 286 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1031 11:00:55.245000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:55.316000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:55.399000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:55.502000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:55.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:00:55.692000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:00:55.733000 284 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1031 11:00:55.794000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:00:55.970000 287 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank6]:W1031 11:00:56.271000 289 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_413 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_407 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_414 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_405 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_402 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8569 seconds and 0.1507 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_421 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_418 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_405 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9025 seconds and 0.1313 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_419 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0061 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_420 0.0061 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0061 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0061 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_414 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_416 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8622 seconds and 0.1424 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_411 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_405 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_412 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_403 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_415 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9311 seconds and 0.1305 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_420 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_412 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_405 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8414 seconds and 0.1406 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_411 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0064 ms 99.4% 
  triton_bmm_403 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_404 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_407 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_410 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8592 seconds and 0.1329 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_417 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_405 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_415 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_423 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_404 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7899 seconds and 0.1294 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_405 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_403 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_401 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_404 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_411 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9324 seconds and 0.1508 seconds precompiling for 25 choices
[rank2]:W1031 11:01:05.149000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:05.305000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:05.325000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:05.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:05.744000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:05.795000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:05.813000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:05.833000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:06.213000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:06.249000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:06.294000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:06.726000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:07.170000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:07.437000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:07.634000 285 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank4]:W1031 11:01:07.673000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:07.937000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:08.118000 283 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank7]:W1031 11:01:08.206000 290 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank5]:W1031 11:01:08.905000 288 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank1]:W1031 11:01:08.907000 284 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1031 11:01:08.925000 286 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank4]:W1031 11:01:09.798000 287 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank6]:W1031 11:01:10.665000 289 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_426 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_431 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0081 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8486 seconds and 0.4096 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0066 ms 99.4% 
  triton_bmm_437 0.0070 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0071 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0078 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0078 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0085 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7448 seconds and 0.4087 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_427 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_435 0.0075 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0077 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_431 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_446 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8181 seconds and 0.4101 seconds precompiling for 25 choices
[rank2]:W1031 11:01:13.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:13.925000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:14.025000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0070 ms 93.1% 
  triton_bmm_437 0.0070 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0071 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0074 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0074 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0084 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8038 seconds and 0.4031 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0066 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0068 ms 96.5% 
  triton_bmm_436 0.0070 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0070 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9059 seconds and 0.4117 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0068 ms 94.1% 
  triton_bmm_436 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_446 0.0083 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8943 seconds and 0.4055 seconds precompiling for 25 choices
[rank0]:W1031 11:01:14.334000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:14.389000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:14.414000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:14.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:14.515000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:14.566000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_427 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0085 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7769 seconds and 0.4138 seconds precompiling for 25 choices
[rank5]:W1031 11:01:15.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:15.304000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:15.380000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:15.382000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:15.480000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:15.482000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:15.598000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:15.675000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:15.774000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_426 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_437 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0080 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0084 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7960 seconds and 0.4309 seconds precompiling for 25 choices
[rank4]:W1031 11:01:16.064000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:16.141000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:16.242000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:16.969000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:17.047000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:17.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:19.568000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:19.644000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:19.694000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:20.272000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:20.328000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:20.348000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:20.397000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:20.404000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:20.455000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:20.512000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:20.591000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:20.643000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:21.008000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:21.086000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:21.136000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:21.610000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:21.685000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:21.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:21.772000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:21.850000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:21.901000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:22.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:22.733000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:22.822000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:22.899000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:22.948000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:22.999000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:23.174000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:23.441000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:23.635000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:23.708000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:23.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:23.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:24.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:24.171000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:24.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:24.365000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:24.479000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:24.599000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:24.660000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:24.747000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:24.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:24.929000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:25.011000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:25.081000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:25.161000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:25.193000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:25.276000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:25.815000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:25.897000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:26.002000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:26.119000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:26.133000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:26.200000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:26.242000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:26.303000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:26.322000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:26.398000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:26.430000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:26.665000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:26.779000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:26.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:26.967000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:26.985000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:27.047000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:27.150000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:27.318000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:27.399000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:27.509000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:28.583000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:28.661000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:28.764000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0300 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0344 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0346 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0475 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0478 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0507 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0507 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_454 0.0592 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0050 seconds and 0.2160 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0300 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0342 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0347 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0461 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0461 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0514 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0514 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0619 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1173 seconds and 0.2275 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_451 0.0311 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0317 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_461 0.0321 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0330 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0491 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0491 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0503 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0503 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_454 0.0597 ms 15.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0906 seconds and 0.2216 seconds precompiling for 25 choices
[rank2]:W1031 11:01:32.087000 285 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 11:01:32.110000 285 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:32 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0305 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0307 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0347 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0348 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0471 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0480 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0502 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0504 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_471 0.0620 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1529 seconds and 0.2277 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_451 0.0295 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0301 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0302 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0311 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0461 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0469 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0495 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0509 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0549 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1196 seconds and 0.2419 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_461 0.0299 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0301 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0334 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0337 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0458 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0458 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0512 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0514 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0545 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0752 seconds and 0.2146 seconds precompiling for 25 choices
[rank3]:W1031 11:01:33.049000 286 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 11:01:33.072000 286 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:33 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_461 0.0298 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0299 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0316 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0329 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0456 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0506 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0506 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_454 0.0538 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0812 seconds and 0.2213 seconds precompiling for 25 choices
[rank0]:W1031 11:01:33.333000 283 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 11:01:33.357000 283 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:33 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 11:01:33.527000 290 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 11:01:33.550000 290 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:33 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:01:33.894000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:34.129000 284 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank1]:W1031 11:01:34.152000 284 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[rank2]:W1031 11:01:34.169000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:34.195000 288 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 11:01:34.218000 288 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:34 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:34 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:01:34.433000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0295 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0303 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_461 0.0303 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_459 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0457 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0502 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0504 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_454 0.0542 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0818 seconds and 0.2162 seconds precompiling for 25 choices
[rank4]:W1031 11:01:34.513000 287 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank4]:W1031 11:01:34.536000 287 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:34 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:01:34.701000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:34.721000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:34.860000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:34.964000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:34.983000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:35.130000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:35.233000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:35.248000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:35.403000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:35.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:35.501000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:35.513000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:35.672000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:35.727000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:35.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:35.756000 289 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank2]:W1031 11:01:35.771000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:35.779000 289 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[rank3]:W1031 11:01:35.779000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:35.792000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:01:35 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:01:35.940000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:35.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:36.020000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:36.037000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:36.046000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:36.062000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:36.218000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:36.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:36.287000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:36.301000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:36.308000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:36.316000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:36.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:36.484000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:36.533000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:36.561000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:36.567000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:36.577000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:36.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:36.592000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:36.751000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:36.801000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:36.829000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:36.836000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:36.844000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:36.851000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:36.858000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:37.026000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:37.067000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:37.092000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:37.105000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:37.112000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:37.120000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:37.130000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:37.291000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:37.298000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:37.335000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:37.360000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:37.369000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:37.379000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:37.387000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:37.396000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:37.561000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:37.575000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:37.605000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:37.629000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:37.635000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:37.647000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:37.655000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:37.663000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:37.829000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:37.843000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:37.873000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:37.893000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:37.903000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:37.911000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:37.922000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:37.928000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:38.091000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:38.112000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:38.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:38.156000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:38.173000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:38.181000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:38.189000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:38.197000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:38.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:38.380000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:38.407000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:38.423000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:38.440000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:38.448000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:38.458000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:38.465000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:38.633000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:38.647000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:38.677000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:38.693000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:38.704000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:38.711000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:38.725000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:38.731000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:38.901000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:38.915000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:38.946000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:38.957000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:38.971000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:38.979000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:38.990000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:38.996000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:39.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:39.195000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:39.213000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:39.221000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:39.235000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:39.248000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:39.257000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:39.264000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:39.437000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:39.463000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:39.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:39.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:39.499000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:39.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:39.523000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:39.531000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:39.705000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:39.731000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:39.749000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:39.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:39.763000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:39.783000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:39.791000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:39.799000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:39.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:40.005000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:40.015000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:40.022000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:40.030000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:40.053000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:40.060000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:40.068000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:40.239000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:40.272000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:40.283000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:40.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:40.298000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:40.321000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:40.330000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:40.337000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:40.507000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:40.540000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:40.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:40.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:40.566000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:40.585000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:40.597000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:40.604000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:40.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:40.808000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:40.823000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:40.830000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:40.838000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:40.849000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:40.865000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:40.872000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:41.045000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:41.083000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:41.097000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:41.105000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:41.112000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:41.118000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:41.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:41.139000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:41.313000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:41.355000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:41.365000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:41.373000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:41.380000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:41.387000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:41.402000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:41.408000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:41.581000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:41.627000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:41.638000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:41.647000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:41.654000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:41.662000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:41.673000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:41.679000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:41.853000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:41.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:41.914000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:41.921000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:41.930000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:41.936000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:41.947000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:41.953000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:42.121000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:42.179000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:42.186000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:42.192000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:42.202000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:42.209000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:42.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:42.225000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:42.388000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:42.451000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:42.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:42.465000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:42.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:42.481000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:42.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:42.497000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:42.657000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:42.723000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:42.730000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:42.736000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:42.745000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:42.753000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:42.763000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:42.769000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:42.925000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:42.995000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:43.002000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:43.008000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:43.017000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:43.025000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:43.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:43.041000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:43.193000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:43.267000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:43.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:43.281000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:43.290000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:43.298000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:43.307000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:43.313000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:43.461000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:43.539000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:43.547000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:43.553000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:43.562000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:43.570000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:43.578000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:43.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:43.729000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:43.811000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:43.819000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:43.824000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:43.833000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:43.841000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:43.849000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:43.857000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:43.997000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:44.083000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:44.090000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:44.097000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:44.105000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:44.113000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:44.123000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:44.129000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:44.265000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:44.355000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:44.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:44.369000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:44.378000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:44.386000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:44.395000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:44.401000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:44.533000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:44.627000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:44.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:44.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:44.649000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:44.658000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:44.666000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:44.673000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:44.800000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:44.900000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:44.907000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:44.915000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:44.922000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:44.931000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:44.938000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:44.947000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:45.072000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:45.167000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:45.179000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:45.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:45.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:45.204000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:45.214000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:45.221000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:45.346000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:45.440000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:45.448000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:45.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:45.469000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:45.481000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:45.492000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:45.618000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:45.715000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:45.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:45.733000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:45.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:45.750000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:45.763000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:45.889000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:45.967000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:45.995000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:46.001000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:46.010000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:46.019000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:46.027000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:46.045000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:46.162000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:46.255000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:46.275000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:46.282000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:46.290000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:46.300000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:46.307000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:46.315000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:46.437000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:46.531000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:46.547000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:46.553000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:46.565000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:46.575000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:46.583000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:46.591000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:46.710000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:46.808000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:46.819000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:46.842000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:46.853000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:46.864000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:46.990000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:47.089000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:47.122000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:47.130000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:47.266000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:47.282000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:47.312000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:47.368000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:47.405000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:47.541000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:47.561000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:47.583000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:47.607000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:47.629000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:47.645000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:47.681000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:47.817000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:47.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:47.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:47.863000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:47.891000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:47.911000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:47.919000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:47.953000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:48.088000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:48.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:48.121000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:48.135000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:48.163000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:48.184000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:48.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:48.221000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:48.358000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:48.385000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:48.393000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:48.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:48.435000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:48.455000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:48.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:48.670000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:48.677000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:48.683000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:48.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:48.727000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:48.739000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:48.948000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:48.957000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:48.963000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:48.980000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:49.001000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:49.012000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:49.019000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:49.084000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:49.223000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:49.232000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:49.239000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:49.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:49.273000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:49.285000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:49.293000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:49.356000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:49.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:49.509000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:49.516000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:49.524000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:49.545000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:49.557000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:01:49.565000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:49.628000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:49.776000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:49.789000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:49.795000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:49.809000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:49.825000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:49.834000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:49.900000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:50.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:50.061000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:50.072000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:50.080000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:50.097000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:50.108000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:50.172000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:50.328000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:01:50.335000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:50.353000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:50.360000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:50.369000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:50.380000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:50.445000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:50.608000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:01:50.636000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:50.644000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:50.652000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:50.661000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:50.716000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:50.887000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:50.925000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:50.931000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:50.939000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:50.988000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:01:51.169000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:51.197000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:51.208000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:51.215000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:51.261000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:01:51.486000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:51.492000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:01:51.498000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:51.540000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:51.775000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:51.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:01:52.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:52.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:52.373000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:52.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:01:52.924000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_474 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1085 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1086 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1122 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_478 0.1397 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0308 seconds and 0.1840 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_474 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1089 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1089 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1127 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1130 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1405 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0911 seconds and 0.1956 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_475 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1047 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1047 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1088 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1088 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1122 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1123 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1407 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0542 seconds and 0.2040 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_475 0.1009 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1010 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1010 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1012 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1051 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1053 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1086 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1086 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1332 ms 45.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0473 seconds and 0.1883 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_475 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1044 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_474 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_492 0.1091 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1092 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1125 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1126 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1406 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0687 seconds and 0.2018 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_474 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1094 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1128 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1407 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0948 seconds and 0.2071 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0612 ms 100.0% 
  triton_mm_474 0.1043 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1045 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1089 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1092 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1126 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1126 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1403 ms 43.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0345 seconds and 0.1868 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_474 0.1043 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1046 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1098 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1100 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1129 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1129 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1407 ms 43.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0683 seconds and 0.1851 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=39.66 GB):  94%|| 49/52 [10:10<03:56, 78.67s/it]Capturing batches (bs=4 avail_mem=39.08 GB):  94%|| 49/52 [10:10<03:56, 78.67s/it][rank0]:W1031 11:02:06.516000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:06.597000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:06.642000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:06.707000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:06.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:06.829000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:06.877000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:06.915000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:06.923000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:06.932000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:06.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:06.967000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:06.992000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:07.001000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:07.009000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:07.045000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:07.063000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:07.099000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:07.108000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:07.116000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:07.154000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:07.490000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:07.569000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:07.676000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:08.245000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:08.395000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:08.574000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:08.628000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:08.636000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:08.642000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:08.673000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:09.222000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:10.388000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:10.403000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:10.427000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:10.468000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:10.473000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:10.482000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:10.507000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:10.518000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:10.532000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:10.551000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:10.558000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:10.601000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:10.944000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:11.023000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:11.073000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:11.082000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:11.160000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:11.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:11.229000 288 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank6]:W1031 11:02:11.231000 289 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank0]:W1031 11:02:11.258000 283 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank4]:W1031 11:02:11.298000 287 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1031 11:02:11.373000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:11.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:11.462000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:11.515000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:11.543000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:11.556000 285 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1031 11:02:11.594000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:11.702000 290 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1031 11:02:11.994000 284 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1031 11:02:12.072000 286 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_511 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_515 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_509 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8378 seconds and 0.1505 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_500 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_503 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_507 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_499 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_501 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_508 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_515 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8435 seconds and 0.1521 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_510 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_514 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_516 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_519 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_506 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8835 seconds and 0.1537 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_508 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_510 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_500 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_507 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_515 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9126 seconds and 0.1482 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_502 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_506 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_498 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_518 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_500 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_501 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_507 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8320 seconds and 0.1597 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_515 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_514 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_507 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_509 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_503 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_506 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8959 seconds and 0.1318 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_500 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_499 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 99.4% 
  triton_bmm_496 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_498 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_501 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_503 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_508 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_514 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_509 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8567 seconds and 0.1320 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_499 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_509 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_498 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0063 ms 98.7% 
  triton_bmm_500 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9130 seconds and 0.1540 seconds precompiling for 25 choices
[rank5]:W1031 11:02:21.130000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:21.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:21.635000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:21.776000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:21.946000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:22.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:22.117000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:22.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:22.308000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:22.344000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:22.368000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:22.446000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:22.626000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:22.809000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:22.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:22.869000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:24.519000 288 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank6]:W1031 11:02:24.586000 289 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank2]:W1031 11:02:24.589000 285 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank7]:W1031 11:02:25.025000 290 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1031 11:02:25.096000 284 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank3]:W1031 11:02:25.729000 286 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank0]:W1031 11:02:26.373000 283 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank4]:W1031 11:02:26.736000 287 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0082 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8280 seconds and 0.4145 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_523 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0078 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0084 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8588 seconds and 0.4094 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_523 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0080 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9183 seconds and 0.4146 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 99.4% 
  triton_bmm_522 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0073 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8894 seconds and 0.4297 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0074 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0074 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0082 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8240 seconds and 0.4314 seconds precompiling for 25 choices
[rank5]:W1031 11:02:30.717000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:30.794000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:30.807000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:30.843000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:30.878000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:30.885000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:30.935000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:30.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:31.004000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_532 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0072 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0084 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8316 seconds and 0.4158 seconds precompiling for 25 choices
[rank1]:W1031 11:02:31.305000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:31.384000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:31.430000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:31.434000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:31.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:31.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 97.5% 
  triton_bmm_532 0.0070 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0083 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8373 seconds and 0.4058 seconds precompiling for 25 choices
[rank3]:W1031 11:02:32.041000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0068 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0080 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8636 seconds and 0.4096 seconds precompiling for 25 choices
[rank3]:W1031 11:02:32.119000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:32.169000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:32.684000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:32.764000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:32.816000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:33.095000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:33.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:33.222000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:36.537000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:36.618000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:36.723000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:36.831000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:36.910000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:36.972000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:37.013000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:37.050000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:37.070000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:37.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:37.153000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:37.190000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:37.248000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:37.268000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:37.372000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:37.710000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:37.786000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:37.887000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:38.531000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:38.609000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:38.712000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:38.906000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:38.983000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:39.084000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:39.932000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:40.009000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:40.147000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:40.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:40.221000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:40.281000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:40.347000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:40.420000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:40.477000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:40.495000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:40.550000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:40.623000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:40.668000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:40.698000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:40.765000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:40.895000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:40.948000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:41.215000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:41.815000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:41.867000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:42.089000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:42.130000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:42.143000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:42.157000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:42.209000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:42.235000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:42.264000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:42.311000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:42.336000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:42.342000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:42.355000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:42.357000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:42.419000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:42.444000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:42.447000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:42.549000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:42.551000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:42.628000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:42.730000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:42.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:42.948000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:43.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:44.042000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:44.111000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:44.119000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:44.189000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:44.220000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:44.291000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_557 0.0297 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0302 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0345 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0348 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0453 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0455 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0563 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0333 seconds and 0.2339 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_547 0.0289 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0296 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0296 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0326 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0450 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0451 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0540 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0659 seconds and 0.2409 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_547 0.0295 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0299 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0300 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0311 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0450 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0451 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0483 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0483 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0538 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1194 seconds and 0.2141 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0296 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0300 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0337 ms 27.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0345 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0460 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0461 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_566 0.0612 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1131 seconds and 0.2342 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_556 0.0303 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0321 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0346 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0348 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0460 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0468 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0479 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_566 0.0614 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0928 seconds and 0.2141 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_557 0.0296 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0296 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0340 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0343 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0454 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0455 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_567 0.0619 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1119 seconds and 0.2289 seconds precompiling for 25 choices
[rank2]:W1031 11:02:49.507000 285 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 11:02:49.530000 285 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[rank6]:W1031 11:02:49.555000 289 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 11:02:49.578000 289 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[rank7]:W1031 11:02:49.597000 290 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:49 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 11:02:49.620000 290 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:49 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:49 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:02:49.721000 284 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank5]:W1031 11:02:49.840000 288 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 11:02:49.863000 288 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_557 0.0294 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0298 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_547 0.0298 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0319 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0447 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0447 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0478 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0843 seconds and 0.2233 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:49 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_547 0.0319 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0321 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_556 0.0325 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0326 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0483 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0483 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0483 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_555 0.0492 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0598 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0600 seconds and 0.2227 seconds precompiling for 25 choices
[rank3]:W1031 11:02:50.084000 286 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 11:02:50.107000 286 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:50 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:02:50.305000 284 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:50 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1031 11:02:51.216000 287 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank4]:W1031 11:02:51.239000 287 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[rank0]:W1031 11:02:51.300000 283 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:51 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:02:51.323000 283 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:02:51 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:02:51.612000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:51.650000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:51.804000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:51.821000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:51.903000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:51.922000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:51.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:52.100000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:52.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:52.168000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:52.185000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:52.202000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:52.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:52.380000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:52.388000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:52.448000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:52.464000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:52.479000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:52.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:52.659000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:52.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:52.734000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:52.753000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:52.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:52.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:52.940000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:52.948000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:53.009000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:53.037000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:53.045000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:53.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:53.104000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:53.222000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:53.232000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:53.285000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:53.318000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:53.328000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:53.335000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:53.357000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:53.376000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:53.497000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:53.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:53.561000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:53.597000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:53.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:53.615000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:53.632000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:53.648000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:53.772000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:53.793000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:53.836000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:53.872000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:53.889000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:53.898000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:53.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:53.921000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:54.048000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:54.077000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:54.112000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:54.152000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:54.170000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:54.178000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:54.186000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:54.195000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:54.324000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:54.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:54.390000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:54.433000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:54.452000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:54.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:54.469000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:54.477000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:54.613000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:54.644000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:54.665000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:54.709000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:54.736000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:54.744000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:54.752000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:54.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:54.889000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:54.924000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:54.941000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:54.985000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:55.016000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:55.024000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:55.032000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:55.040000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:55.165000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:55.204000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:55.225000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:55.273000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:55.296000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:55.303000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:55.316000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:55.324000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:55.436000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:55.485000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:55.496000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:55.548000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:55.577000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:55.586000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:55.594000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:55.603000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:55.712000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:55.762000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:55.769000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:55.828000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:55.858000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:55.865000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:55.877000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:55.884000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:55.988000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:56.038000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:56.044000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:56.108000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:56.130000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:56.141000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:56.151000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:56.163000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:56.264000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:56.313000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:56.320000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:56.388000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:56.402000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:56.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:56.427000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:56.443000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:56.540000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:56.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:56.596000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:56.668000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:56.678000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:56.694000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:56.703000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:56.723000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:56.817000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:56.864000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:56.874000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:56.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:56.962000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:56.970000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:56.977000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:57.013000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:57.093000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:57.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:57.149000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:57.228000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:57.248000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:57.256000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:57.263000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:57.300000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:57.365000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:57.418000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:57.425000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:57.509000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:57.529000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:57.536000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:57.545000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:57.575000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:57.636000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:57.698000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:57.704000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:57.786000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:57.805000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:57.816000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:57.823000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:57.851000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:57.908000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:57.974000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:57.981000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:58.061000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:58.082000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:58.090000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:58.104000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:58.127000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:58.180000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:58.250000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:58.257000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:58.337000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:58.357000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:58.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:58.384000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:58.403000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:58.452000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:58.526000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:58.533000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:58.613000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:58.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:58.641000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:58.676000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:58.691000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:58.736000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:58.802000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:58.809000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:58.891000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:58.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:58.917000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:58.956000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:58.967000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:59.024000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:59.078000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:59.096000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:59.166000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:59.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:59.193000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:59.236000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:59.243000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:59.300000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:59.354000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:59.372000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:59.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:59.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:59.469000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:59.515000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:59.522000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:59.576000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:59.630000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:59.648000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:59.717000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:02:59.737000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:02:59.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:02:59.795000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:02:59.802000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:02:59.852000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:02:59.906000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:02:59.924000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:02:59.993000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:00.013000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:00.021000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:00.075000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:00.082000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:00.128000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:00.186000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:00.200000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:00.277000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:00.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:00.305000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:00.356000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:00.362000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:00.404000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:00.466000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:00.476000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:00.557000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:00.577000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:00.585000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:00.647000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:00.654000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:00.679000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:00.742000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:00.752000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:00.834000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:00.853000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:00.860000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:00.928000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:00.934000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:00.955000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:01.018000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:01.028000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:01.110000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:01.129000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:01.137000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:01.220000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:01.226000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:01.243000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:01.294000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:01.312000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:01.386000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:01.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:01.417000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:01.499000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:01.506000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:01.520000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:01.570000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:01.588000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:01.662000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:01.689000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:01.697000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:01.779000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:01.786000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:01.796000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:01.846000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:01.864000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:01.938000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:01.969000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:01.977000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:02.060000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:02.066000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:02.073000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:02.122000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:02.140000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:02.214000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:02.249000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:02.257000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:02.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:02.346000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:02.354000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:02.398000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:02.416000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:02.490000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:02.529000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:02.537000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:02.619000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:02.626000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:02.633000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:02.674000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:02.693000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:02.770000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:02.809000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:02.817000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:02.911000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:02.918000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:02.925000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:02.954000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:02.968000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:03.058000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:03.090000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:03.098000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:03.196000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:03.203000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:03.210000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:03.234000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:03.244000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:03.338000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:03.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:03.381000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:03.480000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:03.487000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:03.496000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:03.518000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:03.532000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:03.622000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:03.654000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:03.662000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:03.764000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:03.770000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:03.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:03.803000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:03.810000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:03.906000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:03.938000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:03.946000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:04.047000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:04.055000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:04.061000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:04.082000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:04.089000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:04.186000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:04.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:04.226000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:04.332000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:04.339000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:04.345000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:04.363000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:04.369000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:04.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:04.498000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:04.506000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:04.616000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:04.622000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:04.629000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:04.646000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:04.652000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:04.746000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:04.777000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:04.786000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:04.900000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:04.907000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:04.913000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:04.926000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:04.932000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:05.026000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:05.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:05.066000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:05.183000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:05.190000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:05.197000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:05.208000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:05.215000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:05.306000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:05.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:05.346000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:05.468000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:05.475000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:05.482000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:05.493000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:05.499000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:05.586000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:05.618000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:05.626000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:05.756000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:05.762000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:05.770000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:05.779000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:05.787000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:05.866000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:05.898000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:05.906000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:06.044000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:06.050000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:06.056000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:06.066000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:06.074000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:06.146000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:06.177000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:06.186000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:06.328000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:06.334000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:06.341000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:06.350000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:06.358000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:06.426000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:06.457000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:06.466000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:06.624000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:06.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:06.637000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:06.644000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:06.651000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:06.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:06.737000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:06.745000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:06.908000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:06.916000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:06.922000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:06.929000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:06.936000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:06.986000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:07.018000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:07.026000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:07.192000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:07.200000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:07.206000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:07.212000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:07.219000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:07.266000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:07.298000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:07.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:07.477000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:07.483000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:07.491000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:07.497000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:07.504000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:07.578000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:07.586000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:07.772000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:07.780000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:07.861000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:08.051000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:08.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:08.340000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:08.420000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:08.650000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:08.713000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:08.948000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:09.224000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_580 0.1008 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1009 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_570 0.1011 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1011 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_589 0.1046 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1047 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1079 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1079 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1268 ms 47.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0856 seconds and 0.1908 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_580 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_570 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1084 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1084 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1122 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1122 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1336 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0862 seconds and 0.1929 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_581 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_570 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_588 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1119 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1334 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0776 seconds and 0.1942 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_571 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_580 0.1046 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1046 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1082 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1113 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1322 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1225 seconds and 0.2241 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_570 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1085 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1085 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1121 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1122 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_575 0.1334 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1345 seconds and 0.1808 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_580 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1044 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_588 0.1085 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1085 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1335 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0863 seconds and 0.2018 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_570 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1045 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1090 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_579 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1332 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0462 seconds and 0.2127 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_570 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1046 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1088 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1088 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1117 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1332 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0411 seconds and 0.1893 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=39.08 GB):  96%|| 50/52 [11:26<02:35, 77.89s/it]Capturing batches (bs=2 avail_mem=38.47 GB):  96%|| 50/52 [11:26<02:35, 77.89s/it][rank3]:W1031 11:03:22.226000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:22.288000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:22.305000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:22.368000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:22.388000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:22.414000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:22.467000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:22.478000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:22.512000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:22.575000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:22.591000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:22.699000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:23.006000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:23.073000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:23.084000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:23.151000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:23.151000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:23.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:23.228000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:23.231000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:23.258000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:23.312000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:23.336000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:23.423000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:23.944000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:23.995000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:24.088000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:24.229000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:24.722000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:24.810000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:24.866000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:24.943000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:25.495000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:25.550000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:25.575000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:25.626000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:25.630000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:25.682000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:25.922000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:26.003000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:26.055000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:26.159000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:26.238000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:26.261000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:26.271000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:26.289000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:26.324000 286 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank6]:W1031 11:03:26.342000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:26.352000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:26.384000 283 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank6]:W1031 11:03:26.394000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:26.406000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:26.493000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:26.534000 287 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank5]:W1031 11:03:26.582000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:26.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:26.650000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:26.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:26.785000 285 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1031 11:03:26.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:26.886000 289 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1031 11:03:26.905000 290 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank5]:W1031 11:03:27.132000 288 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1031 11:03:27.298000 284 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_602 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_604 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_610 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_611 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8678 seconds and 0.1340 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_603 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_614 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_615 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_597 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_596 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8405 seconds and 0.1624 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_605 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_615 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_595 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_607 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8785 seconds and 0.1547 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_611 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_613 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_602 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_596 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8683 seconds and 0.1539 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0062 ms 100.0% 
  triton_bmm_606 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_612 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_615 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_593 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_594 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_595 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8745 seconds and 0.1616 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_615 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_593 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_603 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_595 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_605 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9023 seconds and 0.1518 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_605 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_612 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_593 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_595 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9071 seconds and 0.1612 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_602 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_594 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_598 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_610 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8862 seconds and 0.1445 seconds precompiling for 25 choices
[rank0]:W1031 11:03:36.058000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:36.573000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:36.620000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:36.688000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:36.728000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:36.892000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:37.126000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:37.177000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:37.190000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:37.225000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:37.395000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:37.689000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:37.844000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:38.054000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:38.358000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:38.561000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:39.413000 286 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank0]:W1031 11:03:39.459000 283 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank2]:W1031 11:03:39.833000 285 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank6]:W1031 11:03:40.000000 289 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank4]:W1031 11:03:40.152000 287 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank7]:W1031 11:03:40.820000 290 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank5]:W1031 11:03:42.677000 288 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank1]:W1031 11:03:42.689000 284 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0077 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0081 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0084 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7964 seconds and 0.4240 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_628 0.0067 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0069 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 80.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0082 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9034 seconds and 0.4287 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0063 ms 98.7% 
  triton_bmm_629 0.0066 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0075 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0076 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0080 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0080 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0081 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8305 seconds and 0.4073 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_623 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_621 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8903 seconds and 0.4089 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_629 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_623 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8752 seconds and 0.3963 seconds precompiling for 25 choices
[rank0]:W1031 11:03:45.636000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:45.714000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:45.763000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0063 ms 100.0% 
  triton_bmm_618 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8614 seconds and 0.4067 seconds precompiling for 25 choices
[rank6]:W1031 11:03:46.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:46.270000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:46.304000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:46.325000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:46.349000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:46.375000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:46.383000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:46.399000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:46.434000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:46.820000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:46.899000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:46.949000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:47.169000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:47.249000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:47.299000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_623 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8332 seconds and 0.4185 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_619 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_628 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0077 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0077 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0081 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8628 seconds and 0.4106 seconds precompiling for 25 choices
[rank5]:W1031 11:03:48.899000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:48.916000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:48.979000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:48.994000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:49.030000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:49.044000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:50.758000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:50.836000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:50.940000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:51.821000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:51.898000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:52.001000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:52.064000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:52.131000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:52.144000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:52.210000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:52.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:52.325000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:52.475000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:52.554000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:52.657000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:52.881000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:52.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:53.064000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:54.427000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:54.711000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:54.759000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:54.838000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:54.839000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:54.917000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:54.941000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:55.001000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:55.019000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:55.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:55.444000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:55.452000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:55.531000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:55.725000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:55.732000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:55.821000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:55.848000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:56.005000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:56.101000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:56.128000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:56.266000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:56.409000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:56.536000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:56.546000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:56.616000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:03:56.719000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:56.821000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:57.389000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:57.472000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:03:57.574000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:57.619000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:57.668000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:57.699000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:57.747000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:03:57.802000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:03:57.849000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:58.078000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:58.142000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:58.159000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:58.227000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:03:58.270000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:58.441000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:58.498000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:58.514000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:58.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:03:58.684000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:03:58.725000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:03:58.794000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:00.411000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:00.461000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:00.490000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:00.539000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:00.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:00.640000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_643 0.0319 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_653 0.0325 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_642 0.0326 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_652 0.0329 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0469 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0469 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0480 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0484 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_646 0.0587 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1484 seconds and 0.2344 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_652 0.0299 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0301 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0346 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0350 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0445 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0447 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0454 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0455 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0571 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1536 seconds and 0.2247 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_653 0.0290 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0291 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0332 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0338 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0448 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0451 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0452 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_646 0.0563 ms 16.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1261 seconds and 0.2341 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0293 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0296 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0299 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0312 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0445 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0446 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0447 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0450 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_646 0.0537 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1566 seconds and 0.2189 seconds precompiling for 25 choices
[rank0]:W1031 11:04:03.854000 283 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 11:04:03.877000 283 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:03 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0290 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0291 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0300 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0315 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0441 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0442 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0445 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0445 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0537 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1212 seconds and 0.2119 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_653 0.0299 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0303 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0339 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0348 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0443 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0445 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0448 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0451 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.0596 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1348 seconds and 0.2222 seconds precompiling for 25 choices
[rank3]:W1031 11:04:04.512000 286 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 11:04:04.535000 286 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:04 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:04:04.736000 285 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 11:04:04.759000 285 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:04 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1031 11:04:04.914000 289 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 11:04:04.937000 289 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:05 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:04:05.392000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:05.459000 287 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank4]:W1031 11:04:05.482000 287 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:05 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 11:04:05.582000 290 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank7]:W1031 11:04:05.605000 290 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[rank0]:W1031 11:04:05.673000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:05 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:04:05.949000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:06.034000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:06.235000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:06.265000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_653 0.0295 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0297 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0337 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0351 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0445 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0446 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0452 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0459 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.0596 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0731 seconds and 0.2128 seconds precompiling for 25 choices
[rank3]:W1031 11:04:06.314000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_653 0.0290 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0291 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0292 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0311 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0440 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0441 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0449 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0451 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0537 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1251 seconds and 0.2211 seconds precompiling for 25 choices
[rank6]:W1031 11:04:06.421000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:06.529000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:06.562000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:06.602000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:06.707000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:06.821000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:06.846000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:06.881000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:06.972000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:06.988000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:06.995000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:07.101000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:07.126000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:07.163000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:07.256000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:07.272000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:07.279000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:07.385000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:07.410000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:07.446000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:07.484000 284 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank1]:W1031 11:04:07.507000 284 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[rank7]:W1031 11:04:07.541000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:07.556000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:07.563000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:07 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:04:07.669000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:07.694000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:07.726000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:07.800000 288 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank5]:W1031 11:04:07.823000 288 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[rank7]:W1031 11:04:07.825000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:07.840000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:07.847000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:04:07 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:04:07.953000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:07.978000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:08.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:08.109000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:08.121000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:08.129000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:08.237000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:08.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:08.286000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:08.397000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:08.404000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:08.421000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:08.533000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:08.558000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:08.577000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:08.681000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:08.688000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:08.705000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:08.847000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:08.858000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:08.966000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:08.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:08.981000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:08.989000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:09.138000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:09.147000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:09.252000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:09.259000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:09.270000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:09.278000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:09.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:09.447000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:09.482000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:09.542000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:09.558000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:09.566000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:09.617000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:09.778000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:09.834000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:09.846000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:09.906000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:10.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:10.133000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:10.140000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:10.199000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:10.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:10.364000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:10.384000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:10.420000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:10.427000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:10.490000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:10.534000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:10.550000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:10.652000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:10.664000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:10.708000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:10.715000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:10.778000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:10.787000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:10.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:10.834000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:10.944000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:10.951000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:10.996000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:11.003000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:11.062000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:11.072000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:11.102000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:11.122000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:11.244000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:11.251000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:11.296000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:11.303000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:11.346000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:11.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:11.386000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:11.402000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:11.536000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:11.542000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:11.580000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:11.592000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:11.630000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:11.642000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:11.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:11.686000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:11.828000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:11.835000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:11.860000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:11.880000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:11.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:11.959000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:11.974000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:12.117000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:12.125000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:12.175000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:12.221000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:12.249000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:12.261000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:12.410000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:12.419000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:12.470000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:12.513000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:12.536000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:12.548000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:12.569000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:12.702000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:12.711000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:12.770000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:12.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:12.808000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:12.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:12.836000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:12.857000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:12.989000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:12.998000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:13.058000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:13.088000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:13.096000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:13.113000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:13.124000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:13.145000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:13.276000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:13.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:13.344000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:13.376000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:13.385000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:13.402000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:13.414000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:13.434000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:13.564000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:13.571000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:13.638000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:13.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:13.673000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:13.689000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:13.700000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:13.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:13.855000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:13.863000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:13.930000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:13.946000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:13.961000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:13.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:13.988000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:14.009000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:14.143000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:14.151000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:14.217000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:14.228000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:14.250000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:14.266000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:14.278000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:14.298000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:14.428000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:14.436000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:14.505000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:14.512000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:14.538000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:14.554000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:14.564000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:14.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:14.719000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:14.729000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:14.806000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:14.814000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:14.825000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:14.836000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:14.848000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:14.873000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:15.005000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:15.017000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:15.094000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:15.102000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:15.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:15.124000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:15.133000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:15.165000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:15.288000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:15.308000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:15.380000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:15.387000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:15.403000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:15.412000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:15.425000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:15.459000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:15.584000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:15.607000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:15.682000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:15.691000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:15.700000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:15.706000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:15.716000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:15.749000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:15.869000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:15.897000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:15.970000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:15.978000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:15.988000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:15.995000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:16.003000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:16.037000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:16.153000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:16.185000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:16.258000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:16.266000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:16.276000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:16.283000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:16.292000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:16.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:16.438000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:16.473000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:16.546000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:16.554000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:16.565000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:16.571000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:16.580000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:16.612000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:16.722000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:16.761000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:16.834000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:16.842000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:16.853000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:16.859000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:16.868000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:16.900000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:17.006000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:17.049000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:17.123000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:17.131000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:17.141000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:17.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:17.155000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:17.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:17.302000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:17.345000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:17.414000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:17.422000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:17.431000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:17.438000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:17.446000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:17.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:17.586000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:17.633000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:17.702000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:17.710000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:17.721000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:17.727000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:17.736000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:17.764000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:17.882000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:17.933000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:18.002000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:18.011000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:18.019000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:18.027000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:18.034000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:18.053000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:18.166000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:18.221000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:18.290000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:18.298000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:18.308000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:18.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:18.323000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:18.340000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:18.450000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:18.509000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:18.578000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:18.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:18.602000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:18.609000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:18.616000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:18.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:18.737000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:18.800000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:18.864000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:18.871000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:18.895000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:18.903000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:18.912000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:18.923000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:19.020000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:19.088000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:19.156000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:19.163000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:19.182000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:19.192000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:19.202000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:19.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:19.304000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:19.377000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:19.450000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:19.458000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:19.469000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:19.478000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:19.485000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:19.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:19.590000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:19.677000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:19.744000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:19.759000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:19.765000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:19.775000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:19.783000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:19.794000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:19.872000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:19.964000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:20.028000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:20.046000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:20.058000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:20.065000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:20.075000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:20.085000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:20.156000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:20.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:20.316000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:20.335000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:20.343000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:20.356000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:20.365000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:20.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:20.441000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:20.548000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:20.612000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:20.622000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:20.630000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:20.650000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:20.659000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:20.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:20.724000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:20.836000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:20.896000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:20.910000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:20.918000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:20.934000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:20.948000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:20.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:21.008000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:21.124000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:21.180000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:21.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:21.206000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:21.217000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:21.240000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:21.249000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:21.292000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:21.412000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:21.464000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:21.486000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:21.495000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:21.504000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:21.532000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:21.541000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:21.576000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:21.704000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:21.752000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:21.774000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:21.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:21.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:21.824000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:21.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:21.860000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:21.992000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:22.040000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:22.062000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:22.071000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:22.079000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:22.116000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:22.125000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:22.144000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:22.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:22.328000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:22.350000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:22.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:22.368000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:22.413000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:22.423000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:22.442000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:22.614000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:22.636000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:22.644000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:22.652000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:22.700000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:22.714000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:22.730000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:22.909000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:22.930000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:22.939000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:22.948000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:22.994000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:23.010000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:23.196000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:23.226000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:23.236000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:23.245000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:23.286000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:23.301000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:23.487000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:23.521000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:23.537000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:23.589000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:23.782000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:23.817000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:23.829000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:23.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:24.073000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:24.187000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:24.388000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:24.482000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:24.685000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:24.779000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:24.980000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:25.077000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:25.267000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:25.357000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:25.551000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:25.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:25.835000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:25.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:26.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_676 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1044 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1044 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1069 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1114 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1114 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1291 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1862 seconds and 0.1897 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_677 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_667 0.1043 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1044 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_684 0.1068 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1119 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1119 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_671 0.1296 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1396 seconds and 0.1935 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_677 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1044 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_685 0.1067 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1069 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1118 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1120 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1294 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1068 seconds and 0.2097 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0599 ms 100.0% 
  triton_mm_676 0.1044 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1044 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1045 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1045 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_685 0.1065 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1066 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1111 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1111 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1280 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0431 seconds and 0.2020 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_676 0.1009 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1009 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1011 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1011 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_684 0.1035 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1038 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1077 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1077 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1229 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0538 seconds and 0.1904 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_676 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1045 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1068 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1115 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1116 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1291 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1004 seconds and 0.2042 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_677 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1077 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1078 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1120 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1124 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1293 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0841 seconds and 0.1920 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_676 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_666 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_685 0.1076 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1078 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1121 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1122 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1292 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0976 seconds and 0.1819 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.47 GB):  98%|| 51/52 [12:44<01:17, 77.70s/it]Capturing batches (bs=1 avail_mem=37.90 GB):  98%|| 51/52 [12:44<01:17, 77.70s/it][rank4]:W1031 11:04:39.115000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:39.196000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:39.207000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:39.241000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:39.298000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:39.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:39.320000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:39.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:39.401000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:39.409000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:39.430000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:39.460000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:39.460000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:39.513000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:39.539000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:39.540000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:39.647000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:39.660000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:39.661000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:39.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:39.839000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:39.990000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:40.070000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:40.181000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:40.782000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:40.893000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:40.909000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:40.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:41.113000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:41.119000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:41.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:41.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:42.325000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:42.401000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:42.407000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:42.446000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:42.484000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:42.514000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:42.528000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:42.590000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:42.633000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:42.673000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:42.692000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:42.754000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:42.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:42.834000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:42.858000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:42.878000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:42.916000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:42.997000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:43.021000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:43.081000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:43.177000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:43.189000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:43.260000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:43.365000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:47.999000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:48.035000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:48.216000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:48.381000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:48.502000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:48.535000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:48.577000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:48.715000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:48.889000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:48.900000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:49.084000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:49.182000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:49.413000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:49.686000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:49.754000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:50.273000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:51.322000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:51.403000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:51.449000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:51.454000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:51.517000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:51.528000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:51.579000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:51.598000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:51.651000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:52.084000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:52.167000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:52.220000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:52.226000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:52.307000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:52.358000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:52.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:53.057000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:53.108000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:53.153000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:53.233000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:53.284000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:54.345000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:54.426000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:54.478000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:55.489000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:55.567000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:55.670000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:55.806000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:55.896000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:56.001000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:56.716000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:56.796000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:56.902000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:57.000000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:57.079000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:57.182000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:57.458000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:57.540000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:57.557000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:57.637000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:57.646000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:04:57.740000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:58.059000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:58.147000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:58.206000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:04:58.252000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:58.304000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:58.506000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:58.592000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:04:58.786000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:04:58.876000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:59.201000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:59.284000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:04:59.390000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:04:59.785000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:04:59.878000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:04:59.918000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:00.076000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:00.149000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:00.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:00.218000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:00.229000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:00.270000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:00.333000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:00.367000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:00.456000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:00.465000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:00.509000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:00.530000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:00.557000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:00.610000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:00.712000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:00.753000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:00.846000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:01.049000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:01.895000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:02.043000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:02.125000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:02.188000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:02.230000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:02.252000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:02.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:02.340000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:02.421000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:02.432000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:02.440000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:02.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:02.505000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:02.512000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:02.525000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:02.585000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:02.616000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:02.687000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:04.025000 284 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d25020>
[rank1]:W1031 11:05:04.058000 284 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec787d24f90>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:04 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1031 11:05:04.131000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:04.212000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:04.316000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:04.453000 287 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca10b0>
[rank4]:W1031 11:05:04.477000 287 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f15feca0f00>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:04 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:05:05.566000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:05.824000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:05.854000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:06.112000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:06.116000 286 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56f9264bd0>
[rank3]:W1031 11:05:06.139000 286 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f56fa368ae0>
[rank1]:W1031 11:05:06.144000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:06 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1031 11:05:06.228000 285 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd5f0>
[rank2]:W1031 11:05:06.251000 285 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f75b47fd590>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:06 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1031 11:05:06.385000 290 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c918c0>
[rank4]:W1031 11:05:06.400000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:06.408000 290 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f95f3c910b0>
[rank1]:W1031 11:05:06.434000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:06 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1031 11:05:06.634000 283 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249260>
[rank0]:W1031 11:05:06.658000 283 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7efa8e249230>
[rank4]:W1031 11:05:06.701000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:06 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1031 11:05:06.734000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:06.864000 289 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf10750>
[rank6]:W1031 11:05:06.887000 289 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f0c7bf106c0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:06 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1031 11:05:06.992000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:07.022000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:07.280000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:07.314000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:07.574000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:07.603000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:07.661000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:07.774000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:07.864000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:07.902000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:07.950000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:08.051000 288 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145244f60>
[rank2]:W1031 11:05:08.070000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:08.074000 288 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3145245440>
[rank7]:W1031 11:05:08.078000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-31 11:05:08 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1031 11:05:08.152000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:08.181000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:08.202000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:08.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:08.358000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:08.373000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:08.393000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:08.440000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:08.473000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:08.490000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:08.534000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:08.650000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:08.665000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:08.684000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:08.728000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:08.761000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:08.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:08.822000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:08.942000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:08.956000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:08.972000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:09.016000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:09.049000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:09.066000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:09.110000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:09.234000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:09.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:09.264000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:09.308000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:09.341000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:09.358000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:09.398000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:09.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:09.526000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:09.553000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:09.559000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:09.600000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:09.633000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:09.646000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:09.686000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:09.713000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:09.818000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:09.848000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:09.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:09.892000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:09.921000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:09.938000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:09.974000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:10.005000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:10.110000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:10.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:10.152000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:10.180000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:10.209000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:10.230000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:10.266000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:10.301000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:10.414000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:10.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:10.447000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:10.468000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:10.497000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:10.518000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:10.552000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:10.597000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:10.718000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:10.736000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:10.743000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:10.760000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:10.797000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:10.822000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:10.842000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:10.893000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:11.010000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:11.033000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:11.040000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:11.052000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:11.089000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:11.114000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:11.126000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:11.193000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:11.310000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:11.328000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:11.336000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:11.344000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:11.381000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:11.406000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:11.417000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:11.489000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:11.602000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:11.624000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:11.631000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:11.639000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:11.673000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:11.694000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:11.705000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:11.785000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:11.898000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:11.920000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:11.927000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:11.934000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:11.965000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:11.986000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:11.996000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:12.081000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:12.202000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:12.217000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:12.224000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:12.231000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:12.269000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:12.286000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:12.297000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:12.377000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:12.494000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:12.512000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:12.520000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:12.528000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:12.569000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:12.586000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:12.597000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:12.673000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:12.786000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:12.808000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:12.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:12.823000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:12.861000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:12.878000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:12.889000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:12.969000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:13.082000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:13.105000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:13.113000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:13.120000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:13.153000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:13.171000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:13.181000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:13.265000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:13.378000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:13.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:13.408000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:13.415000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:13.445000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:13.462000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:13.473000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:13.561000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:13.673000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:13.699000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:13.708000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:13.716000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:13.736000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:13.756000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:13.764000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:13.859000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:13.968000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:13.995000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:14.004000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:14.012000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:14.032000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:14.048000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:14.056000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:14.155000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:14.264000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:14.286000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:14.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:14.306000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:14.328000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:14.340000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:14.352000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:14.451000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:14.576000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:14.587000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:14.596000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:14.605000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:14.638000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:14.650000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:14.659000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:14.753000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:14.874000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:14.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:14.892000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:14.900000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:14.933000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:14.948000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:14.957000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:15.055000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:15.182000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:15.189000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:15.199000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:15.207000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:15.236000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:15.246000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:15.254000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:15.355000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:15.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:15.489000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:15.499000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:15.507000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:15.532000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:15.540000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:15.649000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:15.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:15.794000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:15.802000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:15.834000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:15.843000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:15.957000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:16.073000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:16.094000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:16.105000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:16.126000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:16.137000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:16.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:16.338000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:16.373000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:16.395000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:16.403000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:16.418000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:16.429000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:16.561000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:16.589000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:16.634000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:16.669000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:16.690000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:16.702000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:16.710000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:16.721000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:16.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:16.884000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:16.930000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:16.965000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:16.986000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:16.998000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:17.006000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:17.015000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:17.161000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:17.180000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:17.238000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:17.261000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:17.282000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:17.291000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:17.302000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:17.310000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:17.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:17.476000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:17.538000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:17.557000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:17.579000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:17.597000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:17.607000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:17.765000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:17.773000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:17.855000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:17.862000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:17.893000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:17.910000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:18.065000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:18.073000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:18.163000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:18.193000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:18.214000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:18.373000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:18.381000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:18.407000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:18.463000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:18.681000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:18.690000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:18.698000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:18.708000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:18.766000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:18.985000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:18.995000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:19.004000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:19.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:19.020000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:19.066000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:19.289000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:19.297000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:19.306000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:19.317000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:19.324000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:19.332000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:19.366000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:19.598000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:19.606000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:19.614000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:19.625000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:19.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:19.640000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:19.662000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:19.904000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:19.913000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:19.923000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:19.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:19.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:19.948000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:19.970000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:20.117000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:20.200000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:20.214000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:20.223000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:20.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:20.241000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:20.250000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:20.266000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:20.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:20.496000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:20.514000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:20.523000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:20.532000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:20.541000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:20.550000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:20.562000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:20.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:20.792000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:20.814000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:20.824000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:20.832000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:20.841000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:20.850000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:20.861000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:21.025000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:21.088000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:21.114000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:21.123000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:21.132000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:21.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:21.150000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:21.160000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:21.329000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:21.388000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:21.426000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:21.436000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:21.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:21.451000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:21.461000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:21.472000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:21.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:21.688000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:21.730000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:21.739000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:21.749000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:21.758000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:21.766000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:21.778000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:21.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:21.988000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:22.030000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:22.039000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:22.049000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:22.057000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:22.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:22.077000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:22.241000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:22.284000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:22.334000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:22.342000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:22.353000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:22.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:22.371000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:22.379000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:22.545000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:22.580000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:22.634000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:22.643000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:22.653000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:22.662000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:22.672000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:22.680000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:22.849000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:22.876000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:22.934000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:22.943000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:22.953000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:22.963000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:22.973000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1031 11:05:22.981000 284 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:23.153000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1031 11:05:23.172000 287 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:23.234000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:23.244000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:23.255000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:23.273000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:23.282000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:23.456000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:23.542000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:23.551000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:23.561000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:23.573000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:23.581000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:23.761000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:23.855000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:23.866000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:23.876000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:23.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:23.894000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:24.065000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:24.158000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:24.169000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:24.179000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:24.190000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:24.198000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:24.368000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:24.458000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:24.474000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:24.483000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:24.493000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:24.507000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:24.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:24.754000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:24.769000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:24.778000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:24.793000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:24.805000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:24.977000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:25.050000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1031 11:05:25.066000 286 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:25.074000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:25.097000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:25.113000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:25.280000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1031 11:05:25.346000 285 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:25.369000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:25.395000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:25.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:25.585000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1031 11:05:25.677000 283 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1031 11:05:25.699000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:25.716000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:25.889000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1031 11:05:26.022000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:26.200000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:26.505000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:26.809000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1031 11:05:27.109000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=37.90 GB): 100%|| 52/52 [13:41<00:00, 71.63s/it]Capturing batches (bs=1 avail_mem=37.90 GB): 100%|| 52/52 [13:41<00:00, 15.80s/it]
[2025-10-31 11:05:33 TP0] Registering 6396 cuda graph addresses
[2025-10-31 11:05:34 TP4] Capture cuda graph end. Time elapsed: 823.68 s. mem usage=6.12 GB. avail mem=37.09 GB.
[2025-10-31 11:05:34 TP3] Capture cuda graph end. Time elapsed: 823.64 s. mem usage=6.09 GB. avail mem=37.08 GB.
[2025-10-31 11:05:34 TP6] Capture cuda graph end. Time elapsed: 823.57 s. mem usage=6.13 GB. avail mem=37.16 GB.
[2025-10-31 11:05:34 TP1] Capture cuda graph end. Time elapsed: 823.70 s. mem usage=6.10 GB. avail mem=37.06 GB.
[2025-10-31 11:05:34 TP7] Capture cuda graph end. Time elapsed: 823.73 s. mem usage=6.12 GB. avail mem=37.17 GB.
[2025-10-31 11:05:34 TP5] Capture cuda graph end. Time elapsed: 823.68 s. mem usage=6.19 GB. avail mem=37.11 GB.
[2025-10-31 11:05:34 TP2] Capture cuda graph end. Time elapsed: 823.69 s. mem usage=6.10 GB. avail mem=37.06 GB.
[2025-10-31 11:05:34 TP0] Capture cuda graph end. Time elapsed: 823.84 s. mem usage=6.11 GB. avail mem=37.47 GB.
[2025-10-31 11:05:34 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=37.47 GB
[2025-10-31 11:05:35] INFO:     Started server process [43]
[2025-10-31 11:05:35] INFO:     Waiting for application startup.
[2025-10-31 11:05:35] INFO:     Application startup complete.
[2025-10-31 11:05:35] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-31 11:05:36] INFO:     Shutting down
[2025-10-31 11:05:36] INFO:     Waiting for application shutdown.
[2025-10-31 11:07:35] Initialization failed. warmup error: Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9c56e02a80>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=30000): Max retries exceeded with url: /get_model_info (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9c56e02a80>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py", line 1446, in _execute_server_warmup
    res = requests.get(url + "/get_model_info", timeout=5, headers=headers)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=30000): Max retries exceeded with url: /get_model_info (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9c56e02a80>: Failed to establish a new connection: [Errno 111] Connection refused'))

