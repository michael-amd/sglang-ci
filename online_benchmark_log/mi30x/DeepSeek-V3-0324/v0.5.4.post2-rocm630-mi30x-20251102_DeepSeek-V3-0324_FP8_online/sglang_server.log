INFO 11-02 15:15:42 __init__.py:179] Automatically detected platform rocm.
WARNING 11-02 15:15:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-02 15:15:42] WARNING server_args.py:1153: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-02 15:15:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:43] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=479366308, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=-1, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0)
[2025-11-02 15:15:43] Using default HuggingFace chat template with detected content format: string
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-02 15:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-02 15:15:53 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-11-02 15:15:53 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-02 15:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:53 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-02 15:15:53 TP2] Init torch distributed begin.
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-02 15:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-02 15:15:53 TP0] Init torch distributed begin.
[2025-11-02 15:15:54 TP1] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-02 15:15:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-02 15:15:54 __init__.py:179] Automatically detected platform rocm.
INFO 11-02 15:15:54 __init__.py:179] Automatically detected platform rocm.
[2025-11-02 15:15:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-02 15:15:54 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-11-02 15:15:54 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-02 15:15:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:54] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-02 15:15:54 TP7] Init torch distributed begin.
[2025-11-02 15:15:54 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-02 15:15:54 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-02 15:15:54 TP4] Init torch distributed begin.
[2025-11-02 15:15:54 TP3] Init torch distributed begin.
[2025-11-02 15:15:54 TP6] Init torch distributed begin.
[2025-11-02 15:15:54 TP5] Init torch distributed begin.
[2025-11-02 15:15:54 TP0] sglang is using nccl==2.21.5
[2025-11-02 15:15:56 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-02 15:15:56 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-02 15:15:56 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-02 15:15:56 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-02 15:15:56 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-02 15:15:56 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-02 15:15:56 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-02 15:15:56 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-02 15:15:58 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-02 15:15:58 TP7] Load weight begin. avail mem=187.32 GB
[2025-11-02 15:15:58 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-02 15:15:58 TP6] Load weight begin. avail mem=187.31 GB
[2025-11-02 15:15:58 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-02 15:15:58 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-02 15:15:58 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-02 15:15:58 TP0] Detected fp8 checkpoint.
[2025-11-02 15:15:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-02 15:15:58 TP1] Load weight begin. avail mem=187.19 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:29,  5.53it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:14, 11.01it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:11, 13.89it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:00<00:11, 13.27it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:23,  6.55it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:17,  8.56it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:01<00:14, 10.33it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:01<00:12, 11.89it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:01<00:10, 13.98it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:01<00:09, 14.58it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:01<00:09, 15.14it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:02<00:10, 12.75it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:02<00:09, 14.15it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:02<00:09, 13.88it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:02<00:08, 16.04it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:02<00:09, 13.82it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:19,  6.57it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:03<00:14,  8.75it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:03<00:12, 10.09it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:03<00:12, 10.00it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:03<00:10, 11.13it/s]
Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:04<00:10, 11.45it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:04<00:08, 13.83it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:04<00:07, 15.03it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:04<00:09, 12.18it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:04<00:07, 14.89it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:04<00:06, 15.72it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:05<00:07, 13.93it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:05<00:05, 16.78it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:05<00:05, 16.83it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:06<00:13,  6.86it/s]
Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:06<00:10,  9.16it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:06<00:08, 10.23it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:06<00:08, 10.46it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:06<00:07, 11.95it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:06<00:05, 14.74it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:06<00:06, 13.30it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:07<00:05, 13.77it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:07<00:05, 14.80it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:07<00:05, 13.89it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:07<00:04, 15.11it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:07<00:03, 17.50it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:07<00:04, 14.78it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:07<00:04, 15.77it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:08<00:04, 15.53it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:08<00:06,  9.69it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:08<00:05, 11.36it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:09<00:09,  5.90it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:09<00:07,  7.40it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:09<00:06,  8.76it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:09<00:04, 11.77it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:09<00:03, 12.99it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:09<00:02, 15.37it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:02, 16.82it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:10<00:02, 17.39it/s]
Loading safetensors checkpoint shards:  77% Completed | 125/163 [00:10<00:02, 18.75it/s]
Loading safetensors checkpoint shards:  79% Completed | 128/163 [00:10<00:01, 20.07it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:10<00:02, 13.41it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:10<00:02, 14.25it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:11<00:01, 16.15it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:02,  9.77it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:11<00:02, 11.05it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:11<00:01, 11.95it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:11<00:01, 12.61it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:01, 13.66it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:12<00:00, 15.22it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:13<00:01,  6.33it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:01,  7.46it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00,  8.85it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:13<00:00,  9.50it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:13<00:00, 10.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.78it/s]

[2025-11-02 15:16:42 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-02 15:16:42 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-11-02 15:16:42 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-02 15:16:43 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-11-02 15:16:54 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-11-02 15:16:54 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-11-02 15:16:54 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-11-02 15:16:54 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-11-02 15:16:54 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-02 15:16:54 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP7] Memory pool end. avail mem=43.50 GB
[2025-11-02 15:16:54 TP1] Memory pool end. avail mem=43.37 GB
[2025-11-02 15:16:54 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP0] Memory pool end. avail mem=43.78 GB
[2025-11-02 15:16:54 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP4] Memory pool end. avail mem=43.42 GB
[2025-11-02 15:16:54 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP6] Memory pool end. avail mem=43.49 GB
[2025-11-02 15:16:54 TP5] Memory pool end. avail mem=43.51 GB
[2025-11-02 15:16:54 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-02 15:16:54 TP3] Memory pool end. avail mem=43.37 GB
[2025-11-02 15:16:54 TP2] Memory pool end. avail mem=43.36 GB
[2025-11-02 15:16:56 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-02 15:16:56 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-11-02 15:16:56 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-02 15:16:56 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-02 15:16:56 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
[2025-11-02 15:16:56 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-02 15:16:56 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-11-02 15:16:56 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-11-02 15:16:57 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:58 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:59 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-02 15:16:59 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:16:59 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:00 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:00 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:00 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:00 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:01 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:01 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:01 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:01 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:01 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-02 15:17:01 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:02 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:02 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:02 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:02 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:02 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:05<04:22,  5.14s/it]Capturing batches (bs=496 avail_mem=42.28 GB):   2%|         | 1/52 [00:05<04:22,  5.14s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:02 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.28 GB):   4%|         | 2/52 [00:05<02:09,  2.59s/it]Capturing batches (bs=480 avail_mem=42.27 GB):   4%|         | 2/52 [00:05<02:09,  2.59s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.27 GB):   6%|         | 3/52 [00:06<01:17,  1.59s/it]Capturing batches (bs=464 avail_mem=42.27 GB):   6%|         | 3/52 [00:06<01:17,  1.59s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:03 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.27 GB):   8%|         | 4/52 [00:06<00:53,  1.12s/it]Capturing batches (bs=448 avail_mem=42.26 GB):   8%|         | 4/52 [00:06<00:53,  1.12s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.26 GB):  10%|         | 5/52 [00:07<00:40,  1.16it/s]Capturing batches (bs=432 avail_mem=42.26 GB):  10%|         | 5/52 [00:07<00:40,  1.16it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.26 GB):  12%|        | 6/52 [00:07<00:32,  1.41it/s]Capturing batches (bs=416 avail_mem=42.25 GB):  12%|        | 6/52 [00:07<00:32,  1.41it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:04 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.25 GB):  13%|        | 7/52 [00:07<00:27,  1.64it/s]Capturing batches (bs=400 avail_mem=42.25 GB):  13%|        | 7/52 [00:07<00:27,  1.64it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.25 GB):  15%|        | 8/52 [00:08<00:23,  1.84it/s]Capturing batches (bs=384 avail_mem=42.24 GB):  15%|        | 8/52 [00:08<00:23,  1.84it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:05 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.24 GB):  17%|        | 9/52 [00:08<00:19,  2.20it/s]Capturing batches (bs=368 avail_mem=42.24 GB):  17%|        | 9/52 [00:08<00:19,  2.20it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.24 GB):  19%|        | 10/52 [00:09<00:18,  2.27it/s]Capturing batches (bs=352 avail_mem=42.23 GB):  19%|        | 10/52 [00:09<00:18,  2.27it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.23 GB):  21%|        | 11/52 [00:09<00:17,  2.32it/s]Capturing batches (bs=336 avail_mem=42.23 GB):  21%|        | 11/52 [00:09<00:17,  2.32it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:06 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.23 GB):  23%|       | 12/52 [00:09<00:16,  2.36it/s]Capturing batches (bs=320 avail_mem=42.22 GB):  23%|       | 12/52 [00:09<00:16,  2.36it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.22 GB):  25%|       | 13/52 [00:10<00:14,  2.67it/s]Capturing batches (bs=304 avail_mem=42.22 GB):  25%|       | 13/52 [00:10<00:14,  2.67it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.22 GB):  27%|       | 14/52 [00:10<00:14,  2.58it/s]Capturing batches (bs=288 avail_mem=42.22 GB):  27%|       | 14/52 [00:10<00:14,  2.58it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:07 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.22 GB):  29%|       | 15/52 [00:10<00:12,  2.86it/s]Capturing batches (bs=272 avail_mem=42.21 GB):  29%|       | 15/52 [00:10<00:12,  2.86it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.21 GB):  31%|       | 16/52 [00:11<00:13,  2.72it/s]Capturing batches (bs=256 avail_mem=42.21 GB):  31%|       | 16/52 [00:11<00:13,  2.72it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:08 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:08 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.21 GB):  33%|      | 17/52 [00:11<00:13,  2.63it/s]Capturing batches (bs=248 avail_mem=42.20 GB):  33%|      | 17/52 [00:11<00:13,  2.63it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.20 GB):  35%|      | 18/52 [00:12<00:13,  2.57it/s]Capturing batches (bs=240 avail_mem=42.20 GB):  35%|      | 18/52 [00:12<00:13,  2.57it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.20 GB):  37%|      | 19/52 [00:12<00:13,  2.53it/s]Capturing batches (bs=232 avail_mem=42.19 GB):  37%|      | 19/52 [00:12<00:13,  2.53it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:09 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.19 GB):  38%|      | 20/52 [00:12<00:12,  2.51it/s]Capturing batches (bs=224 avail_mem=42.19 GB):  38%|      | 20/52 [00:12<00:12,  2.51it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.19 GB):  40%|      | 21/52 [00:13<00:12,  2.49it/s]Capturing batches (bs=216 avail_mem=42.18 GB):  40%|      | 21/52 [00:13<00:12,  2.49it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.18 GB):  42%|     | 22/52 [00:13<00:12,  2.48it/s]Capturing batches (bs=208 avail_mem=42.18 GB):  42%|     | 22/52 [00:13<00:12,  2.48it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:10 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.18 GB):  44%|     | 23/52 [00:13<00:10,  2.76it/s]Capturing batches (bs=200 avail_mem=42.17 GB):  44%|     | 23/52 [00:13<00:10,  2.76it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.17 GB):  46%|     | 24/52 [00:14<00:10,  2.66it/s]Capturing batches (bs=192 avail_mem=42.17 GB):  46%|     | 24/52 [00:14<00:10,  2.66it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.17 GB):  48%|     | 25/52 [00:14<00:09,  2.92it/s]Capturing batches (bs=184 avail_mem=42.16 GB):  48%|     | 25/52 [00:14<00:09,  2.92it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:11 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.16 GB):  50%|     | 26/52 [00:14<00:08,  3.13it/s]Capturing batches (bs=176 avail_mem=42.16 GB):  50%|     | 26/52 [00:14<00:08,  3.13it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.16 GB):  52%|    | 27/52 [00:15<00:07,  3.30it/s]Capturing batches (bs=168 avail_mem=42.16 GB):  52%|    | 27/52 [00:15<00:07,  3.30it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.16 GB):  54%|    | 28/52 [00:15<00:08,  2.99it/s]Capturing batches (bs=160 avail_mem=42.15 GB):  54%|    | 28/52 [00:15<00:08,  2.99it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:12 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.15 GB):  56%|    | 29/52 [00:15<00:07,  3.16it/s]Capturing batches (bs=152 avail_mem=42.15 GB):  56%|    | 29/52 [00:15<00:07,  3.16it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.15 GB):  58%|    | 30/52 [00:16<00:07,  2.91it/s]Capturing batches (bs=144 avail_mem=42.15 GB):  58%|    | 30/52 [00:16<00:07,  2.91it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.15 GB):  60%|    | 31/52 [00:16<00:06,  3.13it/s]Capturing batches (bs=136 avail_mem=42.14 GB):  60%|    | 31/52 [00:16<00:06,  3.13it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:13 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.14 GB):  62%|   | 32/52 [00:16<00:06,  3.31it/s]Capturing batches (bs=128 avail_mem=42.14 GB):  62%|   | 32/52 [00:16<00:06,  3.31it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:14 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.14 GB):  63%|   | 33/52 [00:17<00:05,  3.41it/s]Capturing batches (bs=120 avail_mem=42.14 GB):  63%|   | 33/52 [00:17<00:05,  3.41it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.14 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s]Capturing batches (bs=112 avail_mem=42.13 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:14 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.13 GB):  67%|   | 35/52 [00:17<00:05,  3.21it/s]Capturing batches (bs=104 avail_mem=42.13 GB):  67%|   | 35/52 [00:17<00:05,  3.21it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.13 GB):  69%|   | 36/52 [00:18<00:05,  2.94it/s]Capturing batches (bs=96 avail_mem=42.12 GB):  69%|   | 36/52 [00:18<00:05,  2.94it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.12 GB):  71%|   | 37/52 [00:18<00:04,  3.14it/s]Capturing batches (bs=88 avail_mem=42.12 GB):  71%|   | 37/52 [00:18<00:04,  3.14it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:15 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.12 GB):  73%|  | 38/52 [00:18<00:04,  2.89it/s]Capturing batches (bs=80 avail_mem=42.12 GB):  73%|  | 38/52 [00:18<00:04,  2.89it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.12 GB):  75%|  | 39/52 [00:19<00:04,  3.11it/s]Capturing batches (bs=72 avail_mem=42.11 GB):  75%|  | 39/52 [00:19<00:04,  3.11it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.11 GB):  77%|  | 40/52 [00:19<00:04,  2.84it/s]Capturing batches (bs=64 avail_mem=42.11 GB):  77%|  | 40/52 [00:19<00:04,  2.84it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:16 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:17:16 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:16 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.11 GB):  79%|  | 41/52 [00:19<00:03,  3.04it/s]Capturing batches (bs=56 avail_mem=42.10 GB):  79%|  | 41/52 [00:19<00:03,  3.04it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.10 GB):  81%|  | 42/52 [00:20<00:03,  2.84it/s]Capturing batches (bs=48 avail_mem=42.10 GB):  81%|  | 42/52 [00:20<00:03,  2.84it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.10 GB):  83%| | 43/52 [00:20<00:02,  3.07it/s]Capturing batches (bs=40 avail_mem=42.09 GB):  83%| | 43/52 [00:20<00:02,  3.07it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:17 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.09 GB):  85%| | 44/52 [00:20<00:02,  2.86it/s]Capturing batches (bs=32 avail_mem=42.09 GB):  85%| | 44/52 [00:20<00:02,  2.86it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.09 GB):  87%| | 45/52 [00:21<00:02,  3.09it/s]Capturing batches (bs=24 avail_mem=42.08 GB):  87%| | 45/52 [00:21<00:02,  3.09it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.08 GB):  88%| | 46/52 [00:21<00:02,  2.85it/s]Capturing batches (bs=16 avail_mem=42.08 GB):  88%| | 46/52 [00:21<00:02,  2.85it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:17:18 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:18 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.08 GB):  90%| | 47/52 [00:21<00:01,  3.06it/s]Capturing batches (bs=12 avail_mem=42.07 GB):  90%| | 47/52 [00:21<00:01,  3.06it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.07 GB):  92%|| 48/52 [00:22<00:01,  3.27it/s]Capturing batches (bs=8 avail_mem=42.07 GB):  92%|| 48/52 [00:22<00:01,  3.27it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.07 GB):  94%|| 49/52 [00:22<00:00,  3.43it/s]Capturing batches (bs=4 avail_mem=42.07 GB):  94%|| 49/52 [00:22<00:00,  3.43it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.07 GB):  96%|| 50/52 [00:22<00:00,  3.52it/s]Capturing batches (bs=2 avail_mem=42.07 GB):  96%|| 50/52 [00:22<00:00,  3.52it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:19 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.07 GB):  98%|| 51/52 [00:22<00:00,  3.62it/s]Capturing batches (bs=1 avail_mem=42.06 GB):  98%|| 51/52 [00:22<00:00,  3.62it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:20 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:23<00:00,  2.41it/s]Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:23<00:00,  2.21it/s]
[2025-11-02 15:17:21 TP0] Registering 6396 cuda graph addresses
[2025-11-02 15:17:21 TP3] Capture cuda graph end. Time elapsed: 24.82 s. mem usage=1.52 GB. avail mem=41.64 GB.
[2025-11-02 15:17:21 TP7] Capture cuda graph end. Time elapsed: 24.84 s. mem usage=1.52 GB. avail mem=41.77 GB.
[2025-11-02 15:17:21 TP5] Capture cuda graph end. Time elapsed: 23.93 s. mem usage=1.52 GB. avail mem=41.78 GB.
[2025-11-02 15:17:21 TP6] Capture cuda graph end. Time elapsed: 24.87 s. mem usage=1.52 GB. avail mem=41.76 GB.
[2025-11-02 15:17:21 TP4] Capture cuda graph end. Time elapsed: 24.86 s. mem usage=1.52 GB. avail mem=41.69 GB.
[2025-11-02 15:17:21 TP1] Capture cuda graph end. Time elapsed: 24.83 s. mem usage=1.52 GB. avail mem=41.64 GB.
[2025-11-02 15:17:21 TP0] Capture cuda graph end. Time elapsed: 24.82 s. mem usage=1.52 GB. avail mem=42.05 GB.
[2025-11-02 15:17:21 TP2] Capture cuda graph end. Time elapsed: 25.05 s. mem usage=1.52 GB. avail mem=41.63 GB.
[2025-11-02 15:17:21 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.05 GB
[2025-11-02 15:17:22] INFO:     Started server process [47]
[2025-11-02 15:17:22] INFO:     Waiting for application startup.
[2025-11-02 15:17:22] INFO:     Application startup complete.
[2025-11-02 15:17:22] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-02 15:17:23] INFO:     127.0.0.1:52592 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:17:23 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:17:24] INFO:     127.0.0.1:52614 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:24 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:26] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:26] The server is fired up and ready to roll!
[2025-11-02 15:17:31] INFO:     127.0.0.1:45878 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:17:31 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:31 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:31 TP0] Prefill batch, #new-seq: 43, #new-token: 2612, #cached-token: 28681, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:32 TP0] Prefill batch, #new-seq: 140, #new-token: 8542, #cached-token: 93660, token usage: 0.00, #running-req: 44, #queue-req: 0, 
[2025-11-02 15:17:32 TP0] Prefill batch, #new-seq: 137, #new-token: 8023, #cached-token: 91694, token usage: 0.01, #running-req: 184, #queue-req: 0, 
[2025-11-02 15:17:33 TP0] Prefill batch, #new-seq: 276, #new-token: 16381, #cached-token: 184776, token usage: 0.02, #running-req: 321, #queue-req: 79, 
[2025-11-02 15:17:34 TP0] Prefill batch, #new-seq: 275, #new-token: 16340, #cached-token: 184176, token usage: 0.04, #running-req: 597, #queue-req: 147, 
[2025-11-02 15:17:36 TP0] Prefill batch, #new-seq: 152, #new-token: 9631, #cached-token: 101809, token usage: 0.05, #running-req: 872, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:36 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:36 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:36 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:36 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:36 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:36 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-02 15:17:37 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:37 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:17:39] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP0] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:39 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:40 TP0] Decode batch, #running-req: 1024, #token: 95621, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1781.47, #queue-req: 294, 
[2025-11-02 15:17:40] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-02 15:17:41] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 4, #new-token: 240, #cached-token: 2679, token usage: 0.11, #running-req: 1020, #queue-req: 289, 
[2025-11-02 15:17:41] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 4, #new-token: 279, #cached-token: 2677, token usage: 0.11, #running-req: 1020, #queue-req: 285, 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (279, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 5, #new-token: 326, #cached-token: 3353, token usage: 0.11, #running-req: 1019, #queue-req: 280, 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 3, #new-token: 171, #cached-token: 2010, token usage: 0.11, #running-req: 1021, #queue-req: 277, 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (171, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:47172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:41] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:41 TP0] Prefill batch, #new-seq: 8, #new-token: 426, #cached-token: 5363, token usage: 0.11, #running-req: 1016, #queue-req: 269, 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (426, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42 TP0] Prefill batch, #new-seq: 5, #new-token: 287, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 264, 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (287, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] Prefill batch, #new-seq: 7, #new-token: 333, #cached-token: 4688, token usage: 0.11, #running-req: 1017, #queue-req: 257, 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (333, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:51942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42 TP0] Prefill batch, #new-seq: 4, #new-token: 209, #cached-token: 2682, token usage: 0.11, #running-req: 1020, #queue-req: 253, 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (209, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] Prefill batch, #new-seq: 11, #new-token: 664, #cached-token: 7368, token usage: 0.11, #running-req: 1013, #queue-req: 242, 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:48808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:42] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] Prefill batch, #new-seq: 6, #new-token: 479, #cached-token: 4022, token usage: 0.11, #running-req: 1018, #queue-req: 236, 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP3] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP2] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP6] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP0] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP1] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP5] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP4] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:42 TP7] [fused_moe] using default for (479, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43 TP0] Prefill batch, #new-seq: 6, #new-token: 387, #cached-token: 4024, token usage: 0.11, #running-req: 1018, #queue-req: 230, 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43 TP0] Prefill batch, #new-seq: 11, #new-token: 690, #cached-token: 7368, token usage: 0.11, #running-req: 1013, #queue-req: 219, 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43 TP0] Prefill batch, #new-seq: 8, #new-token: 399, #cached-token: 5355, token usage: 0.12, #running-req: 1016, #queue-req: 211, 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (399, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] Prefill batch, #new-seq: 10, #new-token: 523, #cached-token: 6698, token usage: 0.12, #running-req: 1014, #queue-req: 201, 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:43 TP0] Prefill batch, #new-seq: 10, #new-token: 564, #cached-token: 6699, token usage: 0.12, #running-req: 1014, #queue-req: 191, 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:43 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44 TP0] Prefill batch, #new-seq: 6, #new-token: 414, #cached-token: 4018, token usage: 0.12, #running-req: 1018, #queue-req: 185, 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP3] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP6] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP0] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP2] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP5] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP4] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP1] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP7] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44 TP0] Prefill batch, #new-seq: 6, #new-token: 346, #cached-token: 4020, token usage: 0.12, #running-req: 1018, #queue-req: 179, 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP3] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP6] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP2] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP5] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP0] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP1] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP4] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP7] [fused_moe] using default for (346, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44 TP0] Prefill batch, #new-seq: 10, #new-token: 528, #cached-token: 6699, token usage: 0.12, #running-req: 1014, #queue-req: 169, 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP3] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP6] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP0] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP2] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP5] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP4] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP1] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP7] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44 TP0] Prefill batch, #new-seq: 5, #new-token: 216, #cached-token: 3350, token usage: 0.12, #running-req: 1019, #queue-req: 164, 
[2025-11-02 15:17:44] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:44 TP0] Prefill batch, #new-seq: 8, #new-token: 441, #cached-token: 5359, token usage: 0.12, #running-req: 1016, #queue-req: 156, 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP3] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP2] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP0] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP6] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP1] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP5] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP4] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:44 TP7] [fused_moe] using default for (441, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45 TP0] Prefill batch, #new-seq: 8, #new-token: 481, #cached-token: 5359, token usage: 0.12, #running-req: 1016, #queue-req: 148, 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (481, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45] INFO:     127.0.0.1:46222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45 TP0] Prefill batch, #new-seq: 7, #new-token: 541, #cached-token: 4691, token usage: 0.12, #running-req: 1017, #queue-req: 141, 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] Prefill batch, #new-seq: 12, #new-token: 599, #cached-token: 8042, token usage: 0.12, #running-req: 1012, #queue-req: 129, 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45 TP0] Prefill batch, #new-seq: 8, #new-token: 596, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 121, 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45] INFO:     127.0.0.1:48328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:45 TP0] Prefill batch, #new-seq: 11, #new-token: 695, #cached-token: 7372, token usage: 0.12, #running-req: 1013, #queue-req: 110, 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP3] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP2] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP6] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP0] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP1] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP4] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP5] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:45 TP7] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46 TP0] Prefill batch, #new-seq: 10, #new-token: 802, #cached-token: 6702, token usage: 0.12, #running-req: 1014, #queue-req: 100, 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP3] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP2] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP6] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP1] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP4] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP5] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP7] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] Prefill batch, #new-seq: 16, #new-token: 813, #cached-token: 10721, token usage: 0.12, #running-req: 1008, #queue-req: 84, 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP3] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP2] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP1] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP4] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP6] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP5] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP7] [fused_moe] using default for (813, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46 TP0] Prefill batch, #new-seq: 6, #new-token: 346, #cached-token: 4021, token usage: 0.12, #running-req: 1018, #queue-req: 78, 
[2025-11-02 15:17:46] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46 TP0] Prefill batch, #new-seq: 11, #new-token: 781, #cached-token: 7366, token usage: 0.12, #running-req: 1013, #queue-req: 67, 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:51702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:46] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:46 TP0] Prefill batch, #new-seq: 20, #new-token: 1347, #cached-token: 13396, token usage: 0.12, #running-req: 1004, #queue-req: 47, 
[2025-11-02 15:17:47] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] Prefill batch, #new-seq: 13, #new-token: 755, #cached-token: 8708, token usage: 0.12, #running-req: 1011, #queue-req: 34, 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47 TP0] Prefill batch, #new-seq: 11, #new-token: 627, #cached-token: 7374, token usage: 0.13, #running-req: 1013, #queue-req: 23, 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] Prefill batch, #new-seq: 19, #new-token: 1162, #cached-token: 12726, token usage: 0.12, #running-req: 1005, #queue-req: 4, 
[2025-11-02 15:17:47 TP0] Decode batch, #running-req: 1005, #token: 120890, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5541.84, #queue-req: 4, 
[2025-11-02 15:17:47] INFO:     127.0.0.1:46396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] Prefill batch, #new-seq: 4, #new-token: 210, #cached-token: 2684, token usage: 0.13, #running-req: 1015, #queue-req: 0, 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (210, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:47] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:47 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:47528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (977, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (969, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:48] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP4] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP5] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP1] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP0] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP2] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP6] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP3] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:48 TP7] [fused_moe] using default for (914, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:54958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:49] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP6] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP2] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP4] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP0] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP5] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP1] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP3] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:49 TP7] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:45942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (794, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (787, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:45958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:47246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:50] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP0] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP4] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP5] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP6] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP2] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP1] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP3] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:50 TP7] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (723, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (707, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (684, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (623, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:51] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:51 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] Decode batch, #running-req: 610, #token: 91784, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7566.37, #queue-req: 0, 
[2025-11-02 15:17:52] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:47042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:46026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:57116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:47658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP2] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP4] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP6] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP0] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP5] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP1] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP3] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52 TP7] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:17:52] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:51666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:52] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:45986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:49766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:53] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54 TP0] Decode batch, #running-req: 282, #token: 53070, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6033.06, #queue-req: 0, 
[2025-11-02 15:17:54] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:54] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:52884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:46232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:55] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:51136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56 TP0] Decode batch, #running-req: 109, #token: 25399, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3808.03, #queue-req: 0, 
[2025-11-02 15:17:56] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:56] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:57] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:46710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58 TP0] Decode batch, #running-req: 39, #token: 10994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1771.79, #queue-req: 0, 
[2025-11-02 15:17:58] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:58] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59 TP0] Decode batch, #running-req: 11, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 706.27, #queue-req: 0, 
[2025-11-02 15:17:59] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:17:59] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:00 TP0] Decode batch, #running-req: 2, #token: 1355, token usage: 0.00, cuda graph: True, gen throughput (token/s): 208.54, #queue-req: 0, 
[2025-11-02 15:18:00] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:00] INFO:     127.0.0.1:47778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:13] INFO:     127.0.0.1:43270 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:18:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:18:13] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:18:13 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27662, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:13 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32813, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37109, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39455, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42825, token usage: 0.02, #running-req: 189, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44336, token usage: 0.02, #running-req: 248, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48113, token usage: 0.02, #running-req: 309, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 54054, token usage: 0.03, #running-req: 375, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49195, token usage: 0.03, #running-req: 449, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:14 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 83, #new-token: 83, #cached-token: 60341, token usage: 0.04, #running-req: 517, #queue-req: 0, 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP3] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP2] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP4] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP6] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP1] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP5] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP7] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52322, token usage: 0.04, #running-req: 600, #queue-req: 0, 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 89, #new-token: 89, #cached-token: 64836, token usage: 0.05, #running-req: 672, #queue-req: 0, 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP3] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP2] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP6] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP1] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP4] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP5] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP7] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 77, #new-token: 77, #cached-token: 56395, token usage: 0.05, #running-req: 761, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 96, #new-token: 96, #cached-token: 69630, token usage: 0.06, #running-req: 838, #queue-req: 0, 
[2025-11-02 15:18:15 TP0] Prefill batch, #new-seq: 75, #new-token: 75, #cached-token: 55124, token usage: 0.06, #running-req: 934, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:15 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10986, token usage: 0.06, #running-req: 1009, #queue-req: 19, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:16 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:17 TP0] Decode batch, #running-req: 1024, #token: 80190, token usage: 0.08, cuda graph: False, gen throughput (token/s): 990.18, #queue-req: 295, 
[2025-11-02 15:18:18] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-02 15:18:19] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-02 15:18:19] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-02 15:18:19] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-11-02 15:18:20] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:43328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:47718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 3034, token usage: 0.11, #running-req: 1020, #queue-req: 287, 
[2025-11-02 15:18:20] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5849, token usage: 0.11, #running-req: 1016, #queue-req: 279, 
[2025-11-02 15:18:20] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:44458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:20] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2939, token usage: 0.11, #running-req: 1020, #queue-req: 275, 
[2025-11-02 15:18:21] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5026, token usage: 0.11, #running-req: 1017, #queue-req: 268, 
[2025-11-02 15:18:21] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4364, token usage: 0.11, #running-req: 1018, #queue-req: 262, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3573, token usage: 0.11, #running-req: 1019, #queue-req: 257, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21] INFO:     127.0.0.1:43604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2185, token usage: 0.11, #running-req: 1021, #queue-req: 254, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:21] INFO:     127.0.0.1:44050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:44080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:45660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:21] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6579, token usage: 0.11, #running-req: 1015, #queue-req: 245, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:22] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5912, token usage: 0.11, #running-req: 1016, #queue-req: 237, 
[2025-11-02 15:18:22] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4386, token usage: 0.11, #running-req: 1018, #queue-req: 231, 
[2025-11-02 15:18:22] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:44526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6622, token usage: 0.12, #running-req: 1015, #queue-req: 222, 
[2025-11-02 15:18:22] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5791, token usage: 0.12, #running-req: 1016, #queue-req: 214, 
[2025-11-02 15:18:22] INFO:     127.0.0.1:43862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:22] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7850, token usage: 0.12, #running-req: 1013, #queue-req: 203, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:23] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8041, token usage: 0.12, #running-req: 1013, #queue-req: 192, 
[2025-11-02 15:18:23] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6555, token usage: 0.12, #running-req: 1015, #queue-req: 183, 
[2025-11-02 15:18:23] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5815, token usage: 0.12, #running-req: 1016, #queue-req: 175, 
[2025-11-02 15:18:23] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6450, token usage: 0.12, #running-req: 1015, #queue-req: 166, 
[2025-11-02 15:18:23 TP0] Decode batch, #running-req: 1015, #token: 114855, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6865.64, #queue-req: 166, 
[2025-11-02 15:18:23] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:46744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:23 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4351, token usage: 0.12, #running-req: 1018, #queue-req: 160, 
[2025-11-02 15:18:24] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2894, token usage: 0.12, #running-req: 1020, #queue-req: 156, 
[2025-11-02 15:18:24] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3582, token usage: 0.12, #running-req: 1019, #queue-req: 151, 
[2025-11-02 15:18:24] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:45840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8163, token usage: 0.12, #running-req: 1013, #queue-req: 140, 
[2025-11-02 15:18:24] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:45402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10150, token usage: 0.12, #running-req: 1010, #queue-req: 126, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:24] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:24 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4443, token usage: 0.12, #running-req: 1018, #queue-req: 120, 
[2025-11-02 15:18:25] INFO:     127.0.0.1:43830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:49560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9605, token usage: 0.12, #running-req: 1011, #queue-req: 107, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:45092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7394, token usage: 0.12, #running-req: 1014, #queue-req: 97, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:25] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:45676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11530, token usage: 0.12, #running-req: 1008, #queue-req: 81, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:25] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:45570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7304, token usage: 0.12, #running-req: 1014, #queue-req: 71, 
[2025-11-02 15:18:25] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:25 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9600, token usage: 0.12, #running-req: 1011, #queue-req: 58, 
[2025-11-02 15:18:26] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP0] Prefill batch, #new-seq: 21, #new-token: 21, #cached-token: 15378, token usage: 0.12, #running-req: 1003, #queue-req: 37, 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP3] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP0] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP2] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP6] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP1] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP4] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP5] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP7] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8709, token usage: 0.12, #running-req: 1012, #queue-req: 25, 
[2025-11-02 15:18:26] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8088, token usage: 0.13, #running-req: 1013, #queue-req: 14, 
[2025-11-02 15:18:26] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8000, token usage: 0.13, #running-req: 1013, #queue-req: 3, 
[2025-11-02 15:18:26] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:48114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:26] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:26 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.12, #running-req: 1006, #queue-req: 0, 
[2025-11-02 15:18:27] INFO:     127.0.0.1:44102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46740 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (981, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (964, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:43890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:27] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:27 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:43454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (891, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47170 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:44650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:44400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28] INFO:     127.0.0.1:44152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:28] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:28 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:51386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:43344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29 TP0] Decode batch, #running-req: 778, #token: 106642, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6559.88, #queue-req: 0, 
[2025-11-02 15:18:29] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:29] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:29] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:43802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47736 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (687, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (649, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:30] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:30 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (604, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49104 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (581, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:44970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (555, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:47996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:31] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP4] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP2] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP6] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP0] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP5] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP1] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP3] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:31 TP7] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:32] INFO:     127.0.0.1:45998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:44088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:44470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:44954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:32] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33 TP0] Decode batch, #running-req: 394, #token: 68164, token usage: 0.07, cuda graph: True, gen throughput (token/s): 6574.24, #queue-req: 0, 
[2025-11-02 15:18:33] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:43984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:33] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:43428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:34] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:51350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35 TP0] Decode batch, #running-req: 165, #token: 36317, token usage: 0.04, cuda graph: True, gen throughput (token/s): 4749.38, #queue-req: 0, 
[2025-11-02 15:18:35] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:47534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:35] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:44194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:44912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36 TP0] Decode batch, #running-req: 54, #token: 14660, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2448.84, #queue-req: 0, 
[2025-11-02 15:18:36] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:36] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:44282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:37] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38 TP0] Decode batch, #running-req: 20, #token: 6007, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1086.97, #queue-req: 0, 
[2025-11-02 15:18:38] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:46956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:38] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:39 TP0] Decode batch, #running-req: 3, #token: 1625, token usage: 0.00, cuda graph: True, gen throughput (token/s): 379.62, #queue-req: 0, 
[2025-11-02 15:18:39] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:39] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:39 TP0] Decode batch, #running-req: 1, #token: 1089, token usage: 0.00, cuda graph: True, gen throughput (token/s): 86.06, #queue-req: 0, 
[2025-11-02 15:18:40 TP0] Decode batch, #running-req: 1, #token: 1129, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.60, #queue-req: 0, 
[2025-11-02 15:18:41 TP0] Decode batch, #running-req: 1, #token: 1169, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.60, #queue-req: 0, 
[2025-11-02 15:18:42 TP0] Decode batch, #running-req: 1, #token: 1209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.60, #queue-req: 0, 
[2025-11-02 15:18:42] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:55] INFO:     127.0.0.1:35818 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:18:55] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 33, #new-token: 33, #cached-token: 23976, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP3] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP2] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP1] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP6] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP4] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP5] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP7] [fused_moe] using default for (33, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 44, #new-token: 44, #cached-token: 31935, token usage: 0.01, #running-req: 34, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38629, token usage: 0.01, #running-req: 78, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39566, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-11-02 15:18:55 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44379, token usage: 0.02, #running-req: 185, #queue-req: 0, 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45715, token usage: 0.02, #running-req: 246, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 69, #new-token: 69, #cached-token: 50282, token usage: 0.02, #running-req: 309, #queue-req: 0, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 51152, token usage: 0.03, #running-req: 378, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55063, token usage: 0.03, #running-req: 448, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 77, #new-token: 77, #cached-token: 55936, token usage: 0.04, #running-req: 524, #queue-req: 0, 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 82, #new-token: 82, #cached-token: 59608, token usage: 0.04, #running-req: 601, #queue-req: 0, 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP3] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP2] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP1] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP5] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP4] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP6] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP7] [fused_moe] using default for (82, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:56 TP0] Prefill batch, #new-seq: 87, #new-token: 87, #cached-token: 63297, token usage: 0.05, #running-req: 683, #queue-req: 0, 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP3] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP2] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP4] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP5] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP6] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP1] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP7] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] Prefill batch, #new-seq: 86, #new-token: 86, #cached-token: 62867, token usage: 0.05, #running-req: 770, #queue-req: 0, 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP3] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP2] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP4] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP5] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP6] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP1] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP7] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] Prefill batch, #new-seq: 82, #new-token: 82, #cached-token: 59621, token usage: 0.06, #running-req: 856, #queue-req: 0, 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP2] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP3] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP1] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP6] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP4] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP7] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP5] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:18:57 TP0] Prefill batch, #new-seq: 32, #new-token: 32, #cached-token: 23609, token usage: 0.06, #running-req: 938, #queue-req: 0, 
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:18:57 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39561, token usage: 0.06, #running-req: 970, #queue-req: 21, 
[2025-11-02 15:18:58 TP0] Decode batch, #running-req: 1024, #token: 73975, token usage: 0.08, cuda graph: False, gen throughput (token/s): 673.79, #queue-req: 295, 
[2025-11-02 15:19:00] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-02 15:19:00] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-02 15:19:01] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-02 15:19:01] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:01] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:01] INFO:     127.0.0.1:40392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:36198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2254, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-02 15:19:02] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:38602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7383, token usage: 0.11, #running-req: 1014, #queue-req: 279, 
[2025-11-02 15:19:02] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2168, token usage: 0.11, #running-req: 1021, #queue-req: 276, 
[2025-11-02 15:19:02] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:39006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5060, token usage: 0.11, #running-req: 1017, #queue-req: 269, 
[2025-11-02 15:19:02] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2154, token usage: 0.11, #running-req: 1021, #queue-req: 266, 
[2025-11-02 15:19:02] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:02] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5079, token usage: 0.11, #running-req: 1017, #queue-req: 259, 
[2025-11-02 15:19:03] INFO:     127.0.0.1:41980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2153, token usage: 0.11, #running-req: 1021, #queue-req: 256, 
[2025-11-02 15:19:03] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:40206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4318, token usage: 0.11, #running-req: 1018, #queue-req: 250, 
[2025-11-02 15:19:03] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:38190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8200, token usage: 0.11, #running-req: 1013, #queue-req: 239, 
[2025-11-02 15:19:03] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:42292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6553, token usage: 0.11, #running-req: 1015, #queue-req: 230, 
[2025-11-02 15:19:03] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:40332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:40444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:40806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:42856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:03] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6597, token usage: 0.12, #running-req: 1015, #queue-req: 221, 
[2025-11-02 15:19:04 TP0] Decode batch, #running-req: 1015, #token: 111007, token usage: 0.11, cuda graph: False, gen throughput (token/s): 7830.02, #queue-req: 221, 
[2025-11-02 15:19:04] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7224, token usage: 0.12, #running-req: 1014, #queue-req: 211, 
[2025-11-02 15:19:04] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:37312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:43128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 4986, token usage: 0.12, #running-req: 1017, #queue-req: 204, 
[2025-11-02 15:19:04] INFO:     127.0.0.1:36856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:37590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7306, token usage: 0.12, #running-req: 1014, #queue-req: 194, 
[2025-11-02 15:19:04] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4358, token usage: 0.12, #running-req: 1018, #queue-req: 188, 
[2025-11-02 15:19:04] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:40268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:04] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5107, token usage: 0.12, #running-req: 1017, #queue-req: 181, 
[2025-11-02 15:19:05] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:38090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:44798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7981, token usage: 0.12, #running-req: 1013, #queue-req: 170, 
[2025-11-02 15:19:05] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5702, token usage: 0.12, #running-req: 1016, #queue-req: 162, 
[2025-11-02 15:19:05] INFO:     127.0.0.1:36868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6528, token usage: 0.12, #running-req: 1015, #queue-req: 153, 
[2025-11-02 15:19:05] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:44286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6610, token usage: 0.12, #running-req: 1015, #queue-req: 144, 
[2025-11-02 15:19:05] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:05] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5134, token usage: 0.12, #running-req: 1017, #queue-req: 137, 
[2025-11-02 15:19:06] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6484, token usage: 0.12, #running-req: 1015, #queue-req: 128, 
[2025-11-02 15:19:06] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8914, token usage: 0.12, #running-req: 1012, #queue-req: 116, 
[2025-11-02 15:19:06] INFO:     127.0.0.1:37938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:39302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5925, token usage: 0.12, #running-req: 1016, #queue-req: 108, 
[2025-11-02 15:19:06] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7362, token usage: 0.12, #running-req: 1014, #queue-req: 98, 
[2025-11-02 15:19:06] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:39596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:41002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:06] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 12997, token usage: 0.12, #running-req: 1006, #queue-req: 80, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:07] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5825, token usage: 0.12, #running-req: 1016, #queue-req: 72, 
[2025-11-02 15:19:07] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10354, token usage: 0.12, #running-req: 1010, #queue-req: 58, 
[2025-11-02 15:19:07] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11783, token usage: 0.12, #running-req: 1008, #queue-req: 42, 
[2025-11-02 15:19:07] INFO:     127.0.0.1:37446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:44444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8706, token usage: 0.13, #running-req: 1012, #queue-req: 30, 
[2025-11-02 15:19:07] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:41418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:07] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10957, token usage: 0.13, #running-req: 1009, #queue-req: 15, 
[2025-11-02 15:19:08] INFO:     127.0.0.1:36710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8001, token usage: 0.13, #running-req: 1013, #queue-req: 4, 
[2025-11-02 15:19:08] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2891, token usage: 0.13, #running-req: 1010, #queue-req: 0, 
[2025-11-02 15:19:08] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP3] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP0] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP4] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP2] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP1] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP5] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP7] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP6] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:40376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:08] INFO:     127.0.0.1:37838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:08] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (906, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:36304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP4] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP1] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP5] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP3] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP7] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP0] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP2] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09 TP6] [fused_moe] using default for (895, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:09] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:38744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:09] INFO:     127.0.0.1:44454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44752 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (849, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (840, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] Decode batch, #running-req: 840, #token: 111217, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6195.16, #queue-req: 0, 
[2025-11-02 15:19:10] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP4] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP3] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP1] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP5] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP7] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP0] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP2] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10 TP6] [fused_moe] using default for (777, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:10] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:10] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:45856 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:37092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:37294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:41150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:45992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:37850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:11] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:11 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (642, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (621, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:36878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (606, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:37410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12] INFO:     127.0.0.1:37076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:43116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:45900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:12] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP3] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP1] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP4] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP5] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP7] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP0] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP2] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:12 TP6] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:41542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP4] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP3] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP1] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP5] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP7] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP0] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP2] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP6] [fused_moe] using default for (536, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP3] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP4] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP1] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP5] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP7] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP0] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP2] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13 TP6] [fused_moe] using default for (522, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:13] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:35918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:36436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:36788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:44086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:41794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:13] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14 TP0] Decode batch, #running-req: 429, #token: 71319, token usage: 0.07, cuda graph: True, gen throughput (token/s): 6970.61, #queue-req: 0, 
[2025-11-02 15:19:14] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:35998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:37382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:43204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:14] INFO:     127.0.0.1:47890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:36240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:42500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:15] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:40626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:40192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16 TP0] Decode batch, #running-req: 187, #token: 39112, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5151.07, #queue-req: 0, 
[2025-11-02 15:19:16] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:38014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:16] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:41586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:42146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:44818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:38056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:38398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:39896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:17] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18 TP0] Decode batch, #running-req: 70, #token: 17872, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2801.55, #queue-req: 0, 
[2025-11-02 15:19:18] INFO:     127.0.0.1:36762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:42736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:18] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:47796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19 TP0] Decode batch, #running-req: 23, #token: 7370, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1298.96, #queue-req: 0, 
[2025-11-02 15:19:19] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:46118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:47460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:19] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20 TP0] Decode batch, #running-req: 3, #token: 1692, token usage: 0.00, cuda graph: True, gen throughput (token/s): 509.48, #queue-req: 0, 
[2025-11-02 15:19:20] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:20] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:21 TP0] Decode batch, #running-req: 1, #token: 1083, token usage: 0.00, cuda graph: True, gen throughput (token/s): 107.82, #queue-req: 0, 
[2025-11-02 15:19:21] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:34] INFO:     127.0.0.1:55932 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:19:34] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 41, #new-token: 41, #cached-token: 29897, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34797, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37097, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40313, token usage: 0.01, #running-req: 141, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:34 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42938, token usage: 0.02, #running-req: 196, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 44972, token usage: 0.02, #running-req: 255, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48014, token usage: 0.02, #running-req: 317, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 75, #new-token: 75, #cached-token: 54820, token usage: 0.03, #running-req: 383, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48006, token usage: 0.03, #running-req: 458, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 88, #new-token: 88, #cached-token: 63824, token usage: 0.04, #running-req: 524, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48689, token usage: 0.04, #running-req: 612, #queue-req: 0, 
[2025-11-02 15:19:35 TP0] Prefill batch, #new-seq: 101, #new-token: 101, #cached-token: 73553, token usage: 0.05, #running-req: 679, #queue-req: 0, 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP0] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP3] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP2] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP4] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP7] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP6] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP1] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:35 TP5] [fused_moe] using default for (101, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32973, token usage: 0.05, #running-req: 780, #queue-req: 0, 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6610, token usage: 0.05, #running-req: 825, #queue-req: 0, 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35505, token usage: 0.05, #running-req: 834, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35608, token usage: 0.06, #running-req: 883, #queue-req: 0, 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44778, token usage: 0.06, #running-req: 932, #queue-req: 0, 
[2025-11-02 15:19:36 TP0] Prefill batch, #new-seq: 31, #new-token: 31, #cached-token: 22802, token usage: 0.06, #running-req: 993, #queue-req: 27, 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP3] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP2] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP4] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP6] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP7] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP5] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP1] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:36 TP0] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:38 TP0] Decode batch, #running-req: 1024, #token: 81229, token usage: 0.08, cuda graph: False, gen throughput (token/s): 1043.42, #queue-req: 295, 
[2025-11-02 15:19:39] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-02 15:19:40] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-02 15:19:41] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:33890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3743, token usage: 0.11, #running-req: 1019, #queue-req: 288, 
[2025-11-02 15:19:41] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5876, token usage: 0.11, #running-req: 1016, #queue-req: 280, 
[2025-11-02 15:19:41] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2924, token usage: 0.11, #running-req: 1020, #queue-req: 276, 
[2025-11-02 15:19:41] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5035, token usage: 0.11, #running-req: 1017, #queue-req: 269, 
[2025-11-02 15:19:41] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:36842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:41 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3632, token usage: 0.11, #running-req: 1019, #queue-req: 264, 
[2025-11-02 15:19:42] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4302, token usage: 0.11, #running-req: 1018, #queue-req: 258, 
[2025-11-02 15:19:42] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2182, token usage: 0.11, #running-req: 1021, #queue-req: 255, 
[2025-11-02 15:19:42] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7294, token usage: 0.11, #running-req: 1014, #queue-req: 245, 
[2025-11-02 15:19:42] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5912, token usage: 0.11, #running-req: 1016, #queue-req: 237, 
[2025-11-02 15:19:42] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42] INFO:     127.0.0.1:35826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:42 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5860, token usage: 0.11, #running-req: 1016, #queue-req: 229, 
[2025-11-02 15:19:43] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4404, token usage: 0.12, #running-req: 1018, #queue-req: 223, 
[2025-11-02 15:19:43] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:34442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5773, token usage: 0.12, #running-req: 1016, #queue-req: 215, 
[2025-11-02 15:19:43] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8637, token usage: 0.12, #running-req: 1012, #queue-req: 203, 
[2025-11-02 15:19:43] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5851, token usage: 0.12, #running-req: 1016, #queue-req: 195, 
[2025-11-02 15:19:43] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:56976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:43 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4344, token usage: 0.12, #running-req: 1018, #queue-req: 189, 
[2025-11-02 15:19:44] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6571, token usage: 0.12, #running-req: 1015, #queue-req: 180, 
[2025-11-02 15:19:44] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7953, token usage: 0.12, #running-req: 1013, #queue-req: 169, 
[2025-11-02 15:19:44] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5728, token usage: 0.12, #running-req: 1016, #queue-req: 161, 
[2025-11-02 15:19:44 TP0] Decode batch, #running-req: 1016, #token: 115557, token usage: 0.12, cuda graph: False, gen throughput (token/s): 6945.42, #queue-req: 161, 
[2025-11-02 15:19:44] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5061, token usage: 0.12, #running-req: 1017, #queue-req: 154, 
[2025-11-02 15:19:44] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:44 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6602, token usage: 0.12, #running-req: 1015, #queue-req: 145, 
[2025-11-02 15:19:45] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:60664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8783, token usage: 0.12, #running-req: 1012, #queue-req: 133, 
[2025-11-02 15:19:45] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8773, token usage: 0.12, #running-req: 1012, #queue-req: 121, 
[2025-11-02 15:19:45] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:34564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8093, token usage: 0.12, #running-req: 1013, #queue-req: 110, 
[2025-11-02 15:19:45] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5984, token usage: 0.12, #running-req: 1016, #queue-req: 102, 
[2025-11-02 15:19:45] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:45 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7271, token usage: 0.12, #running-req: 1014, #queue-req: 92, 
[2025-11-02 15:19:46] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:32800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10139, token usage: 0.12, #running-req: 1010, #queue-req: 78, 
[2025-11-02 15:19:46] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3630, token usage: 0.12, #running-req: 1019, #queue-req: 73, 
[2025-11-02 15:19:46] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11069, token usage: 0.12, #running-req: 1009, #queue-req: 58, 
[2025-11-02 15:19:46] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46 TP0] Prefill batch, #new-seq: 19, #new-token: 19, #cached-token: 13955, token usage: 0.12, #running-req: 1005, #queue-req: 39, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:46] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:46 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5808, token usage: 0.13, #running-req: 1016, #queue-req: 31, 
[2025-11-02 15:19:47] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11683, token usage: 0.13, #running-req: 1008, #queue-req: 15, 
[2025-11-02 15:19:47] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7274, token usage: 0.13, #running-req: 1014, #queue-req: 5, 
[2025-11-02 15:19:47] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3618, token usage: 0.13, #running-req: 1015, #queue-req: 0, 
[2025-11-02 15:19:47] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP2] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP7] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP3] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP6] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP0] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP5] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP1] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP4] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:47] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP4] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP6] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP2] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP0] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP1] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP7] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP3] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:47 TP5] [fused_moe] using default for (991, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (944, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (903, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (894, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:33356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:48] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP2] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP6] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP4] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP3] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP0] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP7] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP5] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:48 TP1] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP2] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP4] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP6] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP0] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP3] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP5] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP7] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP1] [fused_moe] using default for (865, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP4] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP2] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP6] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP3] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP0] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP5] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP1] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP7] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:37388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP6] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP2] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP4] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP3] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP0] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP7] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP5] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49 TP1] [fused_moe] using default for (806, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:49] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:38100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:49] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] Decode batch, #running-req: 768, #token: 107132, token usage: 0.11, cuda graph: False, gen throughput (token/s): 6699.59, #queue-req: 0, 
[2025-11-02 15:19:50] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (749, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (701, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:50] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP2] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP4] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP6] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP3] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP0] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP7] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP5] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:50 TP1] [fused_moe] using default for (670, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:39492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (617, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:51] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP6] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP4] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP2] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP0] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP3] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP5] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP7] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:51 TP1] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP2] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP6] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP4] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP3] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP0] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP5] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP1] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP7] [fused_moe] using default for (535, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP2] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP6] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP4] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP3] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP0] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP7] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP1] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52 TP5] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:19:52] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:40050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:36854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:59132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:52] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:40054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53 TP0] Decode batch, #running-req: 382, #token: 65988, token usage: 0.07, cuda graph: True, gen throughput (token/s): 6636.80, #queue-req: 0, 
[2025-11-02 15:19:53] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:34854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:53] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:35218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:36740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:35466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:37972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:54] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:33234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:35650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55 TP0] Decode batch, #running-req: 153, #token: 33338, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4615.59, #queue-req: 0, 
[2025-11-02 15:19:55] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:55] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:33032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:56] INFO:     127.0.0.1:39400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:35364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57 TP0] Decode batch, #running-req: 57, #token: 15447, token usage: 0.02, cuda graph: True, gen throughput (token/s): 2439.66, #queue-req: 0, 
[2025-11-02 15:19:57] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:60846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:57] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:37126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:37634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58 TP0] Decode batch, #running-req: 19, #token: 6402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1150.10, #queue-req: 0, 
[2025-11-02 15:19:58] INFO:     127.0.0.1:37428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:35494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:58] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:38764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59 TP0] Decode batch, #running-req: 3, #token: 1718, token usage: 0.00, cuda graph: True, gen throughput (token/s): 403.94, #queue-req: 0, 
[2025-11-02 15:19:59] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:19:59] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:00 TP0] Decode batch, #running-req: 1, #token: 1108, token usage: 0.00, cuda graph: True, gen throughput (token/s): 83.37, #queue-req: 0, 
[2025-11-02 15:20:00] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:13] INFO:     127.0.0.1:49636 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:20:13] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 41, #new-token: 41, #cached-token: 29799, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34173, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37147, token usage: 0.01, #running-req: 89, #queue-req: 0, 
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40277, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-11-02 15:20:13 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42122, token usage: 0.02, #running-req: 195, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45763, token usage: 0.02, #running-req: 253, #queue-req: 0, 
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 64, #new-token: 64, #cached-token: 46618, token usage: 0.02, #running-req: 316, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 51185, token usage: 0.03, #running-req: 380, #queue-req: 0, 
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51407, token usage: 0.03, #running-req: 450, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 78, #new-token: 78, #cached-token: 56711, token usage: 0.04, #running-req: 521, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:14 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 37033, token usage: 0.04, #running-req: 599, #queue-req: 0, 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27630, token usage: 0.04, #running-req: 650, #queue-req: 0, 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38616, token usage: 0.05, #running-req: 688, #queue-req: 0, 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37927, token usage: 0.05, #running-req: 741, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45275, token usage: 0.05, #running-req: 793, #queue-req: 0, 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43402, token usage: 0.06, #running-req: 855, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 51360, token usage: 0.06, #running-req: 915, #queue-req: 0, 
[2025-11-02 15:20:15 TP0] Prefill batch, #new-seq: 39, #new-token: 39, #cached-token: 28648, token usage: 0.06, #running-req: 985, #queue-req: 31, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:15 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:18] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 732, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-02 15:20:19 TP0] Decode batch, #running-req: 1024, #token: 94966, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1717.24, #queue-req: 294, 
[2025-11-02 15:20:19] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 709, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-02 15:20:19] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-02 15:20:20] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:49684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2206, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-02 15:20:20] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7459, token usage: 0.11, #running-req: 1014, #queue-req: 279, 
[2025-11-02 15:20:20] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:20 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1458, token usage: 0.11, #running-req: 1022, #queue-req: 277, 
[2025-11-02 15:20:20] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5839, token usage: 0.11, #running-req: 1016, #queue-req: 269, 
[2025-11-02 15:20:20] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:20 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2914, token usage: 0.11, #running-req: 1020, #queue-req: 265, 
[2025-11-02 15:20:21] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5023, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[2025-11-02 15:20:21] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 712, token usage: 0.11, #running-req: 1023, #queue-req: 257, 
[2025-11-02 15:20:21] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3598, token usage: 0.11, #running-req: 1019, #queue-req: 252, 
[2025-11-02 15:20:21] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6595, token usage: 0.11, #running-req: 1015, #queue-req: 243, 
[2025-11-02 15:20:21] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:21 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5937, token usage: 0.11, #running-req: 1016, #queue-req: 235, 
[2025-11-02 15:20:22] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8095, token usage: 0.11, #running-req: 1013, #queue-req: 224, 
[2025-11-02 15:20:22] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3602, token usage: 0.12, #running-req: 1019, #queue-req: 219, 
[2025-11-02 15:20:22] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5774, token usage: 0.12, #running-req: 1016, #queue-req: 211, 
[2025-11-02 15:20:22] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:53670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8687, token usage: 0.12, #running-req: 1012, #queue-req: 199, 
[2025-11-02 15:20:22] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:54112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:22 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5078, token usage: 0.12, #running-req: 1017, #queue-req: 192, 
[2025-11-02 15:20:23] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4369, token usage: 0.12, #running-req: 1018, #queue-req: 186, 
[2025-11-02 15:20:23] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8702, token usage: 0.12, #running-req: 1012, #queue-req: 174, 
[2025-11-02 15:20:23] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7898, token usage: 0.12, #running-req: 1013, #queue-req: 163, 
[2025-11-02 15:20:23] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4391, token usage: 0.12, #running-req: 1018, #queue-req: 157, 
[2025-11-02 15:20:23] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:23 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5069, token usage: 0.12, #running-req: 1017, #queue-req: 150, 
[2025-11-02 15:20:24] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3665, token usage: 0.12, #running-req: 1019, #queue-req: 145, 
[2025-11-02 15:20:24] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8079, token usage: 0.12, #running-req: 1013, #queue-req: 134, 
[2025-11-02 15:20:24] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5051, token usage: 0.12, #running-req: 1017, #queue-req: 127, 
[2025-11-02 15:20:24] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6709, token usage: 0.12, #running-req: 1015, #queue-req: 118, 
[2025-11-02 15:20:24] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:24 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8075, token usage: 0.12, #running-req: 1013, #queue-req: 107, 
[2025-11-02 15:20:25] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25 TP0] Prefill batch, #new-seq: 20, #new-token: 20, #cached-token: 14610, token usage: 0.12, #running-req: 1004, #queue-req: 87, 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP0] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP3] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP2] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP1] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP6] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP4] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP5] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25 TP7] [fused_moe] using default for (20, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:25] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6519, token usage: 0.12, #running-req: 1015, #queue-req: 78, 
[2025-11-02 15:20:25] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7372, token usage: 0.12, #running-req: 1014, #queue-req: 68, 
[2025-11-02 15:20:25] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9584, token usage: 0.13, #running-req: 1011, #queue-req: 55, 
[2025-11-02 15:20:25] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:25 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11698, token usage: 0.13, #running-req: 1008, #queue-req: 39, 
[2025-11-02 15:20:26] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10132, token usage: 0.13, #running-req: 1010, #queue-req: 25, 
[2025-11-02 15:20:26] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5892, token usage: 0.13, #running-req: 1016, #queue-req: 17, 
[2025-11-02 15:20:26 TP0] Decode batch, #running-req: 1016, #token: 120488, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5605.06, #queue-req: 17, 
[2025-11-02 15:20:26] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10937, token usage: 0.13, #running-req: 1009, #queue-req: 2, 
[2025-11-02 15:20:26] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:57576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1422, token usage: 0.13, #running-req: 1009, #queue-req: 0, 
[2025-11-02 15:20:26] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:51196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:26] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:50042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:27] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:27 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP3] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP1] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP7] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP4] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP5] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP0] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP2] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP6] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP1] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP3] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP7] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP4] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP5] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP0] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP2] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP6] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:28] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:28] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (785, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (775, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:51564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (699, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:29] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP1] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP7] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP4] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP3] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP5] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP0] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP6] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:29 TP2] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP1] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP3] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP4] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP7] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP5] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP0] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP6] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP2] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP1] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP3] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP7] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP4] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP5] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP0] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP2] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP6] [fused_moe] using default for (633, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30 TP0] Decode batch, #running-req: 623, #token: 94535, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7667.29, #queue-req: 0, 
[2025-11-02 15:20:30] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP1] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP4] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP7] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP5] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP3] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP0] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP2] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP6] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:30] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:30] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP1] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP4] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP7] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP3] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP5] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP0] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP2] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP6] [fused_moe] using default for (568, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP1] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP3] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP7] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP4] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP5] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP0] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP2] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31 TP6] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-02 15:20:31] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:51050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:31] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:32] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:32952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33 TP0] Decode batch, #running-req: 286, #token: 54258, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6181.27, #queue-req: 0, 
[2025-11-02 15:20:33] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:32874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:33] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:52206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:34] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35 TP0] Decode batch, #running-req: 111, #token: 26209, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3878.65, #queue-req: 0, 
[2025-11-02 15:20:35] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:33016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:35] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:59618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:50448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:52582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36 TP0] Decode batch, #running-req: 41, #token: 10984, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1842.61, #queue-req: 0, 
[2025-11-02 15:20:36] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:36] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:37] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38 TP0] Decode batch, #running-req: 11, #token: 3706, token usage: 0.00, cuda graph: True, gen throughput (token/s): 888.67, #queue-req: 0, 
[2025-11-02 15:20:38] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:38 TP0] Decode batch, #running-req: 2, #token: 1355, token usage: 0.00, cuda graph: True, gen throughput (token/s): 250.13, #queue-req: 0, 
[2025-11-02 15:20:39] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:39 TP0] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 59.73, #queue-req: 0, 
[2025-11-02 15:20:40 TP0] Decode batch, #running-req: 1, #token: 1144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.61, #queue-req: 0, 
[2025-11-02 15:20:41 TP0] Decode batch, #running-req: 1, #token: 1184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.60, #queue-req: 0, 
[2025-11-02 15:20:42 TP0] Decode batch, #running-req: 1, #token: 1224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.60, #queue-req: 0, 
[2025-11-02 15:20:42] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:20:42 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.59, #queue-req: 0, 
[2025-11-02 15:20:54] INFO:     127.0.0.1:38658 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-02 15:21:00] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:00 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:21:02] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:21:02] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-11-02 15:21:02] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 20, 
[2025-11-02 15:21:02] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:02] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:21:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-02 15:21:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-02 15:21:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-02 15:21:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-02 15:21:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-02 15:21:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-02 15:21:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-02 15:21:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-02 15:21:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-02 15:21:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-02 15:21:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-11-02 15:21:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-11-02 15:21:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-11-02 15:21:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-11-02 15:21:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 81, #queue-req: 42, 
[2025-11-02 15:21:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-11-02 15:21:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-11-02 15:21:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 96, #queue-req: 27, 
[2025-11-02 15:21:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-11-02 15:21:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-11-02 15:21:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 111, #queue-req: 12, 
[2025-11-02 15:21:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-11-02 15:21:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.40, #running-req: 121, #queue-req: 2, 
[2025-11-02 15:21:22 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-11-02 15:21:23 TP0] Decode batch, #running-req: 128, #token: 410527, token usage: 0.42, cuda graph: True, gen throughput (token/s): 25.91, #queue-req: 0, 
[2025-11-02 15:21:25 TP0] Decode batch, #running-req: 128, #token: 415647, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2543.51, #queue-req: 0, 
[2025-11-02 15:21:27 TP0] Decode batch, #running-req: 128, #token: 420767, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2514.24, #queue-req: 0, 
[2025-11-02 15:21:29 TP0] Decode batch, #running-req: 128, #token: 425887, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2499.70, #queue-req: 0, 
[2025-11-02 15:21:31 TP0] Decode batch, #running-req: 128, #token: 431007, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2488.79, #queue-req: 0, 
[2025-11-02 15:21:33 TP0] Decode batch, #running-req: 128, #token: 436127, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2480.55, #queue-req: 0, 
[2025-11-02 15:21:35 TP0] Decode batch, #running-req: 128, #token: 441247, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2473.54, #queue-req: 0, 
[2025-11-02 15:21:38 TP0] Decode batch, #running-req: 128, #token: 446367, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2466.92, #queue-req: 0, 
[2025-11-02 15:21:40 TP0] Decode batch, #running-req: 128, #token: 451487, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2460.27, #queue-req: 0, 
[2025-11-02 15:21:42 TP0] Decode batch, #running-req: 128, #token: 456607, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2454.93, #queue-req: 0, 
[2025-11-02 15:21:44 TP0] Decode batch, #running-req: 128, #token: 461727, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2448.09, #queue-req: 0, 
[2025-11-02 15:21:46 TP0] Decode batch, #running-req: 128, #token: 466847, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2446.72, #queue-req: 0, 
[2025-11-02 15:21:48 TP0] Decode batch, #running-req: 128, #token: 471967, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2437.31, #queue-req: 0, 
[2025-11-02 15:21:50 TP0] Decode batch, #running-req: 128, #token: 477087, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2437.67, #queue-req: 0, 
[2025-11-02 15:21:52 TP0] Decode batch, #running-req: 128, #token: 482207, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2429.37, #queue-req: 0, 
[2025-11-02 15:21:54 TP0] Decode batch, #running-req: 128, #token: 487327, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2421.51, #queue-req: 0, 
[2025-11-02 15:21:56 TP0] Decode batch, #running-req: 128, #token: 492447, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2418.43, #queue-req: 0, 
[2025-11-02 15:21:59 TP0] Decode batch, #running-req: 128, #token: 497567, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2415.88, #queue-req: 0, 
[2025-11-02 15:22:01 TP0] Decode batch, #running-req: 128, #token: 502687, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2414.33, #queue-req: 0, 
[2025-11-02 15:22:03 TP0] Decode batch, #running-req: 128, #token: 507807, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2410.08, #queue-req: 0, 
[2025-11-02 15:22:05] INFO:     127.0.0.1:58720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:22:05] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-11-02 15:22:05] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-11-02 15:22:05] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:05] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:22:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-02 15:22:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-02 15:22:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-02 15:22:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-02 15:22:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-02 15:22:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-02 15:22:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-02 15:22:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-02 15:22:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-02 15:22:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-02 15:22:14 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-11-02 15:22:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-11-02 15:22:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-11-02 15:22:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-11-02 15:22:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 82, #queue-req: 41, 
[2025-11-02 15:22:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-11-02 15:22:19 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-11-02 15:22:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-11-02 15:22:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-11-02 15:22:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-11-02 15:22:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-11-02 15:22:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-11-02 15:22:24 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-11-02 15:22:26 TP0] Decode batch, #running-req: 128, #token: 404125, token usage: 0.42, cuda graph: True, gen throughput (token/s): 225.55, #queue-req: 0, 
[2025-11-02 15:22:28 TP0] Decode batch, #running-req: 128, #token: 409245, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2532.17, #queue-req: 0, 
[2025-11-02 15:22:30 TP0] Decode batch, #running-req: 128, #token: 414365, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2500.83, #queue-req: 0, 
[2025-11-02 15:22:32 TP0] Decode batch, #running-req: 128, #token: 419485, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2488.92, #queue-req: 0, 
[2025-11-02 15:22:34 TP0] Decode batch, #running-req: 128, #token: 424605, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2483.30, #queue-req: 0, 
[2025-11-02 15:22:36 TP0] Decode batch, #running-req: 128, #token: 429725, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2472.68, #queue-req: 0, 
[2025-11-02 15:22:38 TP0] Decode batch, #running-req: 128, #token: 434845, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2467.43, #queue-req: 0, 
[2025-11-02 15:22:40 TP0] Decode batch, #running-req: 128, #token: 439965, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2463.02, #queue-req: 0, 
[2025-11-02 15:22:42 TP0] Decode batch, #running-req: 128, #token: 445085, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2456.48, #queue-req: 0, 
[2025-11-02 15:22:44 TP0] Decode batch, #running-req: 128, #token: 450205, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2452.41, #queue-req: 0, 
[2025-11-02 15:22:46 TP0] Decode batch, #running-req: 128, #token: 455325, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2447.97, #queue-req: 0, 
[2025-11-02 15:22:48 TP0] Decode batch, #running-req: 128, #token: 460445, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2442.64, #queue-req: 0, 
[2025-11-02 15:22:50 TP0] Decode batch, #running-req: 128, #token: 465565, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2438.24, #queue-req: 0, 
[2025-11-02 15:22:52 TP0] Decode batch, #running-req: 128, #token: 470685, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2434.90, #queue-req: 0, 
[2025-11-02 15:22:55 TP0] Decode batch, #running-req: 128, #token: 475805, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2430.49, #queue-req: 0, 
[2025-11-02 15:22:57 TP0] Decode batch, #running-req: 128, #token: 480925, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2425.89, #queue-req: 0, 
[2025-11-02 15:22:59 TP0] Decode batch, #running-req: 128, #token: 486045, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2421.10, #queue-req: 0, 
[2025-11-02 15:23:01 TP0] Decode batch, #running-req: 128, #token: 491165, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2417.26, #queue-req: 0, 
[2025-11-02 15:23:03 TP0] Decode batch, #running-req: 128, #token: 496285, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2410.86, #queue-req: 0, 
[2025-11-02 15:23:05 TP0] Decode batch, #running-req: 128, #token: 501405, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2410.31, #queue-req: 0, 
[2025-11-02 15:23:07] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:23:07] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-11-02 15:23:07] INFO:     127.0.0.1:50976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-11-02 15:23:07] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:07] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:23:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-02 15:23:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-02 15:23:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-02 15:23:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-02 15:23:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-02 15:23:12 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 86, 
[2025-11-02 15:23:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 81, 
[2025-11-02 15:23:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 76, 
[2025-11-02 15:23:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 71, 
[2025-11-02 15:23:15 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 65, 
[2025-11-02 15:23:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.21, #running-req: 63, #queue-req: 60, 
[2025-11-02 15:23:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.22, #running-req: 68, #queue-req: 55, 
[2025-11-02 15:23:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 73, #queue-req: 50, 
[2025-11-02 15:23:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 78, #queue-req: 45, 
[2025-11-02 15:23:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.27, #running-req: 83, #queue-req: 40, 
[2025-11-02 15:23:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 35, 
[2025-11-02 15:23:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.31, #running-req: 93, #queue-req: 30, 
[2025-11-02 15:23:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-11-02 15:23:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-11-02 15:23:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 108, #queue-req: 15, 
[2025-11-02 15:23:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-11-02 15:23:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-11-02 15:23:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-11-02 15:23:28 TP0] Decode batch, #running-req: 128, #token: 410524, token usage: 0.42, cuda graph: True, gen throughput (token/s): 225.24, #queue-req: 0, 
[2025-11-02 15:23:30 TP0] Decode batch, #running-req: 128, #token: 415644, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2531.33, #queue-req: 0, 
[2025-11-02 15:23:32 TP0] Decode batch, #running-req: 128, #token: 420764, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2499.63, #queue-req: 0, 
[2025-11-02 15:23:34 TP0] Decode batch, #running-req: 128, #token: 425884, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2485.22, #queue-req: 0, 
[2025-11-02 15:23:36 TP0] Decode batch, #running-req: 128, #token: 431004, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2479.31, #queue-req: 0, 
[2025-11-02 15:23:38 TP0] Decode batch, #running-req: 128, #token: 436124, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2472.50, #queue-req: 0, 
[2025-11-02 15:23:40 TP0] Decode batch, #running-req: 128, #token: 441244, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2465.16, #queue-req: 0, 
[2025-11-02 15:23:42 TP0] Decode batch, #running-req: 128, #token: 446364, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2459.28, #queue-req: 0, 
[2025-11-02 15:23:44 TP0] Decode batch, #running-req: 128, #token: 451484, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2455.34, #queue-req: 0, 
[2025-11-02 15:23:47 TP0] Decode batch, #running-req: 128, #token: 456604, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2451.42, #queue-req: 0, 
[2025-11-02 15:23:49 TP0] Decode batch, #running-req: 128, #token: 461724, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2448.81, #queue-req: 0, 
[2025-11-02 15:23:51 TP0] Decode batch, #running-req: 128, #token: 466844, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2445.24, #queue-req: 0, 
[2025-11-02 15:23:53 TP0] Decode batch, #running-req: 128, #token: 471964, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2444.96, #queue-req: 0, 
[2025-11-02 15:23:55 TP0] Decode batch, #running-req: 128, #token: 477084, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2438.40, #queue-req: 0, 
[2025-11-02 15:23:57 TP0] Decode batch, #running-req: 128, #token: 482204, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2431.24, #queue-req: 0, 
[2025-11-02 15:23:59 TP0] Decode batch, #running-req: 128, #token: 487324, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2422.77, #queue-req: 0, 
[2025-11-02 15:24:01 TP0] Decode batch, #running-req: 128, #token: 492444, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2420.42, #queue-req: 0, 
[2025-11-02 15:24:03 TP0] Decode batch, #running-req: 128, #token: 497564, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2413.73, #queue-req: 0, 
[2025-11-02 15:24:05 TP0] Decode batch, #running-req: 128, #token: 502684, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2412.08, #queue-req: 0, 
[2025-11-02 15:24:08 TP0] Decode batch, #running-req: 128, #token: 507804, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2406.59, #queue-req: 0, 
[2025-11-02 15:24:09] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:09 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:24:10] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10 TP0] Prefill batch, #new-seq: 2, #new-token: 6396, #cached-token: 6, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:24:10] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.01, #running-req: 3, #queue-req: 14, 
[2025-11-02 15:24:10] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.03, #running-req: 8, #queue-req: 71, 
[2025-11-02 15:24:10] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:10] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:24:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 13, #queue-req: 98, 
[2025-11-02 15:24:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.06, #running-req: 18, #queue-req: 93, 
[2025-11-02 15:24:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.08, #running-req: 23, #queue-req: 88, 
[2025-11-02 15:24:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 28, #queue-req: 83, 
[2025-11-02 15:24:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 33, #queue-req: 78, 
[2025-11-02 15:24:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.13, #running-req: 38, #queue-req: 73, 
[2025-11-02 15:24:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 43, #queue-req: 68, 
[2025-11-02 15:24:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.16, #running-req: 48, #queue-req: 63, 
[2025-11-02 15:24:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 53, #queue-req: 58, 
[2025-11-02 15:24:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.19, #running-req: 58, #queue-req: 53, 
[2025-11-02 15:24:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 63, #queue-req: 48, 
[2025-11-02 15:24:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.22, #running-req: 68, #queue-req: 43, 
[2025-11-02 15:24:21 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.24, #running-req: 73, #queue-req: 37, 
[2025-11-02 15:24:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.26, #running-req: 79, #queue-req: 32, 
[2025-11-02 15:24:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.28, #running-req: 84, #queue-req: 27, 
[2025-11-02 15:24:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.29, #running-req: 89, #queue-req: 22, 
[2025-11-02 15:24:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 17, 
[2025-11-02 15:24:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.33, #running-req: 99, #queue-req: 12, 
[2025-11-02 15:24:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.34, #running-req: 104, #queue-req: 7, 
[2025-11-02 15:24:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.36, #running-req: 109, #queue-req: 2, 
[2025-11-02 15:24:27 TP0] Prefill batch, #new-seq: 2, #new-token: 6395, #cached-token: 7, token usage: 0.38, #running-req: 114, #queue-req: 0, 
[2025-11-02 15:24:29 TP0] Decode batch, #running-req: 116, #token: 371812, token usage: 0.38, cuda graph: True, gen throughput (token/s): 228.20, #queue-req: 0, 
[2025-11-02 15:24:31 TP0] Decode batch, #running-req: 116, #token: 376452, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2342.94, #queue-req: 0, 
[2025-11-02 15:24:33 TP0] Decode batch, #running-req: 116, #token: 381092, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2322.32, #queue-req: 0, 
[2025-11-02 15:24:35 TP0] Decode batch, #running-req: 116, #token: 385732, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2309.32, #queue-req: 0, 
[2025-11-02 15:24:37 TP0] Decode batch, #running-req: 116, #token: 390372, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2301.09, #queue-req: 0, 
[2025-11-02 15:24:39 TP0] Decode batch, #running-req: 116, #token: 395012, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2294.36, #queue-req: 0, 
[2025-11-02 15:24:41 TP0] Decode batch, #running-req: 116, #token: 399652, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2289.74, #queue-req: 0, 
[2025-11-02 15:24:43 TP0] Decode batch, #running-req: 116, #token: 404292, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2285.18, #queue-req: 0, 
[2025-11-02 15:24:45 TP0] Decode batch, #running-req: 116, #token: 408932, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2280.95, #queue-req: 0, 
[2025-11-02 15:24:47 TP0] Decode batch, #running-req: 116, #token: 413572, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2278.26, #queue-req: 0, 
[2025-11-02 15:24:49 TP0] Decode batch, #running-req: 116, #token: 418212, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2273.95, #queue-req: 0, 
[2025-11-02 15:24:51 TP0] Decode batch, #running-req: 116, #token: 422852, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2269.75, #queue-req: 0, 
[2025-11-02 15:24:53 TP0] Decode batch, #running-req: 116, #token: 427492, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2268.16, #queue-req: 0, 
[2025-11-02 15:24:55 TP0] Decode batch, #running-req: 116, #token: 432132, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2265.26, #queue-req: 0, 
[2025-11-02 15:24:57 TP0] Decode batch, #running-req: 116, #token: 436772, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2259.67, #queue-req: 0, 
[2025-11-02 15:24:59 TP0] Decode batch, #running-req: 116, #token: 441412, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2253.38, #queue-req: 0, 
[2025-11-02 15:25:01 TP0] Decode batch, #running-req: 116, #token: 446052, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2247.90, #queue-req: 0, 
[2025-11-02 15:25:03 TP0] Decode batch, #running-req: 116, #token: 450692, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2242.75, #queue-req: 0, 
[2025-11-02 15:25:05 TP0] Decode batch, #running-req: 116, #token: 455332, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2238.61, #queue-req: 0, 
[2025-11-02 15:25:07 TP0] Decode batch, #running-req: 116, #token: 459972, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2237.12, #queue-req: 0, 
[2025-11-02 15:25:09] INFO:     127.0.0.1:43668 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-02 15:25:26] INFO:     127.0.0.1:33352 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-02 15:25:33] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:33 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:25:33 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 152.57, #queue-req: 0, 
[2025-11-02 15:25:35] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:25:35] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-11-02 15:25:35] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 22, 
[2025-11-02 15:25:35] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:35] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:25:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:25:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-02 15:25:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-02 15:25:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-02 15:25:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-02 15:25:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-02 15:25:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-02 15:25:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-02 15:25:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-02 15:25:43 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.19, #running-req: 56, #queue-req: 2, 
[2025-11-02 15:25:44 TP0] Prefill batch, #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-02 15:25:46 TP0] Decode batch, #running-req: 64, #token: 205672, token usage: 0.21, cuda graph: True, gen throughput (token/s): 74.80, #queue-req: 0, 
[2025-11-02 15:25:47 TP0] Decode batch, #running-req: 64, #token: 208232, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1530.42, #queue-req: 0, 
[2025-11-02 15:25:49 TP0] Decode batch, #running-req: 64, #token: 210792, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1512.51, #queue-req: 0, 
[2025-11-02 15:25:51 TP0] Decode batch, #running-req: 64, #token: 213352, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1504.66, #queue-req: 0, 
[2025-11-02 15:25:52 TP0] Decode batch, #running-req: 64, #token: 215912, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.12, #queue-req: 0, 
[2025-11-02 15:25:54 TP0] Decode batch, #running-req: 64, #token: 218472, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1496.46, #queue-req: 0, 
[2025-11-02 15:25:56 TP0] Decode batch, #running-req: 64, #token: 221032, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1489.42, #queue-req: 0, 
[2025-11-02 15:25:57 TP0] Decode batch, #running-req: 64, #token: 223592, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.97, #queue-req: 0, 
[2025-11-02 15:25:59 TP0] Decode batch, #running-req: 64, #token: 226152, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.28, #queue-req: 0, 
[2025-11-02 15:26:01 TP0] Decode batch, #running-req: 64, #token: 228712, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1478.51, #queue-req: 0, 
[2025-11-02 15:26:03 TP0] Decode batch, #running-req: 64, #token: 231272, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1479.02, #queue-req: 0, 
[2025-11-02 15:26:04 TP0] Decode batch, #running-req: 64, #token: 233832, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.21, #queue-req: 0, 
[2025-11-02 15:26:06 TP0] Decode batch, #running-req: 64, #token: 236392, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.83, #queue-req: 0, 
[2025-11-02 15:26:08 TP0] Decode batch, #running-req: 64, #token: 238952, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.33, #queue-req: 0, 
[2025-11-02 15:26:10 TP0] Decode batch, #running-req: 64, #token: 241512, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1480.25, #queue-req: 0, 
[2025-11-02 15:26:11 TP0] Decode batch, #running-req: 64, #token: 244072, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1474.89, #queue-req: 0, 
[2025-11-02 15:26:13 TP0] Decode batch, #running-req: 64, #token: 246632, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1473.38, #queue-req: 0, 
[2025-11-02 15:26:15 TP0] Decode batch, #running-req: 64, #token: 249192, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.83, #queue-req: 0, 
[2025-11-02 15:26:17 TP0] Decode batch, #running-req: 64, #token: 251752, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1469.96, #queue-req: 0, 
[2025-11-02 15:26:18 TP0] Decode batch, #running-req: 64, #token: 254312, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1467.70, #queue-req: 0, 
[2025-11-02 15:26:19] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:26:19] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:19] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:26:20] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 42, 
[2025-11-02 15:26:20] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:20] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:26:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:26:21 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.06, #running-req: 16, #queue-req: 42, 
[2025-11-02 15:26:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-02 15:26:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 32, 
[2025-11-02 15:26:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.11, #running-req: 32, #queue-req: 27, 
[2025-11-02 15:26:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.12, #running-req: 37, #queue-req: 22, 
[2025-11-02 15:26:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-02 15:26:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-02 15:26:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-02 15:26:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.19, #running-req: 57, #queue-req: 2, 
[2025-11-02 15:26:29 TP0] Prefill batch, #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-02 15:26:30 TP0] Decode batch, #running-req: 64, #token: 205655, token usage: 0.21, cuda graph: True, gen throughput (token/s): 211.31, #queue-req: 0, 
[2025-11-02 15:26:32 TP0] Decode batch, #running-req: 64, #token: 208215, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1531.86, #queue-req: 0, 
[2025-11-02 15:26:34 TP0] Decode batch, #running-req: 64, #token: 210775, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1515.22, #queue-req: 0, 
[2025-11-02 15:26:35 TP0] Decode batch, #running-req: 64, #token: 213335, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.15, #queue-req: 0, 
[2025-11-02 15:26:37 TP0] Decode batch, #running-req: 64, #token: 215895, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1502.25, #queue-req: 0, 
[2025-11-02 15:26:39 TP0] Decode batch, #running-req: 64, #token: 218455, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1498.73, #queue-req: 0, 
[2025-11-02 15:26:41 TP0] Decode batch, #running-req: 64, #token: 221015, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.09, #queue-req: 0, 
[2025-11-02 15:26:42 TP0] Decode batch, #running-req: 64, #token: 223575, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1489.09, #queue-req: 0, 
[2025-11-02 15:26:44 TP0] Decode batch, #running-req: 64, #token: 226135, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.83, #queue-req: 0, 
[2025-11-02 15:26:46 TP0] Decode batch, #running-req: 64, #token: 228695, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.51, #queue-req: 0, 
[2025-11-02 15:26:47 TP0] Decode batch, #running-req: 64, #token: 231255, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.76, #queue-req: 0, 
[2025-11-02 15:26:49 TP0] Decode batch, #running-req: 64, #token: 233815, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1487.18, #queue-req: 0, 
[2025-11-02 15:26:51 TP0] Decode batch, #running-req: 64, #token: 236375, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.72, #queue-req: 0, 
[2025-11-02 15:26:53 TP0] Decode batch, #running-req: 64, #token: 238935, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1485.01, #queue-req: 0, 
[2025-11-02 15:26:54 TP0] Decode batch, #running-req: 64, #token: 241495, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.32, #queue-req: 0, 
[2025-11-02 15:26:56 TP0] Decode batch, #running-req: 64, #token: 244055, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.96, #queue-req: 0, 
[2025-11-02 15:26:58 TP0] Decode batch, #running-req: 64, #token: 246615, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1473.80, #queue-req: 0, 
[2025-11-02 15:27:00 TP0] Decode batch, #running-req: 64, #token: 249175, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1473.92, #queue-req: 0, 
[2025-11-02 15:27:01 TP0] Decode batch, #running-req: 64, #token: 251735, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1471.17, #queue-req: 0, 
[2025-11-02 15:27:03 TP0] Decode batch, #running-req: 64, #token: 254295, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1469.74, #queue-req: 0, 
[2025-11-02 15:27:04] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:27:04] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:27:04] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:43990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:04] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-11-02 15:27:05] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:27:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-02 15:27:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-02 15:27:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-02 15:27:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-02 15:27:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-02 15:27:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-02 15:27:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-02 15:27:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-02 15:27:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-02 15:27:13 TP0] Prefill batch, #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-02 15:27:15 TP0] Decode batch, #running-req: 64, #token: 205669, token usage: 0.21, cuda graph: True, gen throughput (token/s): 210.20, #queue-req: 0, 
[2025-11-02 15:27:17 TP0] Decode batch, #running-req: 64, #token: 208229, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1527.46, #queue-req: 0, 
[2025-11-02 15:27:19 TP0] Decode batch, #running-req: 64, #token: 210789, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.33, #queue-req: 0, 
[2025-11-02 15:27:20 TP0] Decode batch, #running-req: 64, #token: 213349, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.63, #queue-req: 0, 
[2025-11-02 15:27:22 TP0] Decode batch, #running-req: 64, #token: 215909, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.36, #queue-req: 0, 
[2025-11-02 15:27:24 TP0] Decode batch, #running-req: 64, #token: 218469, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.27, #queue-req: 0, 
[2025-11-02 15:27:25 TP0] Decode batch, #running-req: 64, #token: 221029, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1484.19, #queue-req: 0, 
[2025-11-02 15:27:27 TP0] Decode batch, #running-req: 64, #token: 223589, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1475.21, #queue-req: 0, 
[2025-11-02 15:27:29 TP0] Decode batch, #running-req: 64, #token: 226149, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1474.43, #queue-req: 0, 
[2025-11-02 15:27:31 TP0] Decode batch, #running-req: 64, #token: 228709, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1471.83, #queue-req: 0, 
[2025-11-02 15:27:32 TP0] Decode batch, #running-req: 64, #token: 231269, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1475.63, #queue-req: 0, 
[2025-11-02 15:27:34 TP0] Decode batch, #running-req: 64, #token: 233829, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1476.68, #queue-req: 0, 
[2025-11-02 15:27:36 TP0] Decode batch, #running-req: 64, #token: 236389, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1479.52, #queue-req: 0, 
[2025-11-02 15:27:38 TP0] Decode batch, #running-req: 64, #token: 238949, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1478.64, #queue-req: 0, 
[2025-11-02 15:27:39 TP0] Decode batch, #running-req: 64, #token: 241509, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1477.04, #queue-req: 0, 
[2025-11-02 15:27:41 TP0] Decode batch, #running-req: 64, #token: 244069, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1473.45, #queue-req: 0, 
[2025-11-02 15:27:43 TP0] Decode batch, #running-req: 64, #token: 246629, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1470.69, #queue-req: 0, 
[2025-11-02 15:27:45 TP0] Decode batch, #running-req: 64, #token: 249189, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1466.60, #queue-req: 0, 
[2025-11-02 15:27:46 TP0] Decode batch, #running-req: 64, #token: 251749, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1465.65, #queue-req: 0, 
[2025-11-02 15:27:48 TP0] Decode batch, #running-req: 64, #token: 254309, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1464.05, #queue-req: 0, 
[2025-11-02 15:27:49] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:27:49] INFO:     127.0.0.1:39720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 1, #queue-req: 9, 
[2025-11-02 15:27:49] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:49] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 7, #queue-req: 39, 
[2025-11-02 15:27:50] INFO:     127.0.0.1:40210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:27:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 12, #queue-req: 47, 
[2025-11-02 15:27:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.06, #running-req: 17, #queue-req: 42, 
[2025-11-02 15:27:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-02 15:27:53 TP0] Prefill batch, #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.10, #running-req: 27, #queue-req: 30, 
[2025-11-02 15:27:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 34, #queue-req: 25, 
[2025-11-02 15:27:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.13, #running-req: 39, #queue-req: 20, 
[2025-11-02 15:27:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.14, #running-req: 44, #queue-req: 15, 
[2025-11-02 15:27:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-11-02 15:27:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 54, #queue-req: 5, 
[2025-11-02 15:27:58 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.19, #running-req: 59, #queue-req: 0, 
[2025-11-02 15:28:00 TP0] Decode batch, #running-req: 64, #token: 202458, token usage: 0.21, cuda graph: True, gen throughput (token/s): 220.90, #queue-req: 0, 
[2025-11-02 15:28:01 TP0] Decode batch, #running-req: 64, #token: 205018, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1518.91, #queue-req: 0, 
[2025-11-02 15:28:03 TP0] Decode batch, #running-req: 64, #token: 207578, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1503.76, #queue-req: 0, 
[2025-11-02 15:28:05 TP0] Decode batch, #running-req: 64, #token: 210138, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.93, #queue-req: 0, 
[2025-11-02 15:28:06 TP0] Decode batch, #running-req: 64, #token: 212698, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.71, #queue-req: 0, 
[2025-11-02 15:28:08 TP0] Decode batch, #running-req: 64, #token: 215258, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.97, #queue-req: 0, 
[2025-11-02 15:28:10 TP0] Decode batch, #running-req: 64, #token: 217818, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.33, #queue-req: 0, 
[2025-11-02 15:28:12 TP0] Decode batch, #running-req: 64, #token: 220378, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.64, #queue-req: 0, 
[2025-11-02 15:28:13 TP0] Decode batch, #running-req: 64, #token: 222938, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1496.11, #queue-req: 0, 
[2025-11-02 15:28:15 TP0] Decode batch, #running-req: 64, #token: 225498, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1493.90, #queue-req: 0, 
[2025-11-02 15:28:17 TP0] Decode batch, #running-req: 64, #token: 228058, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1492.13, #queue-req: 0, 
[2025-11-02 15:28:18 TP0] Decode batch, #running-req: 64, #token: 230618, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.74, #queue-req: 0, 
[2025-11-02 15:28:20 TP0] Decode batch, #running-req: 64, #token: 233178, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1494.11, #queue-req: 0, 
[2025-11-02 15:28:22 TP0] Decode batch, #running-req: 64, #token: 235738, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1493.50, #queue-req: 0, 
[2025-11-02 15:28:24 TP0] Decode batch, #running-req: 64, #token: 238298, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1492.27, #queue-req: 0, 
[2025-11-02 15:28:25 TP0] Decode batch, #running-req: 64, #token: 240858, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1493.22, #queue-req: 0, 
[2025-11-02 15:28:27 TP0] Decode batch, #running-req: 64, #token: 243418, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.53, #queue-req: 0, 
[2025-11-02 15:28:29 TP0] Decode batch, #running-req: 64, #token: 245978, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.75, #queue-req: 0, 
[2025-11-02 15:28:30 TP0] Decode batch, #running-req: 64, #token: 248538, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1482.87, #queue-req: 0, 
[2025-11-02 15:28:32 TP0] Decode batch, #running-req: 64, #token: 251098, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1480.45, #queue-req: 0, 
[2025-11-02 15:28:33] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:28:33] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 11, 
[2025-11-02 15:28:33] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:33] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-11-02 15:28:34] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:28:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:28:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-02 15:28:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-02 15:28:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-02 15:28:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-02 15:28:39 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 22, 
[2025-11-02 15:28:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-02 15:28:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-02 15:28:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-02 15:28:42 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 1, 
[2025-11-02 15:28:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-11-02 15:28:44 TP0] Decode batch, #running-req: 64, #token: 205658, token usage: 0.21, cuda graph: True, gen throughput (token/s): 214.18, #queue-req: 0, 
[2025-11-02 15:28:46 TP0] Decode batch, #running-req: 64, #token: 208218, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1523.07, #queue-req: 0, 
[2025-11-02 15:28:48 TP0] Decode batch, #running-req: 64, #token: 210778, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1510.19, #queue-req: 0, 
[2025-11-02 15:28:49 TP0] Decode batch, #running-req: 64, #token: 213338, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.12, #queue-req: 0, 
[2025-11-02 15:28:51 TP0] Decode batch, #running-req: 64, #token: 215898, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.17, #queue-req: 0, 
[2025-11-02 15:28:53 TP0] Decode batch, #running-req: 64, #token: 218458, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.51, #queue-req: 0, 
[2025-11-02 15:28:54 TP0] Decode batch, #running-req: 64, #token: 221018, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.72, #queue-req: 0, 
[2025-11-02 15:28:56 TP0] Decode batch, #running-req: 64, #token: 223578, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.28, #queue-req: 0, 
[2025-11-02 15:28:58 TP0] Decode batch, #running-req: 64, #token: 226138, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.70, #queue-req: 0, 
[2025-11-02 15:29:00 TP0] Decode batch, #running-req: 64, #token: 228698, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.60, #queue-req: 0, 
[2025-11-02 15:29:01 TP0] Decode batch, #running-req: 64, #token: 231258, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.35, #queue-req: 0, 
[2025-11-02 15:29:03 TP0] Decode batch, #running-req: 64, #token: 233818, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.22, #queue-req: 0, 
[2025-11-02 15:29:05 TP0] Decode batch, #running-req: 64, #token: 236378, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1489.21, #queue-req: 0, 
[2025-11-02 15:29:06 TP0] Decode batch, #running-req: 64, #token: 238938, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.87, #queue-req: 0, 
[2025-11-02 15:29:08 TP0] Decode batch, #running-req: 64, #token: 241498, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1485.43, #queue-req: 0, 
[2025-11-02 15:29:10 TP0] Decode batch, #running-req: 64, #token: 244058, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.27, #queue-req: 0, 
[2025-11-02 15:29:12 TP0] Decode batch, #running-req: 64, #token: 246618, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.10, #queue-req: 0, 
[2025-11-02 15:29:13 TP0] Decode batch, #running-req: 64, #token: 249178, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1479.38, #queue-req: 0, 
[2025-11-02 15:29:15 TP0] Decode batch, #running-req: 64, #token: 251738, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.17, #queue-req: 0, 
[2025-11-02 15:29:17 TP0] Decode batch, #running-req: 64, #token: 254298, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1483.69, #queue-req: 0, 
[2025-11-02 15:29:18] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:29:18] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:29:18] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-11-02 15:29:18] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:18] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:29:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:29:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-02 15:29:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-02 15:29:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-02 15:29:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-02 15:29:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-02 15:29:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-02 15:29:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-02 15:29:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-02 15:29:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-02 15:29:27 TP0] Prefill batch, #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-02 15:29:29 TP0] Decode batch, #running-req: 64, #token: 205672, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.12, #queue-req: 0, 
[2025-11-02 15:29:31 TP0] Decode batch, #running-req: 64, #token: 208232, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1535.19, #queue-req: 0, 
[2025-11-02 15:29:32 TP0] Decode batch, #running-req: 64, #token: 210792, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1511.40, #queue-req: 0, 
[2025-11-02 15:29:34 TP0] Decode batch, #running-req: 64, #token: 213352, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1504.47, #queue-req: 0, 
[2025-11-02 15:29:36 TP0] Decode batch, #running-req: 64, #token: 215912, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.98, #queue-req: 0, 
[2025-11-02 15:29:37 TP0] Decode batch, #running-req: 64, #token: 218472, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.11, #queue-req: 0, 
[2025-11-02 15:29:39 TP0] Decode batch, #running-req: 64, #token: 221032, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.72, #queue-req: 0, 
[2025-11-02 15:29:41 TP0] Decode batch, #running-req: 64, #token: 223592, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1497.72, #queue-req: 0, 
[2025-11-02 15:29:43 TP0] Decode batch, #running-req: 64, #token: 226152, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1499.14, #queue-req: 0, 
[2025-11-02 15:29:44 TP0] Decode batch, #running-req: 64, #token: 228712, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1495.40, #queue-req: 0, 
[2025-11-02 15:29:46 TP0] Decode batch, #running-req: 64, #token: 231272, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1500.17, #queue-req: 0, 
[2025-11-02 15:29:48 TP0] Decode batch, #running-req: 64, #token: 233832, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1504.97, #queue-req: 0, 
[2025-11-02 15:29:49 TP0] Decode batch, #running-req: 64, #token: 236392, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1502.39, #queue-req: 0, 
[2025-11-02 15:29:51 TP0] Decode batch, #running-req: 64, #token: 238952, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1503.48, #queue-req: 0, 
[2025-11-02 15:29:53 TP0] Decode batch, #running-req: 64, #token: 241512, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1499.94, #queue-req: 0, 
[2025-11-02 15:29:55 TP0] Decode batch, #running-req: 64, #token: 244072, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1497.51, #queue-req: 0, 
[2025-11-02 15:29:56 TP0] Decode batch, #running-req: 64, #token: 246632, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.92, #queue-req: 0, 
[2025-11-02 15:29:58 TP0] Decode batch, #running-req: 64, #token: 249192, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1487.71, #queue-req: 0, 
[2025-11-02 15:30:00 TP0] Decode batch, #running-req: 64, #token: 251752, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1487.99, #queue-req: 0, 
[2025-11-02 15:30:01 TP0] Decode batch, #running-req: 64, #token: 254312, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1487.41, #queue-req: 0, 
[2025-11-02 15:30:03] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:30:03] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:30:03] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-11-02 15:30:03] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:03] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-02 15:30:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-02 15:30:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-02 15:30:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-02 15:30:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-02 15:30:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-02 15:30:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-02 15:30:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-02 15:30:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-02 15:30:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-02 15:30:12 TP0] Prefill batch, #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-02 15:30:14 TP0] Decode batch, #running-req: 64, #token: 205662, token usage: 0.21, cuda graph: True, gen throughput (token/s): 209.84, #queue-req: 0, 
[2025-11-02 15:30:15 TP0] Decode batch, #running-req: 64, #token: 208222, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1532.17, #queue-req: 0, 
[2025-11-02 15:30:17 TP0] Decode batch, #running-req: 64, #token: 210782, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1514.04, #queue-req: 0, 
[2025-11-02 15:30:19 TP0] Decode batch, #running-req: 64, #token: 213342, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.34, #queue-req: 0, 
[2025-11-02 15:30:20 TP0] Decode batch, #running-req: 64, #token: 215902, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1498.75, #queue-req: 0, 
[2025-11-02 15:30:22 TP0] Decode batch, #running-req: 64, #token: 218462, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.00, #queue-req: 0, 
[2025-11-02 15:30:24 TP0] Decode batch, #running-req: 64, #token: 221022, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.51, #queue-req: 0, 
[2025-11-02 15:30:26 TP0] Decode batch, #running-req: 64, #token: 223582, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.68, #queue-req: 0, 
[2025-11-02 15:30:27 TP0] Decode batch, #running-req: 64, #token: 226142, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1483.80, #queue-req: 0, 
[2025-11-02 15:30:29 TP0] Decode batch, #running-req: 64, #token: 228702, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.24, #queue-req: 0, 
[2025-11-02 15:30:31 TP0] Decode batch, #running-req: 64, #token: 231262, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.58, #queue-req: 0, 
[2025-11-02 15:30:32 TP0] Decode batch, #running-req: 64, #token: 233822, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.79, #queue-req: 0, 
[2025-11-02 15:30:34 TP0] Decode batch, #running-req: 64, #token: 236382, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1486.54, #queue-req: 0, 
[2025-11-02 15:30:36 TP0] Decode batch, #running-req: 64, #token: 238942, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1485.73, #queue-req: 0, 
[2025-11-02 15:30:38 TP0] Decode batch, #running-req: 64, #token: 241502, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1483.69, #queue-req: 0, 
[2025-11-02 15:30:39 TP0] Decode batch, #running-req: 64, #token: 244062, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.76, #queue-req: 0, 
[2025-11-02 15:30:41 TP0] Decode batch, #running-req: 64, #token: 246622, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.79, #queue-req: 0, 
[2025-11-02 15:30:43 TP0] Decode batch, #running-req: 64, #token: 249182, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1475.07, #queue-req: 0, 
[2025-11-02 15:30:45 TP0] Decode batch, #running-req: 64, #token: 251742, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1473.61, #queue-req: 0, 
[2025-11-02 15:30:46 TP0] Decode batch, #running-req: 64, #token: 254302, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.75, #queue-req: 0, 
[2025-11-02 15:30:47] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:30:47] INFO:     127.0.0.1:35378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:47] INFO:     127.0.0.1:35436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:30:48] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:30:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-11-02 15:30:49 TP0] Prefill batch, #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.04, #running-req: 11, #queue-req: 35, 
[2025-11-02 15:30:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 17, #queue-req: 30, 
[2025-11-02 15:30:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 25, 
[2025-11-02 15:30:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.09, #running-req: 27, #queue-req: 20, 
[2025-11-02 15:30:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.11, #running-req: 32, #queue-req: 15, 
[2025-11-02 15:30:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 37, #queue-req: 10, 
[2025-11-02 15:30:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.14, #running-req: 42, #queue-req: 5, 
[2025-11-02 15:30:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-11-02 15:30:56 TP0] Decode batch, #running-req: 52, #token: 167102, token usage: 0.17, cuda graph: True, gen throughput (token/s): 238.48, #queue-req: 0, 
[2025-11-02 15:30:58 TP0] Decode batch, #running-req: 52, #token: 169182, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1291.24, #queue-req: 0, 
[2025-11-02 15:31:00 TP0] Decode batch, #running-req: 52, #token: 171262, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1279.63, #queue-req: 0, 
[2025-11-02 15:31:01 TP0] Decode batch, #running-req: 52, #token: 173342, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1272.02, #queue-req: 0, 
[2025-11-02 15:31:03 TP0] Decode batch, #running-req: 52, #token: 175422, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1267.46, #queue-req: 0, 
[2025-11-02 15:31:04 TP0] Decode batch, #running-req: 52, #token: 177502, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1265.30, #queue-req: 0, 
[2025-11-02 15:31:06 TP0] Decode batch, #running-req: 52, #token: 179582, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1260.69, #queue-req: 0, 
[2025-11-02 15:31:08 TP0] Decode batch, #running-req: 52, #token: 181662, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1262.82, #queue-req: 0, 
[2025-11-02 15:31:09 TP0] Decode batch, #running-req: 52, #token: 183742, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1259.03, #queue-req: 0, 
[2025-11-02 15:31:11 TP0] Decode batch, #running-req: 52, #token: 185822, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.03, #queue-req: 0, 
[2025-11-02 15:31:13 TP0] Decode batch, #running-req: 52, #token: 187902, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1257.78, #queue-req: 0, 
[2025-11-02 15:31:14 TP0] Decode batch, #running-req: 52, #token: 189982, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1257.98, #queue-req: 0, 
[2025-11-02 15:31:16 TP0] Decode batch, #running-req: 52, #token: 192062, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1258.02, #queue-req: 0, 
[2025-11-02 15:31:18 TP0] Decode batch, #running-req: 52, #token: 194142, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1254.70, #queue-req: 0, 
[2025-11-02 15:31:19 TP0] Decode batch, #running-req: 52, #token: 196222, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1251.59, #queue-req: 0, 
[2025-11-02 15:31:21 TP0] Decode batch, #running-req: 52, #token: 198302, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1246.81, #queue-req: 0, 
[2025-11-02 15:31:23 TP0] Decode batch, #running-req: 52, #token: 200382, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1248.44, #queue-req: 0, 
[2025-11-02 15:31:24 TP0] Decode batch, #running-req: 52, #token: 202462, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1243.63, #queue-req: 0, 
[2025-11-02 15:31:26 TP0] Decode batch, #running-req: 52, #token: 204542, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1246.95, #queue-req: 0, 
[2025-11-02 15:31:28 TP0] Decode batch, #running-req: 52, #token: 206622, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1245.72, #queue-req: 0, 
[2025-11-02 15:31:29] INFO:     127.0.0.1:56184 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-02 15:31:46] INFO:     127.0.0.1:39016 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-02 15:31:52] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:52 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:31:53 TP0] Decode batch, #running-req: 1, #token: 3216, token usage: 0.00, cuda graph: True, gen throughput (token/s): 55.05, #queue-req: 0, 
[2025-11-02 15:31:54] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:31:54] INFO:     127.0.0.1:39044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54] INFO:     127.0.0.1:39164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:31:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-02 15:31:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:31:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:31:57 TP0] Decode batch, #running-req: 16, #token: 51554, token usage: 0.05, cuda graph: True, gen throughput (token/s): 83.06, #queue-req: 0, 
[2025-11-02 15:31:58 TP0] Decode batch, #running-req: 16, #token: 52194, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.08, #queue-req: 0, 
[2025-11-02 15:31:59 TP0] Decode batch, #running-req: 16, #token: 52834, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.83, #queue-req: 0, 
[2025-11-02 15:32:00 TP0] Decode batch, #running-req: 16, #token: 53474, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.74, #queue-req: 0, 
[2025-11-02 15:32:01 TP0] Decode batch, #running-req: 16, #token: 54114, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.79, #queue-req: 0, 
[2025-11-02 15:32:02 TP0] Decode batch, #running-req: 16, #token: 54754, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.96, #queue-req: 0, 
[2025-11-02 15:32:03 TP0] Decode batch, #running-req: 16, #token: 55394, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.97, #queue-req: 0, 
[2025-11-02 15:32:04 TP0] Decode batch, #running-req: 16, #token: 56034, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.34, #queue-req: 0, 
[2025-11-02 15:32:05 TP0] Decode batch, #running-req: 16, #token: 56674, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.06, #queue-req: 0, 
[2025-11-02 15:32:06 TP0] Decode batch, #running-req: 16, #token: 57314, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.19, #queue-req: 0, 
[2025-11-02 15:32:07 TP0] Decode batch, #running-req: 16, #token: 57954, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.75, #queue-req: 0, 
[2025-11-02 15:32:08 TP0] Decode batch, #running-req: 16, #token: 58594, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.50, #queue-req: 0, 
[2025-11-02 15:32:10 TP0] Decode batch, #running-req: 16, #token: 59234, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.49, #queue-req: 0, 
[2025-11-02 15:32:11 TP0] Decode batch, #running-req: 16, #token: 59874, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.19, #queue-req: 0, 
[2025-11-02 15:32:12 TP0] Decode batch, #running-req: 16, #token: 60514, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.25, #queue-req: 0, 
[2025-11-02 15:32:13 TP0] Decode batch, #running-req: 16, #token: 61154, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.14, #queue-req: 0, 
[2025-11-02 15:32:14 TP0] Decode batch, #running-req: 16, #token: 61794, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.02, #queue-req: 0, 
[2025-11-02 15:32:15 TP0] Decode batch, #running-req: 16, #token: 62434, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.72, #queue-req: 0, 
[2025-11-02 15:32:16 TP0] Decode batch, #running-req: 16, #token: 63074, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.11, #queue-req: 0, 
[2025-11-02 15:32:17 TP0] Decode batch, #running-req: 16, #token: 63714, token usage: 0.07, cuda graph: True, gen throughput (token/s): 605.94, #queue-req: 0, 
[2025-11-02 15:32:17] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:32:17] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:17] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:32:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:32:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:32:21 TP0] Decode batch, #running-req: 16, #token: 51548, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.94, #queue-req: 0, 
[2025-11-02 15:32:22 TP0] Decode batch, #running-req: 16, #token: 52188, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.33, #queue-req: 0, 
[2025-11-02 15:32:23 TP0] Decode batch, #running-req: 16, #token: 52828, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.91, #queue-req: 0, 
[2025-11-02 15:32:24 TP0] Decode batch, #running-req: 16, #token: 53468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.81, #queue-req: 0, 
[2025-11-02 15:32:25 TP0] Decode batch, #running-req: 16, #token: 54108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.20, #queue-req: 0, 
[2025-11-02 15:32:26 TP0] Decode batch, #running-req: 16, #token: 54748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.90, #queue-req: 0, 
[2025-11-02 15:32:27 TP0] Decode batch, #running-req: 16, #token: 55388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.10, #queue-req: 0, 
[2025-11-02 15:32:28 TP0] Decode batch, #running-req: 16, #token: 56028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.76, #queue-req: 0, 
[2025-11-02 15:32:29 TP0] Decode batch, #running-req: 16, #token: 56668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.05, #queue-req: 0, 
[2025-11-02 15:32:30 TP0] Decode batch, #running-req: 16, #token: 57308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.73, #queue-req: 0, 
[2025-11-02 15:32:31 TP0] Decode batch, #running-req: 16, #token: 57948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.49, #queue-req: 0, 
[2025-11-02 15:32:32 TP0] Decode batch, #running-req: 16, #token: 58588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.21, #queue-req: 0, 
[2025-11-02 15:32:33 TP0] Decode batch, #running-req: 16, #token: 59228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.69, #queue-req: 0, 
[2025-11-02 15:32:34 TP0] Decode batch, #running-req: 16, #token: 59868, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.84, #queue-req: 0, 
[2025-11-02 15:32:35 TP0] Decode batch, #running-req: 16, #token: 60508, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.25, #queue-req: 0, 
[2025-11-02 15:32:36 TP0] Decode batch, #running-req: 16, #token: 61148, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.71, #queue-req: 0, 
[2025-11-02 15:32:37 TP0] Decode batch, #running-req: 16, #token: 61788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.94, #queue-req: 0, 
[2025-11-02 15:32:38 TP0] Decode batch, #running-req: 16, #token: 62428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.97, #queue-req: 0, 
[2025-11-02 15:32:40 TP0] Decode batch, #running-req: 16, #token: 63068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.59, #queue-req: 0, 
[2025-11-02 15:32:41 TP0] Decode batch, #running-req: 16, #token: 63708, token usage: 0.07, cuda graph: True, gen throughput (token/s): 607.74, #queue-req: 0, 
[2025-11-02 15:32:41] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:32:41] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:32:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:32:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:32:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:32:44 TP0] Decode batch, #running-req: 16, #token: 51550, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.17, #queue-req: 0, 
[2025-11-02 15:32:45 TP0] Decode batch, #running-req: 16, #token: 52190, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.88, #queue-req: 0, 
[2025-11-02 15:32:46 TP0] Decode batch, #running-req: 16, #token: 52830, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.62, #queue-req: 0, 
[2025-11-02 15:32:47 TP0] Decode batch, #running-req: 16, #token: 53470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 619.43, #queue-req: 0, 
[2025-11-02 15:32:48 TP0] Decode batch, #running-req: 16, #token: 54110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.80, #queue-req: 0, 
[2025-11-02 15:32:49 TP0] Decode batch, #running-req: 16, #token: 54750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.42, #queue-req: 0, 
[2025-11-02 15:32:50 TP0] Decode batch, #running-req: 16, #token: 55390, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.09, #queue-req: 0, 
[2025-11-02 15:32:52 TP0] Decode batch, #running-req: 16, #token: 56030, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.89, #queue-req: 0, 
[2025-11-02 15:32:53 TP0] Decode batch, #running-req: 16, #token: 56670, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.54, #queue-req: 0, 
[2025-11-02 15:32:54 TP0] Decode batch, #running-req: 16, #token: 57310, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.58, #queue-req: 0, 
[2025-11-02 15:32:55 TP0] Decode batch, #running-req: 16, #token: 57950, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.38, #queue-req: 0, 
[2025-11-02 15:32:56 TP0] Decode batch, #running-req: 16, #token: 58590, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.73, #queue-req: 0, 
[2025-11-02 15:32:57 TP0] Decode batch, #running-req: 16, #token: 59230, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.52, #queue-req: 0, 
[2025-11-02 15:32:58 TP0] Decode batch, #running-req: 16, #token: 59870, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.16, #queue-req: 0, 
[2025-11-02 15:32:59 TP0] Decode batch, #running-req: 16, #token: 60510, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.83, #queue-req: 0, 
[2025-11-02 15:33:00 TP0] Decode batch, #running-req: 16, #token: 61150, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.05, #queue-req: 0, 
[2025-11-02 15:33:01 TP0] Decode batch, #running-req: 16, #token: 61790, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.35, #queue-req: 0, 
[2025-11-02 15:33:02 TP0] Decode batch, #running-req: 16, #token: 62430, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.81, #queue-req: 0, 
[2025-11-02 15:33:03 TP0] Decode batch, #running-req: 16, #token: 63070, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.34, #queue-req: 0, 
[2025-11-02 15:33:04 TP0] Decode batch, #running-req: 16, #token: 63710, token usage: 0.07, cuda graph: True, gen throughput (token/s): 608.74, #queue-req: 0, 
[2025-11-02 15:33:05] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:33:05] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:33:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:33:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:33:08 TP0] Decode batch, #running-req: 16, #token: 51553, token usage: 0.05, cuda graph: True, gen throughput (token/s): 174.12, #queue-req: 0, 
[2025-11-02 15:33:09 TP0] Decode batch, #running-req: 16, #token: 52193, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.17, #queue-req: 0, 
[2025-11-02 15:33:10 TP0] Decode batch, #running-req: 16, #token: 52833, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.41, #queue-req: 0, 
[2025-11-02 15:33:11 TP0] Decode batch, #running-req: 16, #token: 53473, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.90, #queue-req: 0, 
[2025-11-02 15:33:12 TP0] Decode batch, #running-req: 16, #token: 54113, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.36, #queue-req: 0, 
[2025-11-02 15:33:13 TP0] Decode batch, #running-req: 16, #token: 54753, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.80, #queue-req: 0, 
[2025-11-02 15:33:14 TP0] Decode batch, #running-req: 16, #token: 55393, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.57, #queue-req: 0, 
[2025-11-02 15:33:15 TP0] Decode batch, #running-req: 16, #token: 56033, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.94, #queue-req: 0, 
[2025-11-02 15:33:16 TP0] Decode batch, #running-req: 16, #token: 56673, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.32, #queue-req: 0, 
[2025-11-02 15:33:17 TP0] Decode batch, #running-req: 16, #token: 57313, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.38, #queue-req: 0, 
[2025-11-02 15:33:18 TP0] Decode batch, #running-req: 16, #token: 57953, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.26, #queue-req: 0, 
[2025-11-02 15:33:19 TP0] Decode batch, #running-req: 16, #token: 58593, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.46, #queue-req: 0, 
[2025-11-02 15:33:20 TP0] Decode batch, #running-req: 16, #token: 59233, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.06, #queue-req: 0, 
[2025-11-02 15:33:21 TP0] Decode batch, #running-req: 16, #token: 59873, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.67, #queue-req: 0, 
[2025-11-02 15:33:22 TP0] Decode batch, #running-req: 16, #token: 60513, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.08, #queue-req: 0, 
[2025-11-02 15:33:24 TP0] Decode batch, #running-req: 16, #token: 61153, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.97, #queue-req: 0, 
[2025-11-02 15:33:25 TP0] Decode batch, #running-req: 16, #token: 61793, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.60, #queue-req: 0, 
[2025-11-02 15:33:26 TP0] Decode batch, #running-req: 16, #token: 62433, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.74, #queue-req: 0, 
[2025-11-02 15:33:27 TP0] Decode batch, #running-req: 16, #token: 63073, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.24, #queue-req: 0, 
[2025-11-02 15:33:28 TP0] Decode batch, #running-req: 16, #token: 63713, token usage: 0.07, cuda graph: True, gen throughput (token/s): 609.74, #queue-req: 0, 
[2025-11-02 15:33:28] INFO:     127.0.0.1:43876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:33:28] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:33:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:33:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:33:31 TP0] Decode batch, #running-req: 16, #token: 51548, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.65, #queue-req: 0, 
[2025-11-02 15:33:32 TP0] Decode batch, #running-req: 16, #token: 52188, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.58, #queue-req: 0, 
[2025-11-02 15:33:33 TP0] Decode batch, #running-req: 16, #token: 52828, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.87, #queue-req: 0, 
[2025-11-02 15:33:35 TP0] Decode batch, #running-req: 16, #token: 53468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 621.90, #queue-req: 0, 
[2025-11-02 15:33:36 TP0] Decode batch, #running-req: 16, #token: 54108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.19, #queue-req: 0, 
[2025-11-02 15:33:37 TP0] Decode batch, #running-req: 16, #token: 54748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.39, #queue-req: 0, 
[2025-11-02 15:33:38 TP0] Decode batch, #running-req: 16, #token: 55388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.00, #queue-req: 0, 
[2025-11-02 15:33:39 TP0] Decode batch, #running-req: 16, #token: 56028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.21, #queue-req: 0, 
[2025-11-02 15:33:40 TP0] Decode batch, #running-req: 16, #token: 56668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.22, #queue-req: 0, 
[2025-11-02 15:33:41 TP0] Decode batch, #running-req: 16, #token: 57308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.54, #queue-req: 0, 
[2025-11-02 15:33:42 TP0] Decode batch, #running-req: 16, #token: 57948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.08, #queue-req: 0, 
[2025-11-02 15:33:43 TP0] Decode batch, #running-req: 16, #token: 58588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.17, #queue-req: 0, 
[2025-11-02 15:33:44 TP0] Decode batch, #running-req: 16, #token: 59228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.22, #queue-req: 0, 
[2025-11-02 15:33:45 TP0] Decode batch, #running-req: 16, #token: 59868, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.16, #queue-req: 0, 
[2025-11-02 15:33:46 TP0] Decode batch, #running-req: 16, #token: 60508, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.79, #queue-req: 0, 
[2025-11-02 15:33:47 TP0] Decode batch, #running-req: 16, #token: 61148, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.76, #queue-req: 0, 
[2025-11-02 15:33:48 TP0] Decode batch, #running-req: 16, #token: 61788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.80, #queue-req: 0, 
[2025-11-02 15:33:49 TP0] Decode batch, #running-req: 16, #token: 62428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.28, #queue-req: 0, 
[2025-11-02 15:33:50 TP0] Decode batch, #running-req: 16, #token: 63068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.73, #queue-req: 0, 
[2025-11-02 15:33:51 TP0] Decode batch, #running-req: 16, #token: 63708, token usage: 0.07, cuda graph: True, gen throughput (token/s): 613.20, #queue-req: 0, 
[2025-11-02 15:33:52] INFO:     127.0.0.1:45246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:33:52] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:33:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:33:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:33:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:33:55 TP0] Decode batch, #running-req: 16, #token: 51548, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.37, #queue-req: 0, 
[2025-11-02 15:33:56 TP0] Decode batch, #running-req: 16, #token: 52188, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.13, #queue-req: 0, 
[2025-11-02 15:33:57 TP0] Decode batch, #running-req: 16, #token: 52828, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.63, #queue-req: 0, 
[2025-11-02 15:33:58 TP0] Decode batch, #running-req: 16, #token: 53468, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.54, #queue-req: 0, 
[2025-11-02 15:33:59 TP0] Decode batch, #running-req: 16, #token: 54108, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.45, #queue-req: 0, 
[2025-11-02 15:34:00 TP0] Decode batch, #running-req: 16, #token: 54748, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.83, #queue-req: 0, 
[2025-11-02 15:34:01 TP0] Decode batch, #running-req: 16, #token: 55388, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.37, #queue-req: 0, 
[2025-11-02 15:34:02 TP0] Decode batch, #running-req: 16, #token: 56028, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.19, #queue-req: 0, 
[2025-11-02 15:34:03 TP0] Decode batch, #running-req: 16, #token: 56668, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.37, #queue-req: 0, 
[2025-11-02 15:34:04 TP0] Decode batch, #running-req: 16, #token: 57308, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.37, #queue-req: 0, 
[2025-11-02 15:34:05 TP0] Decode batch, #running-req: 16, #token: 57948, token usage: 0.06, cuda graph: True, gen throughput (token/s): 617.04, #queue-req: 0, 
[2025-11-02 15:34:06 TP0] Decode batch, #running-req: 16, #token: 58588, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.05, #queue-req: 0, 
[2025-11-02 15:34:07 TP0] Decode batch, #running-req: 16, #token: 59228, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.89, #queue-req: 0, 
[2025-11-02 15:34:08 TP0] Decode batch, #running-req: 16, #token: 59868, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.54, #queue-req: 0, 
[2025-11-02 15:34:09 TP0] Decode batch, #running-req: 16, #token: 60508, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.51, #queue-req: 0, 
[2025-11-02 15:34:11 TP0] Decode batch, #running-req: 16, #token: 61148, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.39, #queue-req: 0, 
[2025-11-02 15:34:12 TP0] Decode batch, #running-req: 16, #token: 61788, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.68, #queue-req: 0, 
[2025-11-02 15:34:13 TP0] Decode batch, #running-req: 16, #token: 62428, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.28, #queue-req: 0, 
[2025-11-02 15:34:14 TP0] Decode batch, #running-req: 16, #token: 63068, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.29, #queue-req: 0, 
[2025-11-02 15:34:15 TP0] Decode batch, #running-req: 16, #token: 63708, token usage: 0.07, cuda graph: True, gen throughput (token/s): 615.31, #queue-req: 0, 
[2025-11-02 15:34:15] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:34:15] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:34:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:34:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:34:18 TP0] Decode batch, #running-req: 16, #token: 51550, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.93, #queue-req: 0, 
[2025-11-02 15:34:19 TP0] Decode batch, #running-req: 16, #token: 52190, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.89, #queue-req: 0, 
[2025-11-02 15:34:20 TP0] Decode batch, #running-req: 16, #token: 52830, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.69, #queue-req: 0, 
[2025-11-02 15:34:21 TP0] Decode batch, #running-req: 16, #token: 53470, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.49, #queue-req: 0, 
[2025-11-02 15:34:23 TP0] Decode batch, #running-req: 16, #token: 54110, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.20, #queue-req: 0, 
[2025-11-02 15:34:24 TP0] Decode batch, #running-req: 16, #token: 54750, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.23, #queue-req: 0, 
[2025-11-02 15:34:25 TP0] Decode batch, #running-req: 16, #token: 55390, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.56, #queue-req: 0, 
[2025-11-02 15:34:26 TP0] Decode batch, #running-req: 16, #token: 56030, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.85, #queue-req: 0, 
[2025-11-02 15:34:27 TP0] Decode batch, #running-req: 16, #token: 56670, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.77, #queue-req: 0, 
[2025-11-02 15:34:28 TP0] Decode batch, #running-req: 16, #token: 57310, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.80, #queue-req: 0, 
[2025-11-02 15:34:29 TP0] Decode batch, #running-req: 16, #token: 57950, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.01, #queue-req: 0, 
[2025-11-02 15:34:30 TP0] Decode batch, #running-req: 16, #token: 58590, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.34, #queue-req: 0, 
[2025-11-02 15:34:31 TP0] Decode batch, #running-req: 16, #token: 59230, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.74, #queue-req: 0, 
[2025-11-02 15:34:32 TP0] Decode batch, #running-req: 16, #token: 59870, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.14, #queue-req: 0, 
[2025-11-02 15:34:33 TP0] Decode batch, #running-req: 16, #token: 60510, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.31, #queue-req: 0, 
[2025-11-02 15:34:34 TP0] Decode batch, #running-req: 16, #token: 61150, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.22, #queue-req: 0, 
[2025-11-02 15:34:35 TP0] Decode batch, #running-req: 16, #token: 61790, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.82, #queue-req: 0, 
[2025-11-02 15:34:36 TP0] Decode batch, #running-req: 16, #token: 62430, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.35, #queue-req: 0, 
[2025-11-02 15:34:37 TP0] Decode batch, #running-req: 16, #token: 63070, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.98, #queue-req: 0, 
[2025-11-02 15:34:38 TP0] Decode batch, #running-req: 16, #token: 63710, token usage: 0.07, cuda graph: True, gen throughput (token/s): 603.45, #queue-req: 0, 
[2025-11-02 15:34:39] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:34:39] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:42012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:34:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-02 15:34:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-02 15:34:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-02 15:34:42 TP0] Decode batch, #running-req: 16, #token: 51551, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.51, #queue-req: 0, 
[2025-11-02 15:34:43 TP0] Decode batch, #running-req: 16, #token: 52191, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.70, #queue-req: 0, 
[2025-11-02 15:34:44 TP0] Decode batch, #running-req: 16, #token: 52831, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.06, #queue-req: 0, 
[2025-11-02 15:34:45 TP0] Decode batch, #running-req: 16, #token: 53471, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.92, #queue-req: 0, 
[2025-11-02 15:34:46 TP0] Decode batch, #running-req: 16, #token: 54111, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.92, #queue-req: 0, 
[2025-11-02 15:34:47 TP0] Decode batch, #running-req: 16, #token: 54751, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.02, #queue-req: 0, 
[2025-11-02 15:34:48 TP0] Decode batch, #running-req: 16, #token: 55391, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.01, #queue-req: 0, 
[2025-11-02 15:34:49 TP0] Decode batch, #running-req: 16, #token: 56031, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.54, #queue-req: 0, 
[2025-11-02 15:34:50 TP0] Decode batch, #running-req: 16, #token: 56671, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.93, #queue-req: 0, 
[2025-11-02 15:34:51 TP0] Decode batch, #running-req: 16, #token: 57311, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.16, #queue-req: 0, 
[2025-11-02 15:34:53 TP0] Decode batch, #running-req: 16, #token: 57951, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.92, #queue-req: 0, 
[2025-11-02 15:34:54 TP0] Decode batch, #running-req: 16, #token: 58591, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.68, #queue-req: 0, 
[2025-11-02 15:34:55 TP0] Decode batch, #running-req: 16, #token: 59231, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.64, #queue-req: 0, 
[2025-11-02 15:34:56 TP0] Decode batch, #running-req: 16, #token: 59871, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.98, #queue-req: 0, 
[2025-11-02 15:34:57 TP0] Decode batch, #running-req: 16, #token: 60511, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.31, #queue-req: 0, 
[2025-11-02 15:34:58 TP0] Decode batch, #running-req: 16, #token: 61151, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.68, #queue-req: 0, 
[2025-11-02 15:34:59 TP0] Decode batch, #running-req: 16, #token: 61791, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.90, #queue-req: 0, 
[2025-11-02 15:35:00 TP0] Decode batch, #running-req: 16, #token: 62431, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.10, #queue-req: 0, 
[2025-11-02 15:35:01 TP0] Decode batch, #running-req: 16, #token: 63071, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.00, #queue-req: 0, 
[2025-11-02 15:35:02 TP0] Decode batch, #running-req: 16, #token: 63711, token usage: 0.07, cuda graph: True, gen throughput (token/s): 607.85, #queue-req: 0, 
[2025-11-02 15:35:02] INFO:     127.0.0.1:49512 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-02 15:35:19] INFO:     127.0.0.1:53528 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-02 15:35:25] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:35:26 TP0] Decode batch, #running-req: 1, #token: 3224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 13.09, #queue-req: 0, 
[2025-11-02 15:35:27] INFO:     127.0.0.1:60740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:27] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:35:27] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:27] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:27 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:35:28 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 65.42, #queue-req: 0, 
[2025-11-02 15:35:29 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.90, #queue-req: 0, 
[2025-11-02 15:35:29 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-11-02 15:35:30 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:35:31 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:35:32 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:35:33 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:35:33 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:35:34 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.93, #queue-req: 0, 
[2025-11-02 15:35:35 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:36 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:37 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:38 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:35:38 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:35:39 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:40 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:41 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:35:42 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:43 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:35:43 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:35:44] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:44] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:44] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:44 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:35:44] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:35:44 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:35:44 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.01, #queue-req: 0, 
[2025-11-02 15:35:45 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-11-02 15:35:46 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:35:47 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:35:48 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:35:48 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:35:49 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:35:50 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:35:51 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:35:52 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:35:53 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:35:53 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:35:54 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:35:55 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:35:56 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:35:57 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:35:58 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:35:58 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:35:59 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:36:00 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:00] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:00] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:00] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:36:00] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:36:01 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.88, #queue-req: 0, 
[2025-11-02 15:36:02 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-11-02 15:36:03 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-11-02 15:36:04 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-11-02 15:36:04 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:36:05 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:36:06 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:36:07 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:36:08 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:36:08 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:36:09 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:36:10 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:36:11 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:36:12 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:36:13 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:13 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:14 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:15 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:16 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:17 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:36:17] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:17] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:17] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:36:17] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:36:18 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.97, #queue-req: 0, 
[2025-11-02 15:36:19 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.92, #queue-req: 0, 
[2025-11-02 15:36:19 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:36:20 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:36:21 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:36:22 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:36:23 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:36:24 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:24 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:36:25 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:36:26 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:36:27 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:28 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:28 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:29 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:30 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:36:31 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:32 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:36:33 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:36:33 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:36:34] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:34] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:34] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:36:34] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:36:34 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.29, #queue-req: 0, 
[2025-11-02 15:36:35 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-11-02 15:36:36 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-11-02 15:36:37 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:36:38 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:36:39 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:36:39 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:36:40 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:36:41 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:42 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:36:43 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:44 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:36:44 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:36:45 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:36:46 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:36:47 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:48 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:36:48 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:36:49 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:36:50 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:36:50] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:50] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:50] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:36:50] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:36:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:36:51 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.98, #queue-req: 0, 
[2025-11-02 15:36:52 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:36:53 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:36:54 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:36:54 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:36:55 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:56 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:36:57 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:36:58 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:36:59 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:36:59 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:00 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:37:01 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:02 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:37:03 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:37:03 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:37:04 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:37:05 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:37:06 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:07 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:37:07] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:07] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:07] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:37:07] INFO:     127.0.0.1:37998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:37:08 TP0] Decode batch, #running-req: 4, #token: 12920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0, 
[2025-11-02 15:37:09 TP0] Decode batch, #running-req: 4, #token: 13080, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-11-02 15:37:09 TP0] Decode batch, #running-req: 4, #token: 13240, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.89, #queue-req: 0, 
[2025-11-02 15:37:10 TP0] Decode batch, #running-req: 4, #token: 13400, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-11-02 15:37:11 TP0] Decode batch, #running-req: 4, #token: 13560, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-11-02 15:37:12 TP0] Decode batch, #running-req: 4, #token: 13720, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:37:13 TP0] Decode batch, #running-req: 4, #token: 13880, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:37:14 TP0] Decode batch, #running-req: 4, #token: 14040, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:37:14 TP0] Decode batch, #running-req: 4, #token: 14200, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:37:15 TP0] Decode batch, #running-req: 4, #token: 14360, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:37:16 TP0] Decode batch, #running-req: 4, #token: 14520, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:37:17 TP0] Decode batch, #running-req: 4, #token: 14680, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:37:18 TP0] Decode batch, #running-req: 4, #token: 14840, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:37:19 TP0] Decode batch, #running-req: 4, #token: 15000, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:19 TP0] Decode batch, #running-req: 4, #token: 15160, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:20 TP0] Decode batch, #running-req: 4, #token: 15320, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:37:21 TP0] Decode batch, #running-req: 4, #token: 15480, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:37:22 TP0] Decode batch, #running-req: 4, #token: 15640, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:37:23 TP0] Decode batch, #running-req: 4, #token: 15800, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:23 TP0] Decode batch, #running-req: 4, #token: 15960, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:37:24] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:24] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:24] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:37:24] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:37:24 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.00, #queue-req: 0, 
[2025-11-02 15:37:25 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-11-02 15:37:26 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:37:27 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:37:28 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:37:29 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:37:29 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:37:30 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:37:31 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:37:32 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:37:33 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:37:34 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:37:34 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:37:35 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:37:36 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:37:37 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:37:38 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:37:39 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:37:39 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:37:40 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:37:40] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:40] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:37:40] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:40] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:40 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:37:41 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 161.03, #queue-req: 0, 
[2025-11-02 15:37:42 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-11-02 15:37:43 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.00, #queue-req: 0, 
[2025-11-02 15:37:44 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-11-02 15:37:44 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:37:45 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-11-02 15:37:46 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:37:47 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:37:48 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:37:49 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:37:49 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:37:50 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:37:51 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:37:52 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:37:53 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:37:54 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:37:54 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:37:55 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:37:56 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:37:57 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:37:57] INFO:     127.0.0.1:46066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:57] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:57] INFO:     127.0.0.1:46086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:57 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:37:57] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:37:57 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:37:58 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.73, #queue-req: 0, 
[2025-11-02 15:37:59 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:38:00 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:38:00 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:38:01 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:38:02 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:38:03 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:38:04 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:38:04 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:38:05 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:38:06 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:38:07 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:38:08 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:38:09 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:38:09 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:38:10 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:38:11 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:38:12 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:38:13 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:38:14 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:38:14] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:14] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:14] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:38:14] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:14 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:38:15 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.95, #queue-req: 0, 
[2025-11-02 15:38:15 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:38:16 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:38:17 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:38:18 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:38:19 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:38:20 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:38:20 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:38:21 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:38:22 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:38:23 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:38:24 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:38:24 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:38:25 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:38:26 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:38:27 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:38:28 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:38:29 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:38:29 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:38:30 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:38:30] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:30] INFO:     127.0.0.1:45090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:30] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:30 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:38:30] INFO:     127.0.0.1:45100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:31 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:38:31 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0, 
[2025-11-02 15:38:32 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.09, #queue-req: 0, 
[2025-11-02 15:38:33 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 194.02, #queue-req: 0, 
[2025-11-02 15:38:34 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.99, #queue-req: 0, 
[2025-11-02 15:38:35 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.98, #queue-req: 0, 
[2025-11-02 15:38:35 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.88, #queue-req: 0, 
[2025-11-02 15:38:36 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-11-02 15:38:37 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-11-02 15:38:38 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:38:39 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:38:39 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:38:40 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:38:41 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:38:42 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:38:43 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:38:44 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:38:44 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:38:45 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:38:46 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:38:47 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:38:47] INFO:     127.0.0.1:41090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:47] INFO:     127.0.0.1:41106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:47] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:38:47] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:38:47 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:38:48 TP0] Decode batch, #running-req: 4, #token: 12921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.93, #queue-req: 0, 
[2025-11-02 15:38:49 TP0] Decode batch, #running-req: 4, #token: 13081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:38:50 TP0] Decode batch, #running-req: 4, #token: 13241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:38:50 TP0] Decode batch, #running-req: 4, #token: 13401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:38:51 TP0] Decode batch, #running-req: 4, #token: 13561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:38:52 TP0] Decode batch, #running-req: 4, #token: 13721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:38:53 TP0] Decode batch, #running-req: 4, #token: 13881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:38:54 TP0] Decode batch, #running-req: 4, #token: 14041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:38:55 TP0] Decode batch, #running-req: 4, #token: 14201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:38:55 TP0] Decode batch, #running-req: 4, #token: 14361, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:38:56 TP0] Decode batch, #running-req: 4, #token: 14521, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:38:57 TP0] Decode batch, #running-req: 4, #token: 14681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:38:58 TP0] Decode batch, #running-req: 4, #token: 14841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:38:59 TP0] Decode batch, #running-req: 4, #token: 15001, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:38:59 TP0] Decode batch, #running-req: 4, #token: 15161, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:00 TP0] Decode batch, #running-req: 4, #token: 15321, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:39:01 TP0] Decode batch, #running-req: 4, #token: 15481, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:39:02 TP0] Decode batch, #running-req: 4, #token: 15641, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:39:03 TP0] Decode batch, #running-req: 4, #token: 15801, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:39:04 TP0] Decode batch, #running-req: 4, #token: 15961, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:39:04] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:04] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:04] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:39:04] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:39:05 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.81, #queue-req: 0, 
[2025-11-02 15:39:05 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.86, #queue-req: 0, 
[2025-11-02 15:39:06 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-11-02 15:39:07 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:39:08 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:39:09 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:39:10 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:39:10 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:39:11 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:39:12 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:39:13 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:39:14 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:39:15 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:39:15 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:39:16 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:17 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:39:18 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:19 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:19 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:20 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:21] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:21] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:39:21] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:21] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:21 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:39:21 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.24, #queue-req: 0, 
[2025-11-02 15:39:22 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:39:23 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:39:24 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:39:25 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:39:25 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:39:26 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:39:27 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:39:28 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:39:29 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:39:30 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:39:30 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:39:31 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:39:32 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:39:33 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:39:34 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:35 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:39:35 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:36 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:39:37 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:37] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:37] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:37] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:37 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:39:37] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:37 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:39:38 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.77, #queue-req: 0, 
[2025-11-02 15:39:39 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:39:40 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:39:40 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:39:41 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:39:42 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:39:43 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:39:44 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:39:45 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:39:45 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:39:46 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:47 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:39:48 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:39:49 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:39:50 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:39:50 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:39:51 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:39:52 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:53 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:54 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:39:54] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:54] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:54] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:54 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:39:54] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:39:54 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:39:55 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.63, #queue-req: 0, 
[2025-11-02 15:39:56 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:39:56 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:39:57 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:39:58 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:39:59 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:40:00 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:40:00 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:40:01 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:40:02 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:40:03 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:40:04 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:05 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:05 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:40:06 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:07 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:08 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:09 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:40:10 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:40:10 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:11] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:11] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:40:11] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:11] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:11 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:40:11 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.97, #queue-req: 0, 
[2025-11-02 15:40:12 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-11-02 15:40:13 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:40:14 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:40:15 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:40:16 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:40:16 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:40:17 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:40:18 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:40:19 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:40:20 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:40:20 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:40:21 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:40:22 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:40:23 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:40:24 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:40:25 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:40:25 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:40:26 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:40:27 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:40:27] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:27] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:40:27] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:27] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:27 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:40:28 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.87, #queue-req: 0, 
[2025-11-02 15:40:29 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:40:30 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:40:31 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:40:31 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:32 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:40:33 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:34 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:40:35 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:40:36 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:36 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:40:37 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:38 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-02 15:40:39 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:40:40 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:40:40 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:40:41 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:40:42 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:40:43 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-02 15:40:44 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:40:44] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:44] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:40:44] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:44] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:40:44 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:40:45 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.91, #queue-req: 0, 
[2025-11-02 15:40:46 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:40:46 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:40:47 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:40:48 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:40:49 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:40:50 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:40:51 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:40:51 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:52 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:53 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:40:54 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:40:55 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:40:56 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:40:56 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:40:57 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:40:58 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:40:59 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:41:00 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:41:01 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:41:01] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:01] INFO:     127.0.0.1:47394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:41:01] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:01] INFO:     127.0.0.1:47406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:01 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:41:02 TP0] Decode batch, #running-req: 4, #token: 12921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.44, #queue-req: 0, 
[2025-11-02 15:41:02 TP0] Decode batch, #running-req: 4, #token: 13081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-11-02 15:41:03 TP0] Decode batch, #running-req: 4, #token: 13241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:41:04 TP0] Decode batch, #running-req: 4, #token: 13401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:41:05 TP0] Decode batch, #running-req: 4, #token: 13561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-11-02 15:41:06 TP0] Decode batch, #running-req: 4, #token: 13721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:41:06 TP0] Decode batch, #running-req: 4, #token: 13881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:41:07 TP0] Decode batch, #running-req: 4, #token: 14041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:41:08 TP0] Decode batch, #running-req: 4, #token: 14201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:41:09 TP0] Decode batch, #running-req: 4, #token: 14361, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:41:10 TP0] Decode batch, #running-req: 4, #token: 14521, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:41:11 TP0] Decode batch, #running-req: 4, #token: 14681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:41:11 TP0] Decode batch, #running-req: 4, #token: 14841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:41:12 TP0] Decode batch, #running-req: 4, #token: 15001, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:41:13 TP0] Decode batch, #running-req: 4, #token: 15161, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:41:14 TP0] Decode batch, #running-req: 4, #token: 15321, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:41:15 TP0] Decode batch, #running-req: 4, #token: 15481, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:41:16 TP0] Decode batch, #running-req: 4, #token: 15641, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:41:16 TP0] Decode batch, #running-req: 4, #token: 15801, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:41:17 TP0] Decode batch, #running-req: 4, #token: 15961, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:41:17] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:17] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:17] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:41:17] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:17 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:41:18 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.98, #queue-req: 0, 
[2025-11-02 15:41:19 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.99, #queue-req: 0, 
[2025-11-02 15:41:20 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-11-02 15:41:21 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-11-02 15:41:21 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.87, #queue-req: 0, 
[2025-11-02 15:41:22 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.85, #queue-req: 0, 
[2025-11-02 15:41:23 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.83, #queue-req: 0, 
[2025-11-02 15:41:24 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-11-02 15:41:25 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-11-02 15:41:26 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:41:26 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:41:27 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:41:28 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:41:29 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:41:30 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-11-02 15:41:31 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:41:31 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:41:32 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:41:33 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:41:34 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:41:34] INFO:     127.0.0.1:51506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:34] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:34] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:41:34] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:41:35 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.23, #queue-req: 0, 
[2025-11-02 15:41:36 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:41:37 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:41:37 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:41:38 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:41:39 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:41:40 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:41:41 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:41:41 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:41:42 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:41:43 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:41:44 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:41:45 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:41:46 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:41:46 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:41:47 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-02 15:41:48 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:41:49 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-02 15:41:50 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:41:51 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:41:51] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:51] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:51] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:41:51] INFO:     127.0.0.1:43054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:41:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:41:52 TP0] Decode batch, #running-req: 4, #token: 12921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.89, #queue-req: 0, 
[2025-11-02 15:41:52 TP0] Decode batch, #running-req: 4, #token: 13081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-11-02 15:41:53 TP0] Decode batch, #running-req: 4, #token: 13241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:41:54 TP0] Decode batch, #running-req: 4, #token: 13401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:41:55 TP0] Decode batch, #running-req: 4, #token: 13561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:41:56 TP0] Decode batch, #running-req: 4, #token: 13721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:41:57 TP0] Decode batch, #running-req: 4, #token: 13881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:41:57 TP0] Decode batch, #running-req: 4, #token: 14041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:41:58 TP0] Decode batch, #running-req: 4, #token: 14201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:41:59 TP0] Decode batch, #running-req: 4, #token: 14361, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:42:00 TP0] Decode batch, #running-req: 4, #token: 14521, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:01 TP0] Decode batch, #running-req: 4, #token: 14681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:42:01 TP0] Decode batch, #running-req: 4, #token: 14841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:02 TP0] Decode batch, #running-req: 4, #token: 15001, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:42:03 TP0] Decode batch, #running-req: 4, #token: 15161, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:04 TP0] Decode batch, #running-req: 4, #token: 15321, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:05 TP0] Decode batch, #running-req: 4, #token: 15481, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:42:06 TP0] Decode batch, #running-req: 4, #token: 15641, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:42:06 TP0] Decode batch, #running-req: 4, #token: 15801, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:07 TP0] Decode batch, #running-req: 4, #token: 15961, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:42:07] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:07] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:07] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:42:07] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:08 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:42:08 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.25, #queue-req: 0, 
[2025-11-02 15:42:09 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-11-02 15:42:10 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:42:11 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:42:12 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:42:12 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:42:13 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:42:14 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:42:15 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:42:16 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:42:17 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:42:17 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:42:18 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:42:19 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:42:20 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:42:21 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:22 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:22 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:42:23 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:42:24 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:42:24] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:24] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:24] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:42:24] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-02 15:42:25 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.85, #queue-req: 0, 
[2025-11-02 15:42:26 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-11-02 15:42:27 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:42:27 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-11-02 15:42:28 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:42:29 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:42:30 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:42:31 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:42:32 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:42:32 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:42:33 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:42:34 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:42:35 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:42:36 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:42:37 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:42:37 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:42:38 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:42:39 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:42:40 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:42:41 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:42:41] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:41] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:42:41] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:41] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:42:42 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.87, #queue-req: 0, 
[2025-11-02 15:42:43 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:42:43 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:42:44 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:42:45 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-02 15:42:46 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-02 15:42:47 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-02 15:42:47 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-02 15:42:48 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-02 15:42:49 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-02 15:42:50 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-02 15:42:51 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-02 15:42:52 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-02 15:42:52 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-02 15:42:53 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-02 15:42:54 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-02 15:42:55 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-02 15:42:56 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-02 15:42:57 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-02 15:42:57 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-02 15:42:58] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:58] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:58] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:42:58] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:42:58 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:42:58 TP0] Decode batch, #running-req: 4, #token: 12921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.11, #queue-req: 0, 
[2025-11-02 15:42:59 TP0] Decode batch, #running-req: 4, #token: 13081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:43:00 TP0] Decode batch, #running-req: 4, #token: 13241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-11-02 15:43:01 TP0] Decode batch, #running-req: 4, #token: 13401, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-02 15:43:02 TP0] Decode batch, #running-req: 4, #token: 13561, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:43:03 TP0] Decode batch, #running-req: 4, #token: 13721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:43:03 TP0] Decode batch, #running-req: 4, #token: 13881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:04 TP0] Decode batch, #running-req: 4, #token: 14041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:43:05 TP0] Decode batch, #running-req: 4, #token: 14201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:06 TP0] Decode batch, #running-req: 4, #token: 14361, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:43:07 TP0] Decode batch, #running-req: 4, #token: 14521, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:43:07 TP0] Decode batch, #running-req: 4, #token: 14681, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:43:08 TP0] Decode batch, #running-req: 4, #token: 14841, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:43:09 TP0] Decode batch, #running-req: 4, #token: 15001, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:10 TP0] Decode batch, #running-req: 4, #token: 15161, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:43:11 TP0] Decode batch, #running-req: 4, #token: 15321, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-02 15:43:12 TP0] Decode batch, #running-req: 4, #token: 15481, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:43:12 TP0] Decode batch, #running-req: 4, #token: 15641, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:13 TP0] Decode batch, #running-req: 4, #token: 15801, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-02 15:43:14 TP0] Decode batch, #running-req: 4, #token: 15961, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:43:14] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:14] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:43:14] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:14] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:14 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:43:15 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.75, #queue-req: 0, 
[2025-11-02 15:43:16 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:43:17 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:43:18 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-11-02 15:43:18 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:43:19 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:43:20 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:21 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:43:22 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:43:23 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:43:23 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:43:24 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:43:25 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-02 15:43:26 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:43:27 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:43:28 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:43:28 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:29 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-02 15:43:30 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:43:31 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-02 15:43:31] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:31] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:31] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:43:31] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:31 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:43:32 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.74, #queue-req: 0, 
[2025-11-02 15:43:33 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:43:33 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:43:34 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:35 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:36 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:43:37 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:43:38 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:43:38 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:43:39 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:40 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:41 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-02 15:43:42 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:43 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:43 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-02 15:43:44 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:45 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-02 15:43:46 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-02 15:43:47 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-02 15:43:48 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-02 15:43:48] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:48] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:43:48] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:48] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:43:48 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:43:49 TP0] Decode batch, #running-req: 4, #token: 12922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.73, #queue-req: 0, 
[2025-11-02 15:43:49 TP0] Decode batch, #running-req: 4, #token: 13082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-02 15:43:50 TP0] Decode batch, #running-req: 4, #token: 13242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-11-02 15:43:51 TP0] Decode batch, #running-req: 4, #token: 13402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:43:52 TP0] Decode batch, #running-req: 4, #token: 13562, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-02 15:43:53 TP0] Decode batch, #running-req: 4, #token: 13722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-02 15:43:53 TP0] Decode batch, #running-req: 4, #token: 13882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:54 TP0] Decode batch, #running-req: 4, #token: 14042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:55 TP0] Decode batch, #running-req: 4, #token: 14202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:43:56 TP0] Decode batch, #running-req: 4, #token: 14362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:43:57 TP0] Decode batch, #running-req: 4, #token: 14522, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:43:58 TP0] Decode batch, #running-req: 4, #token: 14682, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-02 15:43:58 TP0] Decode batch, #running-req: 4, #token: 14842, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:43:59 TP0] Decode batch, #running-req: 4, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:00 TP0] Decode batch, #running-req: 4, #token: 15162, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:44:01 TP0] Decode batch, #running-req: 4, #token: 15322, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:02 TP0] Decode batch, #running-req: 4, #token: 15482, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:44:03 TP0] Decode batch, #running-req: 4, #token: 15642, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-02 15:44:03 TP0] Decode batch, #running-req: 4, #token: 15802, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:44:04 TP0] Decode batch, #running-req: 4, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-02 15:44:04] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:04] INFO:     127.0.0.1:38050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:44:04] INFO:     127.0.0.1:38066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:04] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:05 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-02 15:44:05 TP0] Decode batch, #running-req: 4, #token: 12920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.84, #queue-req: 0, 
[2025-11-02 15:44:06 TP0] Decode batch, #running-req: 4, #token: 13080, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-02 15:44:07 TP0] Decode batch, #running-req: 4, #token: 13240, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-02 15:44:08 TP0] Decode batch, #running-req: 4, #token: 13400, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:44:09 TP0] Decode batch, #running-req: 4, #token: 13560, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-11-02 15:44:09 TP0] Decode batch, #running-req: 4, #token: 13720, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-02 15:44:10 TP0] Decode batch, #running-req: 4, #token: 13880, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:44:11 TP0] Decode batch, #running-req: 4, #token: 14040, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-02 15:44:12 TP0] Decode batch, #running-req: 4, #token: 14200, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:44:13 TP0] Decode batch, #running-req: 4, #token: 14360, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:44:13 TP0] Decode batch, #running-req: 4, #token: 14520, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-02 15:44:14 TP0] Decode batch, #running-req: 4, #token: 14680, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:44:15 TP0] Decode batch, #running-req: 4, #token: 14840, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-02 15:44:16 TP0] Decode batch, #running-req: 4, #token: 15000, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-02 15:44:17 TP0] Decode batch, #running-req: 4, #token: 15160, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:18 TP0] Decode batch, #running-req: 4, #token: 15320, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-02 15:44:18 TP0] Decode batch, #running-req: 4, #token: 15480, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:19 TP0] Decode batch, #running-req: 4, #token: 15640, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:20 TP0] Decode batch, #running-req: 4, #token: 15800, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-02 15:44:21 TP0] Decode batch, #running-req: 4, #token: 15960, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-02 15:44:21] INFO:     127.0.0.1:41822 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-02 15:44:38] INFO:     127.0.0.1:53154 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-02 15:44:44] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:44:45 TP0] Decode batch, #running-req: 1, #token: 3232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.95, #queue-req: 0, 
[2025-11-02 15:44:46] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:44:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:44:47 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.08, #queue-req: 0, 
[2025-11-02 15:44:47 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:44:48 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:44:49 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:44:50 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:44:51 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:44:51 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:44:52 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:44:53 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:44:54 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:44:55 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:44:56 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:44:56 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:44:57 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:44:58 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:44:59 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:00 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:00 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:01 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:02 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:45:02] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:45:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:45:03 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 15:45:04 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:45:05 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:05 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:06 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:07 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:08 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:09 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:10 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:10 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:11 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:12 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:13 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:14 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:14 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:15 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:16 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:17 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:18 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:19 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:19] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:45:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:45:19 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 15:45:20 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:45:21 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:45:22 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:23 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:45:24 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:24 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:25 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:45:26 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:45:27 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:28 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:45:28 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:29 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:30 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:31 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:32 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:33 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:33 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:45:34 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:35 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:35] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:45:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:45:36 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:45:37 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:37 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:38 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:45:39 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:40 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:41 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:42 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:42 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:43 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:45:44 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:45 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:46 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:47 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:47 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:45:48 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:45:49 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:50 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:45:51 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:51 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:45:51] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:45:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:45:52 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 15:45:53 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:54 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:55 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:45:56 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:56 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:57 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:45:58 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:45:59 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:00 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:01 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:01 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:02 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:46:03 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:04 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:05 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:05 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:06 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:07 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:08 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:08] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:46:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:46:09 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:46:10 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:46:10 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:46:11 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:46:12 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:46:13 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:46:14 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:46:15 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:46:15 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:46:16 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.76, #queue-req: 0, 
[2025-11-02 15:46:17 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:46:18 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:19 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:19 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:20 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:21 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:22 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:23 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:24 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:24 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:24] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:46:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:46:25 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:46:26 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:46:27 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:46:28 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:46:29 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:46:29 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:46:30 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:46:31 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:46:32 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:46:33 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:46:33 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:34 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:35 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:36 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:37 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:38 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:46:38 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:39 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:40 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:46:41 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:41] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:46:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:46:42 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:46:43 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:43 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:46:44 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:46:45 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:46:46 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:47 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:47 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:48 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:49 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.97, #queue-req: 0, 
[2025-11-02 15:46:50 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:46:51 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:46:52 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:52 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:46:53 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:54 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:46:55 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:46:56 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:56 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:57 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:46:57] INFO:     127.0.0.1:47616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:46:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:46:58 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.91, #queue-req: 0, 
[2025-11-02 15:46:59 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:00 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:01 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:01 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:02 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:03 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:04 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:05 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:06 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:06 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:47:07 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:47:08 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:47:09 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:47:10 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:47:10 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:47:11 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:47:12 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:47:13 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:47:14 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:47:14] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:47:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:47:15 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 15:47:15 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:47:16 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:47:17 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:18 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:19 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:20 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:20 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:21 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:22 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:23 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:47:24 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:24 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:25 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:47:26 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:27 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:28 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:29 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:29 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:30 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:30] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:47:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:47:31 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 15:47:32 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:47:33 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:34 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:47:34 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:35 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:36 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:47:37 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:38 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:38 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:39 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:40 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:47:41 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:42 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:43 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:43 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:44 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:45 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:47:46 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:47 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:47:47] INFO:     127.0.0.1:36026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:47:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:47:48 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:47:48 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:47:49 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:47:50 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:47:51 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:47:52 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:47:52 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:47:53 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:47:54 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:47:55 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.36, #queue-req: 0, 
[2025-11-02 15:47:56 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:47:57 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:57 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:47:58 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:47:59 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:48:00 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:48:01 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:01 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:02 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:03 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:03] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:48:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:48:04 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 15:48:05 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:06 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:06 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:07 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:08 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:09 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:10 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:11 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:11 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:12 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:13 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:14 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:15 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:15 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:16 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:17 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:18 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:19 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:20 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:20] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:48:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:48:20 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 15:48:21 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:48:22 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:48:23 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:24 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:48:25 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:25 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:26 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:27 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:28 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:48:29 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:29 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:30 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:31 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:32 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:33 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:34 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:34 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:35 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:36 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:48:36] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:48:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:48:37 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-11-02 15:48:38 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:48:39 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:39 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:40 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:41 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:42 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:43 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:43 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:44 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:45 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:46 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:47 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:48 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:48 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:49 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:48:50 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:51 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:48:52 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:48:52 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:48:52] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:48:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:48:53 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 15:48:54 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:48:55 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:48:56 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:48:57 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:48:57 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:48:58 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:48:59 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:00 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:01 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:02 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:02 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:03 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:04 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:05 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:06 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:06 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:07 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:08 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:09 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:09] INFO:     127.0.0.1:45782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:49:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:49:10 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.88, #queue-req: 0, 
[2025-11-02 15:49:11 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:49:11 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:49:12 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:49:13 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:49:14 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:49:15 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:49:16 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:49:16 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:49:17 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:49:18 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:49:19 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:49:20 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:20 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:21 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:22 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:23 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:24 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:25 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:25 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:25] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:49:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:49:26 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 15:49:27 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:28 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:29 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:30 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:30 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:31 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:32 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:33 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:34 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:34 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:35 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:36 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:37 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:38 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:39 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:39 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:49:40 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:49:41 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:42 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:49:42] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:49:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:49:43 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:49:44 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:49:44 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:49:45 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:46 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:49:47 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:48 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:49:48 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:49 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:50 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:51 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:49:52 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:49:53 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:53 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:49:54 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:55 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:56 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:57 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:49:57 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:49:58 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:49:58] INFO:     127.0.0.1:37948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:49:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:49:59 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:50:00 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:50:01 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:50:02 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:50:02 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:50:03 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:04 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:05 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:06 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:07 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:50:07 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:50:08 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:50:09 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:50:10 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:50:11 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:50:11 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:50:12 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:13 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:14 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:50:15 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:15] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:50:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:50:16 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 15:50:16 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:50:17 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:50:18 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:19 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:20 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:50:21 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:21 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:50:22 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:23 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:24 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:25 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:25 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:50:26 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:27 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:28 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:29 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:50:30 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:30 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:50:31 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:31] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:50:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:50:32 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-02 15:50:33 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:50:34 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:50:35 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:50:35 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:36 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:50:37 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:38 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:39 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:50:39 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:50:40 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:41 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:42 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:43 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:50:44 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:44 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:50:45 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:46 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:50:47 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:50:48 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:50:48] INFO:     127.0.0.1:56134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:50:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:50:49 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:50:49 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:50:50 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:50:51 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:52 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:50:53 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:53 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:50:54 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:50:55 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:56 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:50:57 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:50:58 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:50:58 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:50:59 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:51:00 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:01 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:51:02 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:02 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:03 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:04 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:04] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:51:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:51:05 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.44, #queue-req: 0, 
[2025-11-02 15:51:06 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:51:07 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:51:07 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:51:08 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:09 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:51:10 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:51:11 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:12 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:12 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:13 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:14 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:15 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:16 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:51:16 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:51:17 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:18 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:19 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:20 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:21 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:21] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:51:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:51:21 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:51:22 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:23 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:24 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:25 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:26 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:26 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:27 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:28 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:29 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:30 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:51:30 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:31 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:32 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:51:33 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:34 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:35 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:51:35 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:36 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:37 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:37] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:51:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:51:38 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 15:51:39 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:40 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:40 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:51:41 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:42 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:51:43 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:44 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:44 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:45 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:51:46 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:47 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:51:48 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:51:49 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:49 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:50 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:51 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:52 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:53 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:51:53 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:51:53] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:51:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:51:54 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 15:51:55 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:51:56 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:51:57 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:51:58 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:58 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:51:59 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:52:00 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:52:01 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:02 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:52:03 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:03 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:04 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:05 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:06 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:07 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:07 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:52:08 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:52:09 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:10 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:52:10] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:52:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:52:11 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.44, #queue-req: 0, 
[2025-11-02 15:52:12 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:52:12 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:52:13 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:14 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:15 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:16 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:17 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:52:17 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:52:18 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:52:19 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:52:20 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:21 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:21 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:22 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:23 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:24 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:25 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:26 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:26 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:26] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:52:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:52:27 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 15:52:28 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:29 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:30 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:52:31 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:52:31 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:52:32 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:52:33 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:52:34 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:52:35 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:52:35 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:52:36 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:52:37 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:52:38 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:52:39 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:52:40 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:52:40 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:52:41 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:52:42 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:52:43 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:52:43] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:52:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:52:44 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 15:52:45 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:52:45 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:52:46 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:52:47 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:52:48 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:52:49 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:49 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:52:50 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:52:51 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:52:52 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:53 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:54 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:52:54 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:55 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:56 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:57 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:58 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:52:58 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:52:59 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:52:59] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:52:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:53:00 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 15:53:01 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:53:02 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:03 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:03 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:53:04 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:05 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:06 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:53:07 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:53:08 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:53:08 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:09 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:10 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:11 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:12 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:12 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:13 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:14 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:15 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:16 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:16] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:53:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:53:17 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:53:17 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:53:18 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:19 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:20 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:53:21 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:22 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:22 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:23 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:24 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:25 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:26 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:26 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:27 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:28 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:29 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:30 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:31 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:31 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:53:32 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:53:32] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:53:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:53:33 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:53:34 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:53:35 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:36 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:53:36 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:53:37 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:38 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:53:39 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:53:40 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:53:40 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:53:41 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:42 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:43 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:44 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:45 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:45 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:53:46 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:53:47 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:48 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:49 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:53:49] INFO:     127.0.0.1:40756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:53:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:53:50 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:53:50 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:53:51 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:53:52 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:53:53 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:54 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:54 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:53:55 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:53:56 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:57 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:58 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:59 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:53:59 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:00 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:01 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:02 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:03 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:54:03 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:54:04 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:05 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:54:05] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:54:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:54:06 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 15:54:07 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:54:08 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:54:08 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:09 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:54:10 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:11 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:12 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:13 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:13 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:14 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:54:15 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:16 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:54:17 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:17 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:18 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:19 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:20 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:21 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:54:22 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:22] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:54:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:54:22 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.46, #queue-req: 0, 
[2025-11-02 15:54:23 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-02 15:54:24 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:54:25 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:54:26 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:54:27 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:54:27 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-02 15:54:28 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:54:29 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-02 15:54:30 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-02 15:54:31 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:54:31 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:54:32 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:54:33 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:54:34 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:54:35 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:54:36 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:54:36 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:54:37 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:54:38 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:54:38] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:54:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:54:39 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:54:40 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:54:41 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:54:41 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:54:42 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:43 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:54:44 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:45 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:45 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:46 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:47 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:54:48 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:54:49 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:50 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:54:50 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:54:51 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:54:52 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:54:53 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:54:54 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:54:54 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:54:54] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:54:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:54:55 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 15:54:56 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:54:57 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:58 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:54:59 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:54:59 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:55:00 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:55:01 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:02 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:55:03 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:55:04 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:55:04 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:05 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:06 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:07 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:08 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:08 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:09 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:10 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:11 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:11] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:55:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:55:12 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-11-02 15:55:13 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:13 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:14 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:15 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:16 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:17 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:18 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:18 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:19 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:20 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:21 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:22 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:22 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:23 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:24 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:25 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:26 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:27 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:27 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:27] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:55:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:55:28 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:55:29 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-02 15:55:30 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:55:31 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:55:32 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:55:32 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:55:33 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:55:34 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:55:35 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:55:36 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:55:36 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:55:37 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:55:38 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:55:39 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:55:40 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:55:41 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:55:41 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:55:42 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:55:43 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:55:44 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:55:44] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:55:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:55:45 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 15:55:46 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:55:46 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:55:47 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:48 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:55:49 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:50 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:50 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:51 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:55:52 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:53 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:54 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:55 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:55:55 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:56 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:57 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:58 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:55:59 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:55:59 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:56:00 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:56:00] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:56:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:56:01 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:56:02 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:56:03 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:56:04 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:56:04 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:56:05 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:56:06 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:07 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:08 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:09 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:09 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:10 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:11 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:12 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:13 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:56:13 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:14 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:56:15 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:16 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:17 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:56:17] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:56:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:56:18 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 15:56:18 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:56:19 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:56:20 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:21 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:22 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:23 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:23 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:24 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:25 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:26 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:27 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:27 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:28 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:29 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:30 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:31 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:32 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:32 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:33 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:33] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:56:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:56:34 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:56:35 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:56:36 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:56:37 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:56:37 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:56:38 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:56:39 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:40 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:41 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:41 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:56:42 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:43 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:44 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:56:45 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:46 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:46 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:47 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:48 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:56:49 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:56:50 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:56:50] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:56:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:56:51 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.93, #queue-req: 0, 
[2025-11-02 15:56:51 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:56:52 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:56:53 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:56:54 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:56:55 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:56:55 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:56:56 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:56:57 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:56:58 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:56:59 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:57:00 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:57:00 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:57:01 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:57:02 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:57:03 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:57:04 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:57:04 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:57:05 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:57:06 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:06] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:57:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:57:07 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 15:57:08 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:57:09 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:57:09 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:10 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:57:11 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:12 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:13 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:14 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:57:14 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:15 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:16 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:17 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:57:18 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:57:18 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:19 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:20 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:21 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:22 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:23 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:23] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:57:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:57:23 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 15:57:24 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:57:25 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 15:57:26 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:57:27 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:57:28 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 15:57:28 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 15:57:29 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:30 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:31 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:32 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:32 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:33 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:34 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:35 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:36 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:37 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:57:37 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:38 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:57:39 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:57:39] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:57:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:57:40 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 15:57:41 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:57:42 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:42 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:43 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:57:44 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:57:45 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:57:46 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:57:46 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:57:47 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:57:48 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:57:49 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:57:50 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:57:51 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:57:51 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:57:52 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:57:53 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:57:54 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:57:55 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:57:55 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:57:55] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:57:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:57:56 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-02 15:57:57 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:57:58 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:57:59 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:00 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:58:00 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:01 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:58:02 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:03 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:04 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:05 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:58:05 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:06 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:07 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:58:08 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:09 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:58:09 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:10 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:11 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:12 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:12] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:58:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:58:13 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-02 15:58:14 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:14 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:15 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:16 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:17 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:18 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:19 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:19 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:20 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:21 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:22 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:23 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:23 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:24 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:25 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:26 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:27 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:28 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:28 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:28] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:58:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:58:29 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-02 15:58:30 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:31 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:32 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:58:33 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:33 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:34 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:58:35 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:36 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:37 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:37 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:58:38 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:39 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:40 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:41 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:42 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:42 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:43 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:44 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:45 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:58:45] INFO:     127.0.0.1:44946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:58:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:58:46 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 15:58:47 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:58:47 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:58:48 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:58:49 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:50 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:51 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:51 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:58:52 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:58:53 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:54 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:55 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:58:56 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:56 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:58:57 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:58 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:58:59 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:59:00 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:59:00 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 15:59:01 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 15:59:01] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:59:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:59:02 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.96, #queue-req: 0, 
[2025-11-02 15:59:03 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 15:59:04 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 15:59:05 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:59:05 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:59:06 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:59:07 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:08 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 15:59:09 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:10 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:10 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:11 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:12 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:13 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:59:14 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:15 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:15 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:16 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:59:17 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:59:18 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:18] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:59:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:59:19 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 15:59:19 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:59:20 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:59:21 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:22 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 15:59:23 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:24 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:24 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 15:59:25 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:26 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:27 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:28 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:59:29 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:29 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:30 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:31 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:32 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:33 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:33 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 15:59:34 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 15:59:34] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:59:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:59:35 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 15:59:36 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-02 15:59:37 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.03, #queue-req: 0, 
[2025-11-02 15:59:38 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-02 15:59:38 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-02 15:59:39 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-02 15:59:40 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:59:41 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-02 15:59:42 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-02 15:59:42 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:59:43 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-02 15:59:44 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:59:45 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-02 15:59:46 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:59:47 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:59:47 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:59:48 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:59:49 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 15:59:50 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 15:59:51 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 15:59:51] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 15:59:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 15:59:52 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 15:59:52 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:53 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:54 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:55 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:56 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:56 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 15:59:57 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 15:59:58 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 15:59:59 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:00 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:01 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:01 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:02 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:00:03 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:00:04 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:05 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:05 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:06 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:07 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:00:07] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:00:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:00:08 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:00:09 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:00:10 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:00:10 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:11 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:12 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:13 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:14 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:00:15 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:15 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:16 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:17 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:18 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:19 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:19 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:20 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:21 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:22 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:23 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:24 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:24] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:00:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:00:24 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:00:25 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:26 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:27 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:28 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:29 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:29 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:30 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:31 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:32 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:33 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:33 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:34 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:35 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:00:36 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:37 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:38 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:38 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:39 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:00:40 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:40] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:00:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:00:41 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:00:42 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:43 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:43 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:44 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:00:45 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:00:46 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:00:47 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:00:48 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:00:48 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:49 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:00:50 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:51 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:00:52 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:00:52 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:53 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:54 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:55 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:00:56 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:00:57 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:00:57] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:00:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:00:57 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:00:58 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:00:59 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:01:00 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:01 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:02 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:02 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:03 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:04 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:05 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:06 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:01:06 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:07 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:08 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:09 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:10 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:11 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:11 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:12 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:13 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:13] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:01:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:01:14 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:01:15 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:01:16 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:01:16 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:17 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:18 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:01:19 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:20 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:20 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:21 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:22 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:01:23 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:24 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:25 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:25 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:26 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:27 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:28 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:29 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:29 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:01:29] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:01:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:01:30 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:01:31 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:32 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:33 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:34 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:01:34 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:35 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:36 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:37 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:38 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:39 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:39 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:40 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:41 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:42 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:43 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:43 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:44 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:45 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:01:46 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:01:46] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:01:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:01:47 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:01:48 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:48 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:01:49 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:50 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:51 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:01:52 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:53 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:53 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:01:54 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:01:55 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:56 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:01:57 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:57 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:01:58 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:01:59 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:02:00 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:01 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:02 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:02 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:02] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:02:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:02:03 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:02:04 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:02:05 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:06 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:07 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:07 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:08 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:09 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:02:10 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:11 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:11 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:12 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:13 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:14 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:15 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:16 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:16 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:02:17 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:18 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:19 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:02:19] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:02:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:02:20 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.72, #queue-req: 0, 
[2025-11-02 16:02:21 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:02:21 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:02:22 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:23 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:24 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:25 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:26 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:26 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:27 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:28 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:29 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:30 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:30 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:31 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:32 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:33 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:02:34 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:02:35 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:02:35 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:35] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:02:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:02:36 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:02:37 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:38 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:39 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:40 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:40 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:41 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:42 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:02:43 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:44 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:44 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:02:45 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:46 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:47 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:48 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:02:49 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:49 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:02:50 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:51 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:02:52 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:02:52] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:02:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:02:53 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:02:54 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:54 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:02:55 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:56 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:57 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:58 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:02:58 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:02:59 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:00 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:01 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:02 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:03 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:03 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:04 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:05 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:06 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:07 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:07 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:08 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:08] INFO:     127.0.0.1:45020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:03:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:03:09 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:03:10 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:11 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:12 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:12 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:03:13 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:14 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:15 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:16 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:17 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:17 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:18 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:03:19 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:20 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:03:21 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:03:21 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:22 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:03:23 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:24 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:03:25 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:03:25] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:03:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:03:26 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:03:26 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:03:27 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:03:28 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:03:29 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:03:30 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:03:31 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:31 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:03:32 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:33 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:03:34 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:35 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:35 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:36 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:37 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:38 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:39 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:40 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:40 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:41 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:41] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:03:42 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 16:03:43 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:03:44 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:03:45 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:03:45 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:03:46 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:03:47 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:03:48 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:49 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:49 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:50 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:51 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:52 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:03:53 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:54 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:54 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:55 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:56 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:03:57 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:03:58 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:03:58] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:03:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:03:59 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:03:59 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:04:00 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:01 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:02 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:04:03 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:03 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:04 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:05 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:06 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:07 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:08 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:08 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:09 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:10 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:11 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:12 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:13 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:13 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:14 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:14] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:04:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:04:15 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.45, #queue-req: 0, 
[2025-11-02 16:04:16 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:04:17 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:04:17 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:04:18 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:04:19 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:04:20 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:21 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:04:22 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:22 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:23 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:24 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:25 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:04:26 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:27 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:27 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:28 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:29 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:30 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:04:31 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:31] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:04:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:04:31 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:04:32 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:33 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:34 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:35 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:36 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:36 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:37 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:38 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:39 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:40 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:04:41 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:41 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:42 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:43 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:04:44 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:45 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:04:45 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:04:46 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:04:47 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:04:47] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:04:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:04:48 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:04:49 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:50 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:50 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:51 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:04:52 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:04:53 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:54 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:55 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:55 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:56 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:57 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:04:58 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:04:59 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:04:59 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:00 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:01 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:05:02 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:03 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:05:04 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:05:04] INFO:     127.0.0.1:34424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:05:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:05:04 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:05:05 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:06 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:05:07 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:08 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:09 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:09 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:10 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:11 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:12 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:13 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:13 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:14 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:15 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:16 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:05:17 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:05:18 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:18 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:05:19 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:20 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:05:20] INFO:     127.0.0.1:40448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:05:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:05:21 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:05:22 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:23 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:23 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:05:24 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:25 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:05:26 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:05:27 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:27 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:28 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:05:29 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:30 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:05:31 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.55, #queue-req: 0, 
[2025-11-02 16:05:32 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:05:32 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:33 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:05:34 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:35 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:36 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:36 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:05:37] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:05:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:05:37 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:05:38 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:05:39 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:05:40 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:41 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:05:41 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-11-02 16:05:42 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:43 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:05:44 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:05:45 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:05:46 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:46 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:05:47 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:48 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:49 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:50 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:50 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:05:51 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:52 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:05:53 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:05:53] INFO:     127.0.0.1:45884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:05:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:05:54 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:05:55 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:55 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:05:56 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:57 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:05:58 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:05:59 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:00 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:00 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:01 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:02 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:03 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:06:04 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:05 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:06:05 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:06:06 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:06:07 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:08 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:06:09 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:06:09 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:06:09] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:06:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:06:10 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.83, #queue-req: 0, 
[2025-11-02 16:06:11 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:06:12 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:06:13 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:14 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:14 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:06:15 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:16 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:17 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:06:18 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:19 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:19 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:20 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:21 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:22 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:23 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:23 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:24 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:25 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:26 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:26] INFO:     127.0.0.1:47572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:06:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:06:27 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:06:28 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:28 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:29 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:30 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:31 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:32 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:33 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:06:33 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:34 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:35 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:36 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:37 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:06:37 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:38 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:39 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:40 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:06:41 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:42 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:42 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:06:42] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:06:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:06:43 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:06:44 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:45 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:46 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:06:47 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:06:47 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:06:48 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:49 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:50 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:06:51 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:51 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:06:52 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:53 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:06:54 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:55 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:56 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:56 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:57 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:58 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:06:59 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:06:59] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:06:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:07:00 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 16:07:01 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:07:01 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:07:02 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:07:03 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:07:04 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:07:05 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:07:05 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:06 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:07:07 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:07:08 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:09 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:10 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:10 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:07:11 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:12 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:13 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:14 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:07:14 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:15 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:07:15] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:07:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:07:16 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:07:17 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:18 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:19 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:07:19 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:07:20 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:21 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:22 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:07:23 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:24 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:07:24 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:07:25 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:07:26 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:27 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:28 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:28 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:07:29 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:30 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:31 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:32 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:07:32] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:07:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:07:33 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:07:33 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:34 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:07:35 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:07:36 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:07:37 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:07:38 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:07:38 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:39 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:40 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:07:41 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:07:42 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:42 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:43 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:44 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:45 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:07:46 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:07:47 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:07:47 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:07:48 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:07:48] INFO:     127.0.0.1:45844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:07:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:07:49 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.88, #queue-req: 0, 
[2025-11-02 16:07:50 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:07:51 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-02 16:07:52 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:07:52 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:53 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:07:54 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:55 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:56 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:56 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:07:57 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:58 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:07:59 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:00 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:01 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:01 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:02 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:03 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:04 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:05 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:05] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:08:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:08:06 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 16:08:06 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:08:07 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:08 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:09 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:10 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:10 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:08:11 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:12 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:08:13 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:08:14 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:08:15 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:08:15 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:16 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:17 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:18 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:19 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:19 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:08:20 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:08:21 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:21] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:08:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:08:22 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 16:08:23 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:08:24 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:08:24 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:25 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:26 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:08:27 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:28 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:08:29 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:29 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:30 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:08:31 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:08:32 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:33 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:33 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:34 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:35 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:08:36 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:37 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:38 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:08:38] INFO:     127.0.0.1:46132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:08:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:08:38 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.07, #queue-req: 0, 
[2025-11-02 16:08:39 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:40 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:41 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:08:42 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:08:43 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:08:43 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:44 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:45 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:08:46 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:08:47 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:47 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:48 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:49 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:08:50 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:51 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:08:52 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:08:52 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:08:53 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:54 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:08:54] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:08:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:08:55 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:08:56 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:08:57 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:57 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:58 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:08:59 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:09:00 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:09:01 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:09:01 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:09:02 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:09:03 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:09:04 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:09:05 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:09:06 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:09:06 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:09:07 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:09:08 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:09:09 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:09:10 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:09:10 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:09:11] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:09:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:09:11 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-02 16:09:12 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:09:13 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:09:14 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:09:15 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:09:15 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:09:16 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:09:17 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:18 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:09:19 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:09:20 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:20 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:21 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:09:22 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:23 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:09:24 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:24 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:09:25 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:26 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:09:27 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:09:27] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:09:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:09:28 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:09:29 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:09:29 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:09:30 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:09:31 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:09:32 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:09:33 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:09:34 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:09:34 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:09:35 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:09:36 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:09:37 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:09:38 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:09:38 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:09:39 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:09:40 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:09:41 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:09:42 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:09:43 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:09:43 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:09:43] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:09:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:09:44 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.76, #queue-req: 0, 
[2025-11-02 16:09:45 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:09:46 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:09:47 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:09:48 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:09:48 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:09:49 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:09:50 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:09:51 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:09:52 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:09:52 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:09:53 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:09:54 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:09:55 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:09:56 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-11-02 16:09:57 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:09:57 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:09:58 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:09:59 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-11-02 16:10:00 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:10:00] INFO:     127.0.0.1:47226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:10:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:10:01 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:10:02 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:10:02 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:10:03 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:04 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:10:05 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:06 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:07 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:07 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:10:08 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:09 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:10 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:11 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:11 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:12 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:13 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:14 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:15 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:16 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:16 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:16] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:10:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:10:17 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:10:18 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:10:19 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:20 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:21 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:21 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:22 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:23 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:10:24 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:25 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:25 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:26 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:10:27 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:28 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:10:29 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:10:30 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:10:30 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:31 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:10:32 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:33 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:10:33] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:10:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:10:34 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-02 16:10:35 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:35 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:36 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:37 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:38 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:39 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:39 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:40 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:41 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:42 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:43 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:44 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:44 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:45 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:46 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:47 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:48 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:48 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:49 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:10:49] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:10:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:10:50 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:10:51 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:10:52 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:53 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:53 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:54 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:55 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:56 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:10:57 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:58 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:10:58 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:10:59 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:00 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:11:01 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:02 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:02 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:03 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:04 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:05 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:11:06 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:06] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:11:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:11:07 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-02 16:11:07 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:11:08 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:11:09 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:11:10 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:11:11 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:11:12 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:11:12 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:11:13 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:11:14 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:11:15 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:11:16 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:11:16 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:11:17 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:11:18 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:11:19 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:20 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:11:21 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:11:21 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:22 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:11:22] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:11:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:11:23 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.79, #queue-req: 0, 
[2025-11-02 16:11:24 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:25 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:11:26 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:26 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:11:27 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:11:28 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:29 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:11:30 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:11:30 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:11:31 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:11:32 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:11:33 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:11:34 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:11:35 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:11:35 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:11:36 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:11:37 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:11:38 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:11:39 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:11:39] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:11:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:11:40 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-02 16:11:40 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:11:41 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:11:42 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:43 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:44 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:45 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:11:45 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:46 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:47 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:48 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:11:49 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:49 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:50 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:11:51 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:52 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:53 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:11:54 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:11:54 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:11:55 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:11:55] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:11:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:11:56 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-02 16:11:57 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:11:58 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 16:11:59 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 16:11:59 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:12:00 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 16:12:01 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:12:02 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:12:03 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-02 16:12:03 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:12:04 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:12:05 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:12:06 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:12:07 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:12:08 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:12:08 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:12:09 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:12:10 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:11 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:12 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:12:12] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:12:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:12:12 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-02 16:12:13 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:12:14 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:15 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:16 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:17 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:17 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:12:18 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:19 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:20 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:21 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:12:22 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:22 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:23 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:24 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:25 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:26 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:26 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:27 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:12:28 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:28] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:12:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:12:29 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-02 16:12:30 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:31 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-02 16:12:31 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:12:32 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:12:33 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:12:34 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:35 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:35 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:12:36 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:12:37 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:12:38 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:12:39 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:12:40 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:40 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:41 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:12:42 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:12:43 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:44 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:44 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:12:45] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:12:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:12:45 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-02 16:12:46 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:47 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:12:48 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:12:49 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:12:49 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:50 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:51 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:12:52 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:53 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:12:54 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:54 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:12:55 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:12:56 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:12:57 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:12:58 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:12:59 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:12:59 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:13:00 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:01 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:13:01] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:13:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:13:02 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:13:03 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:13:04 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:13:04 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:13:05 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:13:06 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:13:07 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:13:08 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:13:08 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:13:09 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:13:10 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:13:11 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:13:12 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:13:13 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:13:13 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:13:14 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:15 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:13:16 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:13:17 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:17 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:17] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:13:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:13:18 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:13:19 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:20 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:21 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:22 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:22 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:23 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:24 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:25 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:26 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:27 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:27 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:28 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:29 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:30 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:31 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:31 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:13:32 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:33 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:34 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:34] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:13:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:13:35 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-11-02 16:13:36 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:36 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:37 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:38 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:39 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:40 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:41 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:41 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:13:42 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:43 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:44 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:45 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:13:45 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:13:46 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-11-02 16:13:47 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:13:48 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:13:49 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:13:50 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-11-02 16:13:50 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-11-02 16:13:50] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:13:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:13:51 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:13:52 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:13:53 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:13:54 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:55 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:55 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:56 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:57 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:13:58 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:13:59 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:00 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:00 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:01 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:02 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:14:03 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:04 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:04 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:05 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:06 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:07 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:07] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:14:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:14:08 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-02 16:14:09 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:14:09 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:10 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:11 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:12 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:13 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:14:14 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:14 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:15 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:16 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:17 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:18 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:18 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:19 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:20 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:21 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:22 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:23 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:23 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:23] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:14:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:14:24 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.82, #queue-req: 0, 
[2025-11-02 16:14:25 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:14:26 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:27 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:28 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:28 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:29 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:30 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:31 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:32 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:32 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:33 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:34 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:35 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:36 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:37 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:37 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:14:38 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:14:39 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:40 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:14:40] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:14:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:14:41 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-02 16:14:42 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:14:42 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:14:43 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:14:44 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:14:45 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:14:46 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:14:46 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:14:47 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:14:48 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:14:49 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:50 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:51 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:14:51 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:52 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:53 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:14:54 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:55 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:14:56 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:56 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:56] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:14:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:14:57 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:14:58 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:14:59 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:00 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:01 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:01 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:02 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:03 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:04 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:05 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:05 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:06 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:15:07 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:15:08 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:15:09 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:15:10 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:15:10 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:15:11 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:15:12 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:15:13 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:15:13] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:15:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:15:14 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:15:15 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:15:15 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-02 16:15:16 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:15:17 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:15:18 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:15:19 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:15:19 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:15:20 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:15:21 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:15:22 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:15:23 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:24 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:15:24 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:25 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:26 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:27 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:28 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:28 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:29 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:29] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:15:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:15:30 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:15:31 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:32 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:33 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:33 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:34 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:15:35 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:15:36 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:15:37 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:38 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:38 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:39 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:40 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:15:41 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:42 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:42 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:43 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:44 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:15:45 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:15:46 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:15:46] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:15:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:15:47 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-02 16:15:47 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-02 16:15:48 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-02 16:15:49 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-02 16:15:50 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:15:51 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:15:52 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:52 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:15:53 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:54 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:55 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:56 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:15:56 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:15:57 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:15:58 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:15:59 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:00 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:01 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:01 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:02 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:02] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:16:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:16:03 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:16:04 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:16:05 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:16:06 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:06 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:07 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:08 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:09 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:10 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:10 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:11 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:12 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:13 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:14 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:15 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:15 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:16:16 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:17 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:16:18 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:19 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:16:19] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:16:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:16:20 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-02 16:16:20 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:21 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:22 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:23 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:24 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:24 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:25 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:26 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:27 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:28 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:29 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:29 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:30 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:31 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:32 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:33 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:16:34 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:16:34 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:16:35 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:16:35] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:16:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:16:36 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-02 16:16:37 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:16:38 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:16:39 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:16:39 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:40 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:41 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:16:42 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:43 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:16:43 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:44 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:45 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:46 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:16:47 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:48 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:48 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:49 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:50 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:51 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:52 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:52] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:16:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:16:53 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-02 16:16:53 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:54 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:55 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:16:56 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:57 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:57 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:16:58 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:16:59 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:00 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:17:01 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:02 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:02 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:03 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:04 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:05 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:17:06 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:06 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:07 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:08 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:17:08] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:17:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:17:09 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-02 16:17:10 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:17:11 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:17:11 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:12 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:17:13 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:14 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:15 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:17:16 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:16 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:17 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:17:18 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:17:19 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:20 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:20 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:17:21 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:22 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:23 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:24 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:25 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:25] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:17:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:17:25 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:17:26 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:27 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:28 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:29 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:30 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:30 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:31 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:17:32 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:33 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:17:34 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:17:34 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:17:35 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:17:36 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:17:37 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:17:38 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:17:39 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:17:39 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:17:40 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-02 16:17:41 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-02 16:17:41] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:17:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:17:42 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-02 16:17:43 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:17:44 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:17:44 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:45 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:17:46 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:47 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:48 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:48 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:49 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:50 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:51 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:17:52 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:53 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:53 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:54 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:55 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:56 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:17:57 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:58 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:17:58] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:17:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:17:58 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:17:59 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:00 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:01 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:02 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:03 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:03 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:04 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:05 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:06 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:07 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:07 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:08 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:09 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:18:10 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:11 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:18:12 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:12 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:18:13 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:18:14 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:18:14] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:18:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:18:15 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-02 16:18:16 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:18:17 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:18:17 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:18:18 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:18:19 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:20 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:18:21 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:18:21 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:22 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:18:23 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:24 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:25 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:26 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:26 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:27 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:28 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:29 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:30 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:30 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:30] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:18:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:18:31 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-02 16:18:32 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:18:33 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:18:34 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:18:35 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:18:35 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:18:36 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:18:37 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:18:38 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:18:39 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:18:40 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:18:40 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:41 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:42 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:43 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:18:44 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:44 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:45 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:18:46 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:18:47 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:47] INFO:     127.0.0.1:37408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:18:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:18:48 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-02 16:18:49 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:49 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:50 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:51 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:18:52 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:53 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:54 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:54 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:55 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:56 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:18:57 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:18:58 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:18:58 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:18:59 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-02 16:19:00 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:19:01 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-02 16:19:02 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:19:03 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:19:03 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-02 16:19:03] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:19:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:19:04 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-02 16:19:05 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:19:06 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:19:07 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:19:08 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:08 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:09 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:19:10 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:11 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:12 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:12 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:13 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:14 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:15 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:16 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:17 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:17 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:18 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:19 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:20 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:19:20] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:19:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:19:21 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-11-02 16:19:22 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-02 16:19:22 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:19:23 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:19:24 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-02 16:19:25 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:19:26 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-02 16:19:26 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:19:27 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:19:28 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-02 16:19:29 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:30 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:19:31 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:31 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:32 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:33 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:34 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:35 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:35 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:36 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:36] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-02 16:19:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-02 16:19:37 TP0] Decode batch, #running-req: 1, #token: 3240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-02 16:19:38 TP0] Decode batch, #running-req: 1, #token: 3280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:39 TP0] Decode batch, #running-req: 1, #token: 3320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-02 16:19:40 TP0] Decode batch, #running-req: 1, #token: 3360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:40 TP0] Decode batch, #running-req: 1, #token: 3400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:41 TP0] Decode batch, #running-req: 1, #token: 3440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:42 TP0] Decode batch, #running-req: 1, #token: 3480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:43 TP0] Decode batch, #running-req: 1, #token: 3520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-02 16:19:44 TP0] Decode batch, #running-req: 1, #token: 3560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:45 TP0] Decode batch, #running-req: 1, #token: 3600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-02 16:19:45 TP0] Decode batch, #running-req: 1, #token: 3640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-02 16:19:46 TP0] Decode batch, #running-req: 1, #token: 3680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:47 TP0] Decode batch, #running-req: 1, #token: 3720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-02 16:19:48 TP0] Decode batch, #running-req: 1, #token: 3760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:49 TP0] Decode batch, #running-req: 1, #token: 3800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:50 TP0] Decode batch, #running-req: 1, #token: 3840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:19:50 TP0] Decode batch, #running-req: 1, #token: 3880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-02 16:19:51 TP0] Decode batch, #running-req: 1, #token: 3920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:52 TP0] Decode batch, #running-req: 1, #token: 3960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:53 TP0] Decode batch, #running-req: 1, #token: 4000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-02 16:19:53] INFO:     127.0.0.1:60752 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-02 16:20:00] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-02 16:20:03] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
