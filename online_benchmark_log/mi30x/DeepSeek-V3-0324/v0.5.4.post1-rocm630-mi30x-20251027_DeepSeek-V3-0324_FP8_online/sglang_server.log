INFO 10-27 15:30:40 __init__.py:179] Automatically detected platform rocm.
WARNING 10-27 15:30:40 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:42] WARNING server_args.py:1105: Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-27 15:30:42] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.81, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=106665584, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-27 15:30:43] Using default HuggingFace chat template with detected content format: string
INFO 10-27 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:50 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-27 15:30:51 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:52] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:52 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-10-27 15:30:52 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-27 15:30:53 TP7] Init torch distributed begin.
[2025-10-27 15:30:53 TP5] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:53 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-27 15:30:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:53 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-27 15:30:53 TP3] Init torch distributed begin.
[2025-10-27 15:30:53 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:53] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-27 15:30:53 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-27 15:30:53 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-27 15:30:54 TP0] Init torch distributed begin.
[2025-10-27 15:30:54 TP6] Init torch distributed begin.
[2025-10-27 15:30:54 TP2] Init torch distributed begin.
[2025-10-27 15:30:54 TP1] Init torch distributed begin.
[2025-10-27 15:30:54 TP4] Init torch distributed begin.
[2025-10-27 15:30:54 TP0] sglang is using nccl==2.21.5
[2025-10-27 15:30:56 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-10-27 15:30:56 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-10-27 15:30:56 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-10-27 15:30:56 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-10-27 15:30:56 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-10-27 15:30:56 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-10-27 15:30:56 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-10-27 15:30:56 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-10-27 15:30:58 TP5] Load weight begin. avail mem=187.33 GB
[2025-10-27 15:30:58 TP7] Load weight begin. avail mem=187.32 GB
[2025-10-27 15:30:58 TP2] Load weight begin. avail mem=187.19 GB
[2025-10-27 15:30:58 TP6] Load weight begin. avail mem=187.31 GB
[2025-10-27 15:30:58 TP4] Load weight begin. avail mem=187.25 GB
[2025-10-27 15:30:58 TP0] Load weight begin. avail mem=187.61 GB
[2025-10-27 15:30:58 TP0] Detected fp8 checkpoint.
[2025-10-27 15:30:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-27 15:30:58 TP3] Load weight begin. avail mem=187.20 GB
[2025-10-27 15:30:58 TP1] Load weight begin. avail mem=187.19 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:17,  9.36it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:28,  5.75it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:23,  6.70it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:22,  7.07it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:30,  5.10it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:22,  7.07it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:24,  6.26it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:22,  6.86it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:21,  7.19it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:20,  7.49it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:46,  3.28it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:37,  3.96it/s]
Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:02<00:32,  4.65it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:31,  4.75it/s]
Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:02<00:27,  5.41it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:03<00:27,  5.35it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:03<00:24,  5.93it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:25,  5.55it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:23,  6.02it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:03<00:25,  5.46it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:03<00:23,  5.97it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:21,  6.42it/s]
Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:04<00:20,  6.95it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:23,  5.98it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:04<00:24,  5.62it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:04<00:21,  6.34it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:19,  6.77it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:04<00:18,  7.10it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:05<00:13,  9.97it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:05<00:20,  6.35it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:05<00:13,  9.31it/s]
Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:06<00:20,  6.16it/s]
Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:06<00:15,  7.73it/s]
Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:06<00:12,  9.31it/s]
Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:06<00:11, 10.62it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:06<00:08, 13.32it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:06<00:07, 14.64it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:07<00:09, 11.87it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:07<00:09, 11.69it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:07<00:07, 14.57it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:07<00:06, 15.15it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:07<00:07, 12.89it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:07<00:06, 15.15it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:08<00:06, 15.54it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:08<00:11,  8.03it/s]
Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:08<00:09,  9.85it/s]
Loading safetensors checkpoint shards:  45% Completed | 74/163 [00:09<00:08, 11.11it/s]
Loading safetensors checkpoint shards:  47% Completed | 76/163 [00:09<00:06, 12.44it/s]
Loading safetensors checkpoint shards:  48% Completed | 79/163 [00:09<00:05, 14.93it/s]
Loading safetensors checkpoint shards:  50% Completed | 81/163 [00:09<00:05, 13.88it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:09<00:05, 14.84it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:09<00:06, 11.78it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:09<00:05, 12.83it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:10<00:04, 14.96it/s]
Loading safetensors checkpoint shards:  57% Completed | 93/163 [00:10<00:04, 14.97it/s]
Loading safetensors checkpoint shards:  58% Completed | 95/163 [00:10<00:04, 15.71it/s]
Loading safetensors checkpoint shards:  60% Completed | 97/163 [00:10<00:04, 16.16it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:10<00:03, 16.34it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:10<00:03, 18.00it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:10<00:02, 20.25it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:11<00:07,  7.68it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:11<00:06,  8.81it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:12<00:04, 10.30it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:12<00:03, 12.39it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:12<00:03, 12.67it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:12<00:02, 14.83it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:12<00:02, 17.15it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:12<00:01, 18.76it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:12<00:01, 17.88it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:13<00:01, 19.51it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:13<00:01, 21.54it/s]
Loading safetensors checkpoint shards:  85% Completed | 139/163 [00:13<00:01, 21.72it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:13<00:01, 18.20it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:13<00:00, 20.14it/s]
Loading safetensors checkpoint shards:  91% Completed | 148/163 [00:13<00:00, 20.32it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:13<00:00, 20.38it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:14<00:00, 15.19it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:15<00:00,  7.03it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:15<00:00,  8.91it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:15<00:00, 10.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:15<00:00, 10.57it/s]

[2025-10-27 15:31:45 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-10-27 15:31:45 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-10-27 15:31:45 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-27 15:31:45 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-10-27 15:31:46 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-10-27 15:31:46 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-27 15:31:46 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-10-27 15:31:46 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-27 15:31:46 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-27 15:31:46 TP4] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP4] Memory pool end. avail mem=34.96 GB
[2025-10-27 15:31:46 TP7] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP7] Memory pool end. avail mem=35.04 GB
[2025-10-27 15:31:46 TP1] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP1] Memory pool end. avail mem=34.91 GB
[2025-10-27 15:31:46 TP2] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP6] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP2] Memory pool end. avail mem=34.90 GB
[2025-10-27 15:31:46 TP0] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP5] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP6] Memory pool end. avail mem=35.03 GB
[2025-10-27 15:31:46 TP0] Memory pool end. avail mem=35.32 GB
[2025-10-27 15:31:46 TP5] Memory pool end. avail mem=35.05 GB
[2025-10-27 15:31:46 TP3] KV Cache is allocated. #tokens: 1100494, KV size: 72.02 GB
[2025-10-27 15:31:46 TP3] Memory pool end. avail mem=34.91 GB
[2025-10-27 15:31:48 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=34.83 GB
[2025-10-27 15:31:48 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=34.76 GB
[2025-10-27 15:31:48 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=35.12 GB
[2025-10-27 15:31:48 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-27 15:31:48 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=34.84 GB
[2025-10-27 15:31:48 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=34.84 GB
[2025-10-27 15:31:48 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=34.71 GB
[2025-10-27 15:31:48 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=34.70 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=34.48 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-27 15:31:49 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=34.70 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-27 15:31:50 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:50 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:52 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:52 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:52 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-27 15:31:53 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:53 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:53 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:53 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:31:53 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:53 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:53 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=34.48 GB):   2%|         | 1/52 [00:05<04:24,  5.20s/it]Capturing batches (bs=496 avail_mem=33.82 GB):   2%|         | 1/52 [00:05<04:24,  5.20s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=33.82 GB):   4%|         | 2/52 [00:05<02:09,  2.59s/it]Capturing batches (bs=480 avail_mem=33.81 GB):   4%|         | 2/52 [00:05<02:09,  2.59s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:54 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=33.81 GB):   6%|         | 3/52 [00:06<01:17,  1.59s/it]Capturing batches (bs=464 avail_mem=33.81 GB):   6%|         | 3/52 [00:06<01:17,  1.59s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=33.81 GB):   8%|         | 4/52 [00:06<00:53,  1.12s/it]Capturing batches (bs=448 avail_mem=33.80 GB):   8%|         | 4/52 [00:06<00:53,  1.12s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=33.80 GB):  10%|         | 5/52 [00:07<00:40,  1.16it/s]Capturing batches (bs=432 avail_mem=33.80 GB):  10%|         | 5/52 [00:07<00:40,  1.16it/s][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:55 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=33.80 GB):  12%|        | 6/52 [00:07<00:32,  1.42it/s]Capturing batches (bs=416 avail_mem=33.79 GB):  12%|        | 6/52 [00:07<00:32,  1.42it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=33.79 GB):  13%|        | 7/52 [00:07<00:27,  1.64it/s]Capturing batches (bs=400 avail_mem=33.79 GB):  13%|        | 7/52 [00:07<00:27,  1.64it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:56 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=33.79 GB):  15%|        | 8/52 [00:08<00:24,  1.83it/s]Capturing batches (bs=384 avail_mem=33.78 GB):  15%|        | 8/52 [00:08<00:24,  1.83it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=33.78 GB):  17%|        | 9/52 [00:08<00:19,  2.19it/s]Capturing batches (bs=368 avail_mem=33.78 GB):  17%|        | 9/52 [00:08<00:19,  2.19it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=33.78 GB):  19%|        | 10/52 [00:09<00:18,  2.27it/s]Capturing batches (bs=352 avail_mem=33.78 GB):  19%|        | 10/52 [00:09<00:18,  2.27it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:57 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=33.78 GB):  21%|        | 11/52 [00:09<00:17,  2.33it/s]Capturing batches (bs=336 avail_mem=33.77 GB):  21%|        | 11/52 [00:09<00:17,  2.33it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=33.77 GB):  23%|       | 12/52 [00:09<00:16,  2.37it/s]Capturing batches (bs=320 avail_mem=33.77 GB):  23%|       | 12/52 [00:09<00:16,  2.37it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=33.77 GB):  25%|       | 13/52 [00:10<00:14,  2.66it/s]Capturing batches (bs=304 avail_mem=33.76 GB):  25%|       | 13/52 [00:10<00:14,  2.66it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:58 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=33.76 GB):  27%|       | 14/52 [00:10<00:14,  2.59it/s]Capturing batches (bs=288 avail_mem=33.76 GB):  27%|       | 14/52 [00:10<00:14,  2.59it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=33.76 GB):  29%|       | 15/52 [00:10<00:12,  2.87it/s]Capturing batches (bs=272 avail_mem=33.75 GB):  29%|       | 15/52 [00:10<00:12,  2.87it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:31:59 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=33.75 GB):  31%|       | 16/52 [00:11<00:13,  2.72it/s]Capturing batches (bs=256 avail_mem=33.75 GB):  31%|       | 16/52 [00:11<00:13,  2.72it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:31:59 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:00 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:00 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=33.75 GB):  33%|      | 17/52 [00:11<00:13,  2.63it/s]Capturing batches (bs=248 avail_mem=33.74 GB):  33%|      | 17/52 [00:11<00:13,  2.63it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=33.74 GB):  35%|      | 18/52 [00:12<00:13,  2.56it/s]Capturing batches (bs=240 avail_mem=33.74 GB):  35%|      | 18/52 [00:12<00:13,  2.56it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:00 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=33.74 GB):  37%|      | 19/52 [00:12<00:13,  2.53it/s]Capturing batches (bs=232 avail_mem=33.73 GB):  37%|      | 19/52 [00:12<00:13,  2.53it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=33.73 GB):  38%|      | 20/52 [00:12<00:12,  2.49it/s]Capturing batches (bs=224 avail_mem=33.73 GB):  38%|      | 20/52 [00:12<00:12,  2.49it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:01 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=33.73 GB):  40%|      | 21/52 [00:13<00:12,  2.47it/s]Capturing batches (bs=216 avail_mem=33.72 GB):  40%|      | 21/52 [00:13<00:12,  2.47it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=33.72 GB):  42%|     | 22/52 [00:13<00:12,  2.47it/s]Capturing batches (bs=208 avail_mem=33.72 GB):  42%|     | 22/52 [00:13<00:12,  2.47it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=33.72 GB):  44%|     | 23/52 [00:13<00:10,  2.76it/s]Capturing batches (bs=200 avail_mem=33.71 GB):  44%|     | 23/52 [00:13<00:10,  2.76it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:02 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=33.71 GB):  46%|     | 24/52 [00:14<00:10,  2.66it/s]Capturing batches (bs=192 avail_mem=33.71 GB):  46%|     | 24/52 [00:14<00:10,  2.66it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=33.71 GB):  48%|     | 25/52 [00:14<00:09,  2.91it/s]Capturing batches (bs=184 avail_mem=33.71 GB):  48%|     | 25/52 [00:14<00:09,  2.91it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=33.71 GB):  50%|     | 26/52 [00:14<00:08,  3.13it/s]Capturing batches (bs=176 avail_mem=33.70 GB):  50%|     | 26/52 [00:14<00:08,  3.13it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=33.70 GB):  52%|    | 27/52 [00:15<00:07,  3.27it/s]Capturing batches (bs=168 avail_mem=33.70 GB):  52%|    | 27/52 [00:15<00:07,  3.27it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:03 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=33.70 GB):  54%|    | 28/52 [00:15<00:08,  2.95it/s]Capturing batches (bs=160 avail_mem=33.70 GB):  54%|    | 28/52 [00:15<00:08,  2.95it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=33.70 GB):  56%|    | 29/52 [00:15<00:07,  3.17it/s]Capturing batches (bs=152 avail_mem=33.69 GB):  56%|    | 29/52 [00:15<00:07,  3.17it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=33.69 GB):  58%|    | 30/52 [00:16<00:07,  2.92it/s]Capturing batches (bs=144 avail_mem=33.69 GB):  58%|    | 30/52 [00:16<00:07,  2.92it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:04 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=33.69 GB):  60%|    | 31/52 [00:16<00:06,  3.15it/s]Capturing batches (bs=136 avail_mem=33.68 GB):  60%|    | 31/52 [00:16<00:06,  3.15it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=33.68 GB):  62%|   | 32/52 [00:16<00:06,  3.28it/s]Capturing batches (bs=128 avail_mem=33.68 GB):  62%|   | 32/52 [00:16<00:06,  3.28it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:05 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=33.68 GB):  63%|   | 33/52 [00:17<00:05,  3.39it/s]Capturing batches (bs=120 avail_mem=33.68 GB):  63%|   | 33/52 [00:17<00:05,  3.39it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:05 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=33.68 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s]Capturing batches (bs=112 avail_mem=33.67 GB):  65%|   | 34/52 [00:17<00:05,  3.05it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=33.67 GB):  67%|   | 35/52 [00:17<00:05,  3.24it/s]Capturing batches (bs=104 avail_mem=33.67 GB):  67%|   | 35/52 [00:17<00:05,  3.24it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=33.67 GB):  69%|   | 36/52 [00:18<00:05,  2.95it/s]Capturing batches (bs=96 avail_mem=33.67 GB):  69%|   | 36/52 [00:18<00:05,  2.95it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:06 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=33.67 GB):  71%|   | 37/52 [00:18<00:04,  3.17it/s]Capturing batches (bs=88 avail_mem=33.66 GB):  71%|   | 37/52 [00:18<00:04,  3.17it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=33.66 GB):  73%|  | 38/52 [00:18<00:04,  2.91it/s]Capturing batches (bs=80 avail_mem=33.66 GB):  73%|  | 38/52 [00:18<00:04,  2.91it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=33.66 GB):  75%|  | 39/52 [00:19<00:04,  3.13it/s]Capturing batches (bs=72 avail_mem=33.65 GB):  75%|  | 39/52 [00:19<00:04,  3.13it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:07 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=33.65 GB):  77%|  | 40/52 [00:19<00:04,  2.89it/s]Capturing batches (bs=64 avail_mem=33.65 GB):  77%|  | 40/52 [00:19<00:04,  2.89it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:32:08 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=33.65 GB):  79%|  | 41/52 [00:19<00:03,  3.12it/s]Capturing batches (bs=56 avail_mem=33.64 GB):  79%|  | 41/52 [00:19<00:03,  3.12it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=33.64 GB):  81%|  | 42/52 [00:20<00:03,  2.88it/s]Capturing batches (bs=48 avail_mem=33.64 GB):  81%|  | 42/52 [00:20<00:03,  2.88it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:08 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=33.64 GB):  83%| | 43/52 [00:20<00:02,  3.08it/s]Capturing batches (bs=40 avail_mem=33.63 GB):  83%| | 43/52 [00:20<00:02,  3.08it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=33.63 GB):  85%| | 44/52 [00:20<00:02,  2.87it/s]Capturing batches (bs=32 avail_mem=33.63 GB):  85%| | 44/52 [00:20<00:02,  2.87it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:09 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=33.63 GB):  87%| | 45/52 [00:21<00:02,  3.10it/s]Capturing batches (bs=24 avail_mem=33.62 GB):  87%| | 45/52 [00:21<00:02,  3.10it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:09 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=33.62 GB):  88%| | 46/52 [00:21<00:02,  2.87it/s]Capturing batches (bs=16 avail_mem=33.62 GB):  88%| | 46/52 [00:21<00:02,  2.87it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:32:10 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=33.62 GB):  90%| | 47/52 [00:21<00:01,  3.11it/s]Capturing batches (bs=12 avail_mem=33.62 GB):  90%| | 47/52 [00:21<00:01,  3.11it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=33.62 GB):  92%|| 48/52 [00:21<00:01,  3.31it/s]Capturing batches (bs=8 avail_mem=33.61 GB):  92%|| 48/52 [00:21<00:01,  3.31it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=33.61 GB):  94%|| 49/52 [00:22<00:00,  3.47it/s]Capturing batches (bs=4 avail_mem=33.61 GB):  94%|| 49/52 [00:22<00:00,  3.47it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:10 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=33.61 GB):  96%|| 50/52 [00:22<00:00,  3.56it/s]Capturing batches (bs=2 avail_mem=33.61 GB):  96%|| 50/52 [00:22<00:00,  3.56it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=33.61 GB):  98%|| 51/52 [00:22<00:00,  3.65it/s]Capturing batches (bs=1 avail_mem=33.60 GB):  98%|| 51/52 [00:22<00:00,  3.65it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:11 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=33.60 GB): 100%|| 52/52 [00:23<00:00,  2.41it/s]Capturing batches (bs=1 avail_mem=33.60 GB): 100%|| 52/52 [00:23<00:00,  2.21it/s]
[2025-10-27 15:32:12 TP0] Registering 6396 cuda graph addresses
[2025-10-27 15:32:12 TP7] Capture cuda graph end. Time elapsed: 24.54 s. mem usage=1.52 GB. avail mem=33.31 GB.
[2025-10-27 15:32:12 TP5] Capture cuda graph end. Time elapsed: 24.54 s. mem usage=1.52 GB. avail mem=33.32 GB.
[2025-10-27 15:32:13 TP0] Capture cuda graph end. Time elapsed: 24.57 s. mem usage=1.52 GB. avail mem=33.60 GB.
[2025-10-27 15:32:13 TP1] Capture cuda graph end. Time elapsed: 23.74 s. mem usage=1.52 GB. avail mem=33.18 GB.
[2025-10-27 15:32:13 TP2] Capture cuda graph end. Time elapsed: 24.39 s. mem usage=1.52 GB. avail mem=33.17 GB.
[2025-10-27 15:32:13 TP4] Capture cuda graph end. Time elapsed: 24.61 s. mem usage=1.52 GB. avail mem=33.23 GB.
[2025-10-27 15:32:13 TP6] Capture cuda graph end. Time elapsed: 24.62 s. mem usage=1.52 GB. avail mem=33.30 GB.
[2025-10-27 15:32:13 TP3] Capture cuda graph end. Time elapsed: 24.56 s. mem usage=1.52 GB. avail mem=33.19 GB.
[2025-10-27 15:32:13 TP0] max_total_num_tokens=1100494, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=33.60 GB
[2025-10-27 15:32:13] INFO:     Started server process [47]
[2025-10-27 15:32:13] INFO:     Waiting for application startup.
[2025-10-27 15:32:13] INFO:     Application startup complete.
[2025-10-27 15:32:13] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-27 15:32:14] INFO:     127.0.0.1:58016 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:32:14 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:15 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:16] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:16] The server is fired up and ready to roll!
[2025-10-27 15:32:18] INFO:     127.0.0.1:58022 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:32:25] INFO:     127.0.0.1:46230 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:32:26 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26] INFO:     127.0.0.1:46236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:26 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP0] Prefill batch, #new-seq: 41, #new-token: 2481, #cached-token: 27347, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:26 TP0] Prefill batch, #new-seq: 135, #new-token: 8006, #cached-token: 90315, token usage: 0.00, #running-req: 42, #queue-req: 0, 
[2025-10-27 15:32:26 TP0] Prefill batch, #new-seq: 127, #new-token: 7695, #cached-token: 85011, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-10-27 15:32:27 TP0] Prefill batch, #new-seq: 278, #new-token: 16364, #cached-token: 186120, token usage: 0.02, #running-req: 304, #queue-req: 43, 
[2025-10-27 15:32:30 TP0] Prefill batch, #new-seq: 273, #new-token: 16346, #cached-token: 182827, token usage: 0.03, #running-req: 582, #queue-req: 464, 
[2025-10-27 15:32:31 TP0] Prefill batch, #new-seq: 169, #new-token: 10497, #cached-token: 113198, token usage: 0.05, #running-req: 855, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-27 15:32:32 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:32 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:35] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP0] Prefill batch, #new-seq: 1, #new-token: 54, #cached-token: 670, token usage: 0.08, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:35 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:36] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP0] Decode batch, #running-req: 1024, #token: 95294, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1436.54, #queue-req: 294, 
[2025-10-27 15:32:36 TP0] Prefill batch, #new-seq: 2, #new-token: 107, #cached-token: 1340, token usage: 0.09, #running-req: 1022, #queue-req: 292, 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP0] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP1] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP3] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP4] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP2] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP7] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP5] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP6] [fused_moe] using default for (107, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:36 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 291, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:36 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37 TP0] Prefill batch, #new-seq: 3, #new-token: 291, #cached-token: 2010, token usage: 0.09, #running-req: 1021, #queue-req: 288, 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (291, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:48196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] Prefill batch, #new-seq: 8, #new-token: 523, #cached-token: 5361, token usage: 0.09, #running-req: 1016, #queue-req: 280, 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (523, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:47506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:49174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37 TP0] Prefill batch, #new-seq: 3, #new-token: 190, #cached-token: 2011, token usage: 0.10, #running-req: 1021, #queue-req: 277, 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (190, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:51124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] Prefill batch, #new-seq: 9, #new-token: 444, #cached-token: 6035, token usage: 0.10, #running-req: 1015, #queue-req: 268, 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (444, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:37] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:37 TP0] Prefill batch, #new-seq: 6, #new-token: 352, #cached-token: 4018, token usage: 0.10, #running-req: 1018, #queue-req: 262, 
[2025-10-27 15:32:38] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] Prefill batch, #new-seq: 4, #new-token: 187, #cached-token: 2678, token usage: 0.10, #running-req: 1020, #queue-req: 258, 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (187, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:51358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38 TP0] Prefill batch, #new-seq: 4, #new-token: 220, #cached-token: 2681, token usage: 0.10, #running-req: 1020, #queue-req: 254, 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (220, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:46956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:48710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] Prefill batch, #new-seq: 10, #new-token: 597, #cached-token: 6698, token usage: 0.10, #running-req: 1014, #queue-req: 244, 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (597, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38] INFO:     127.0.0.1:46250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] Prefill batch, #new-seq: 7, #new-token: 521, #cached-token: 4692, token usage: 0.10, #running-req: 1017, #queue-req: 237, 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:47968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:53412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:38 TP0] Prefill batch, #new-seq: 7, #new-token: 445, #cached-token: 4694, token usage: 0.10, #running-req: 1017, #queue-req: 230, 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP7] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP4] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP1] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP0] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP3] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP6] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP2] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:38 TP5] [fused_moe] using default for (445, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39] INFO:     127.0.0.1:47884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:51856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39 TP0] Prefill batch, #new-seq: 7, #new-token: 452, #cached-token: 4687, token usage: 0.10, #running-req: 1017, #queue-req: 223, 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP3] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP0] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP4] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP7] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP1] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP2] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP5] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP6] [fused_moe] using default for (452, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39 TP0] Prefill batch, #new-seq: 7, #new-token: 394, #cached-token: 4689, token usage: 0.10, #running-req: 1017, #queue-req: 216, 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP1] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP0] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP3] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP2] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP4] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP7] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP5] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP6] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39] INFO:     127.0.0.1:47892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39 TP0] Prefill batch, #new-seq: 7, #new-token: 330, #cached-token: 4687, token usage: 0.10, #running-req: 1017, #queue-req: 209, 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP4] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP3] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP1] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP7] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP0] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP2] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP6] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP5] [fused_moe] using default for (330, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39] INFO:     127.0.0.1:46982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:47640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:47960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39 TP0] Prefill batch, #new-seq: 9, #new-token: 465, #cached-token: 6028, token usage: 0.10, #running-req: 1015, #queue-req: 200, 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP0] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP1] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP3] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP4] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP2] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP7] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP6] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39 TP5] [fused_moe] using default for (465, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:39] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:39 TP0] Prefill batch, #new-seq: 3, #new-token: 192, #cached-token: 2011, token usage: 0.10, #running-req: 1021, #queue-req: 197, 
[2025-10-27 15:32:40] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:48238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40 TP0] Prefill batch, #new-seq: 10, #new-token: 629, #cached-token: 6697, token usage: 0.10, #running-req: 1014, #queue-req: 187, 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:49116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] Prefill batch, #new-seq: 11, #new-token: 632, #cached-token: 7368, token usage: 0.10, #running-req: 1013, #queue-req: 176, 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (632, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40] INFO:     127.0.0.1:47284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40 TP0] Prefill batch, #new-seq: 8, #new-token: 403, #cached-token: 5360, token usage: 0.11, #running-req: 1016, #queue-req: 168, 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40 TP0] Prefill batch, #new-seq: 8, #new-token: 414, #cached-token: 5361, token usage: 0.11, #running-req: 1016, #queue-req: 160, 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (414, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:40 TP0] Prefill batch, #new-seq: 8, #new-token: 493, #cached-token: 5357, token usage: 0.11, #running-req: 1016, #queue-req: 152, 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP1] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP0] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP3] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP7] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP4] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP2] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP6] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:40 TP5] [fused_moe] using default for (493, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41 TP0] Prefill batch, #new-seq: 7, #new-token: 398, #cached-token: 4690, token usage: 0.11, #running-req: 1017, #queue-req: 145, 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:48858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] Prefill batch, #new-seq: 16, #new-token: 940, #cached-token: 10723, token usage: 0.11, #running-req: 1008, #queue-req: 129, 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (940, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41] INFO:     127.0.0.1:48280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41 TP0] Prefill batch, #new-seq: 7, #new-token: 512, #cached-token: 4688, token usage: 0.11, #running-req: 1017, #queue-req: 122, 
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP2] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP0] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP7] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP3] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP1] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP4] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP6] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41 TP5] shape M:512, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:32:41] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:46684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] Prefill batch, #new-seq: 14, #new-token: 995, #cached-token: 9383, token usage: 0.11, #running-req: 1010, #queue-req: 108, 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:41 TP0] Prefill batch, #new-seq: 11, #new-token: 743, #cached-token: 7372, token usage: 0.11, #running-req: 1013, #queue-req: 97, 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:41 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42 TP0] Prefill batch, #new-seq: 14, #new-token: 721, #cached-token: 9381, token usage: 0.11, #running-req: 1010, #queue-req: 83, 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP1] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP7] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP3] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP2] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP4] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP6] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP5] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42] INFO:     127.0.0.1:46946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42 TP0] Prefill batch, #new-seq: 11, #new-token: 636, #cached-token: 7367, token usage: 0.11, #running-req: 1013, #queue-req: 72, 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP3] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP2] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP4] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP1] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP7] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP6] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP5] [fused_moe] using default for (636, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42] INFO:     127.0.0.1:46450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] Prefill batch, #new-seq: 13, #new-token: 910, #cached-token: 8706, token usage: 0.11, #running-req: 1011, #queue-req: 59, 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP1] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP3] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP7] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP2] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP4] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP6] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP5] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:53118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:42 TP0] Prefill batch, #new-seq: 21, #new-token: 1375, #cached-token: 14067, token usage: 0.11, #running-req: 1003, #queue-req: 38, 
[2025-10-27 15:32:42] INFO:     127.0.0.1:47592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:42 TP0] Prefill batch, #new-seq: 13, #new-token: 717, #cached-token: 8713, token usage: 0.11, #running-req: 1011, #queue-req: 25, 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP1] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP2] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP7] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP4] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP3] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP6] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP5] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43] INFO:     127.0.0.1:48938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43 TP0] Prefill batch, #new-seq: 8, #new-token: 541, #cached-token: 5359, token usage: 0.11, #running-req: 1016, #queue-req: 17, 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP1] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP7] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP3] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP2] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP4] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP6] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP5] [fused_moe] using default for (541, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:48144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:53506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43 TP0] Prefill batch, #new-seq: 14, #new-token: 846, #cached-token: 9378, token usage: 0.11, #running-req: 1010, #queue-req: 3, 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] Decode batch, #running-req: 1010, #token: 120931, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5562.95, #queue-req: 3, 
[2025-10-27 15:32:43] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:47630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43 TP0] Prefill batch, #new-seq: 3, #new-token: 138, #cached-token: 2014, token usage: 0.11, #running-req: 1015, #queue-req: 0, 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP3] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP1] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP2] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP4] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP7] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP6] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP5] [fused_moe] using default for (138, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:51204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:51692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:47752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:43] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP1] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP3] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP0] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP4] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP7] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP2] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP6] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:43 TP5] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (986, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:49148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (976, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (949, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (941, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (932, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (922, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:44] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:44] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (902, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (886, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (870, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (851, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (845, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:46696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (817, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45] INFO:     127.0.0.1:46926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:45] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP4] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP6] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP2] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP0] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP3] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP7] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP5] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:45 TP1] [fused_moe] using default for (802, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (783, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (772, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:46466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:48082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:46282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (708, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:46] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP4] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP0] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP6] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP3] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP2] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP1] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP5] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:46 TP7] [fused_moe] using default for (695, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (672, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (653, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:47618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (646, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] Decode batch, #running-req: 626, #token: 94974, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7699.18, #queue-req: 0, 
[2025-10-27 15:32:47] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:47] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:47 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:46534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (586, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (565, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:46782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (553, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (544, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:47160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP4] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP6] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP2] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP0] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP5] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP1] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP3] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48 TP7] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:32:48] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:47816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:56918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:48] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:47574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:49] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:54850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50 TP0] Decode batch, #running-req: 282, #token: 52767, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6138.64, #queue-req: 0, 
[2025-10-27 15:32:50] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:50] INFO:     127.0.0.1:47034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:51] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:53294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:48958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:48178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52 TP0] Decode batch, #running-req: 118, #token: 27443, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3981.50, #queue-req: 0, 
[2025-10-27 15:32:52] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:52] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:53728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:46840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:52662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:53 TP0] Decode batch, #running-req: 43, #token: 12670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1936.21, #queue-req: 0, 
[2025-10-27 15:32:53] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:54] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55 TP0] Decode batch, #running-req: 11, #token: 3975, token usage: 0.00, cuda graph: True, gen throughput (token/s): 918.17, #queue-req: 0, 
[2025-10-27 15:32:55] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:55 TP0] Decode batch, #running-req: 4, #token: 1707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 295.77, #queue-req: 0, 
[2025-10-27 15:32:56] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:56] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:32:56 TP0] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.70, #queue-req: 0, 
[2025-10-27 15:32:57 TP0] Decode batch, #running-req: 1, #token: 1144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.48, #queue-req: 0, 
[2025-10-27 15:32:58 TP0] Decode batch, #running-req: 1, #token: 1184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.46, #queue-req: 0, 
[2025-10-27 15:32:59 TP0] Decode batch, #running-req: 1, #token: 1224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.46, #queue-req: 0, 
[2025-10-27 15:33:00] INFO:     127.0.0.1:47140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:00 TP0] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.44, #queue-req: 0, 
[2025-10-27 15:33:12] INFO:     127.0.0.1:55722 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:33:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:33:13] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27536, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32671, token usage: 0.00, #running-req: 39, #queue-req: 0, 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP2] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP4] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP5] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP6] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP1] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP3] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP7] [fused_moe] using default for (45, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35789, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 38110, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40702, token usage: 0.01, #running-req: 185, #queue-req: 0, 
[2025-10-27 15:33:13 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42873, token usage: 0.02, #running-req: 241, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:13 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45165, token usage: 0.02, #running-req: 300, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47456, token usage: 0.02, #running-req: 362, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 69, #new-token: 69, #cached-token: 50166, token usage: 0.03, #running-req: 427, #queue-req: 0, 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52070, token usage: 0.03, #running-req: 496, #queue-req: 0, 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53900, token usage: 0.03, #running-req: 568, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 77, #new-token: 77, #cached-token: 55989, token usage: 0.04, #running-req: 642, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:14 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58400, token usage: 0.04, #running-req: 719, #queue-req: 0, 
[2025-10-27 15:33:15 TP0] Prefill batch, #new-seq: 83, #new-token: 83, #cached-token: 60398, token usage: 0.05, #running-req: 799, #queue-req: 0, 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP4] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP6] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP5] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP2] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP7] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP0] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP1] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP3] [fused_moe] using default for (83, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:15 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58440, token usage: 0.05, #running-req: 882, #queue-req: 0, 
[2025-10-27 15:33:15 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45531, token usage: 0.06, #running-req: 962, #queue-req: 28, 
[2025-10-27 15:33:18] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.08, #running-req: 1023, #queue-req: 294, 
[2025-10-27 15:33:18] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.09, #running-req: 1023, #queue-req: 293, 
[2025-10-27 15:33:19] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.09, #running-req: 1023, #queue-req: 292, 
[2025-10-27 15:33:19] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19 TP0] Decode batch, #running-req: 1024, #token: 103242, token usage: 0.09, cuda graph: False, gen throughput (token/s): 2062.70, #queue-req: 292, 
[2025-10-27 15:33:19] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:19 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1464, token usage: 0.09, #running-req: 1022, #queue-req: 290, 
[2025-10-27 15:33:20] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5963, token usage: 0.10, #running-req: 1016, #queue-req: 282, 
[2025-10-27 15:33:20] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2934, token usage: 0.10, #running-req: 1020, #queue-req: 278, 
[2025-10-27 15:33:20] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4338, token usage: 0.10, #running-req: 1018, #queue-req: 272, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3583, token usage: 0.10, #running-req: 1019, #queue-req: 267, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:20] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:60936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5073, token usage: 0.10, #running-req: 1017, #queue-req: 260, 
[2025-10-27 15:33:20] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:20] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.10, #running-req: 1022, #queue-req: 258, 
[2025-10-27 15:33:21] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:36268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7285, token usage: 0.10, #running-req: 1014, #queue-req: 248, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:58282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6682, token usage: 0.10, #running-req: 1015, #queue-req: 239, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:21] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6553, token usage: 0.10, #running-req: 1015, #queue-req: 230, 
[2025-10-27 15:33:21] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7311, token usage: 0.10, #running-req: 1014, #queue-req: 220, 
[2025-10-27 15:33:21] INFO:     127.0.0.1:57752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:21] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4356, token usage: 0.10, #running-req: 1018, #queue-req: 214, 
[2025-10-27 15:33:22] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6460, token usage: 0.10, #running-req: 1015, #queue-req: 205, 
[2025-10-27 15:33:22] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:60948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:35550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:36008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7266, token usage: 0.10, #running-req: 1014, #queue-req: 195, 
[2025-10-27 15:33:22] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3617, token usage: 0.10, #running-req: 1019, #queue-req: 190, 
[2025-10-27 15:33:22] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:34472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9461, token usage: 0.10, #running-req: 1011, #queue-req: 177, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:22] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:22] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7194, token usage: 0.11, #running-req: 1014, #queue-req: 167, 
[2025-10-27 15:33:23] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6515, token usage: 0.11, #running-req: 1015, #queue-req: 158, 
[2025-10-27 15:33:23] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5807, token usage: 0.11, #running-req: 1016, #queue-req: 150, 
[2025-10-27 15:33:23] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7381, token usage: 0.11, #running-req: 1014, #queue-req: 140, 
[2025-10-27 15:33:23] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3655, token usage: 0.11, #running-req: 1019, #queue-req: 135, 
[2025-10-27 15:33:23] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:58176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:23] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10185, token usage: 0.11, #running-req: 1010, #queue-req: 121, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:24 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8922, token usage: 0.11, #running-req: 1012, #queue-req: 109, 
[2025-10-27 15:33:24] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:34898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7374, token usage: 0.11, #running-req: 1014, #queue-req: 99, 
[2025-10-27 15:33:24] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:59734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5755, token usage: 0.11, #running-req: 1016, #queue-req: 91, 
[2025-10-27 15:33:24] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8689, token usage: 0.11, #running-req: 1012, #queue-req: 79, 
[2025-10-27 15:33:24] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:58402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:24] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5846, token usage: 0.11, #running-req: 1016, #queue-req: 71, 
[2025-10-27 15:33:25] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11077, token usage: 0.11, #running-req: 1009, #queue-req: 56, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:25] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:34352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:36032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11011, token usage: 0.11, #running-req: 1009, #queue-req: 41, 
[2025-10-27 15:33:25] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7277, token usage: 0.11, #running-req: 1014, #queue-req: 31, 
[2025-10-27 15:33:25] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:34872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8752, token usage: 0.11, #running-req: 1012, #queue-req: 19, 
[2025-10-27 15:33:25] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:25] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8053, token usage: 0.11, #running-req: 1013, #queue-req: 8, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5768, token usage: 0.11, #running-req: 1015, #queue-req: 0, 
[2025-10-27 15:33:26] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:33568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:26] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:26] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (955, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] Decode batch, #running-req: 970, #token: 120385, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5577.07, #queue-req: 0, 
[2025-10-27 15:33:27] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36586 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (937, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (925, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:32828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (876, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP4] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP6] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP2] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP0] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP3] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP5] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP1] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27 TP7] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:27] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:27] INFO:     127.0.0.1:35996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36580 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (838, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:56712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (776, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:37794 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP4] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP6] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP2] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP1] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP5] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP3] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP7] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28 TP0] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:28] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:32964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:28] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (737, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (731, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (671, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:38068 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (660, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:29] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP4] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP6] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP2] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP0] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP5] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP1] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP7] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:29 TP3] [fused_moe] using default for (656, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37894 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (631, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37398 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (571, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (558, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP4] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP7] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP5] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP3] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP1] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP0] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP2] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30 TP6] [fused_moe] using default for (545, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:30] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:33410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:30] INFO:     127.0.0.1:37892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31 TP0] Decode batch, #running-req: 545, #token: 84926, token usage: 0.08, cuda graph: False, gen throughput (token/s): 7643.87, #queue-req: 0, 
[2025-10-27 15:33:31] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:58294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:39152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:38390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:32842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:31] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:34110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:39228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:32] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33 TP0] Decode batch, #running-req: 237, #token: 47445, token usage: 0.04, cuda graph: True, gen throughput (token/s): 5965.61, #queue-req: 0, 
[2025-10-27 15:33:33] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:33] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:34720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:60866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:34194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:37440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:34] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35 TP0] Decode batch, #running-req: 95, #token: 22504, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3490.20, #queue-req: 0, 
[2025-10-27 15:33:35] INFO:     127.0.0.1:57124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:59520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:59686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:37324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:35] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:34826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:37514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36 TP0] Decode batch, #running-req: 31, #token: 9697, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1558.95, #queue-req: 0, 
[2025-10-27 15:33:36] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:36] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:38996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:39094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:37998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37 TP0] Decode batch, #running-req: 7, #token: 2640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 729.04, #queue-req: 0, 
[2025-10-27 15:33:37] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:37] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:38] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:38] INFO:     127.0.0.1:37974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:38] INFO:     127.0.0.1:38032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:38 TP0] Decode batch, #running-req: 1, #token: 1028, token usage: 0.00, cuda graph: True, gen throughput (token/s): 156.44, #queue-req: 0, 
[2025-10-27 15:33:38] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:51] INFO:     127.0.0.1:56720 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:33:51] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 39, #new-token: 39, #cached-token: 28258, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP5] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP6] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP4] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP1] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP7] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP2] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP3] [fused_moe] using default for (39, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34201, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36429, token usage: 0.01, #running-req: 87, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38885, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40642, token usage: 0.01, #running-req: 190, #queue-req: 0, 
[2025-10-27 15:33:51 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42924, token usage: 0.02, #running-req: 246, #queue-req: 0, 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44450, token usage: 0.02, #running-req: 305, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 64, #new-token: 64, #cached-token: 46706, token usage: 0.02, #running-req: 366, #queue-req: 0, 
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 47858, token usage: 0.03, #running-req: 430, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 50719, token usage: 0.03, #running-req: 496, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51715, token usage: 0.03, #running-req: 566, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53886, token usage: 0.04, #running-req: 637, #queue-req: 0, 
[2025-10-27 15:33:52 TP0] Prefill batch, #new-seq: 76, #new-token: 76, #cached-token: 55362, token usage: 0.04, #running-req: 711, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:52 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP0] Prefill batch, #new-seq: 78, #new-token: 78, #cached-token: 56898, token usage: 0.05, #running-req: 787, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58280, token usage: 0.05, #running-req: 865, #queue-req: 0, 
[2025-10-27 15:33:53 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49671, token usage: 0.06, #running-req: 945, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:53 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8262, token usage: 0.06, #running-req: 1013, #queue-req: 3, 
[2025-10-27 15:33:56] INFO:     127.0.0.1:37062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-27 15:33:57 TP0] Decode batch, #running-req: 1024, #token: 99557, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1968.27, #queue-req: 294, 
[2025-10-27 15:33:57] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.09, #running-req: 1023, #queue-req: 293, 
[2025-10-27 15:33:57] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2238, token usage: 0.09, #running-req: 1021, #queue-req: 290, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:33:57] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2251, token usage: 0.09, #running-req: 1021, #queue-req: 287, 
[2025-10-27 15:33:58] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:34040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2902, token usage: 0.10, #running-req: 1020, #queue-req: 283, 
[2025-10-27 15:33:58] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3645, token usage: 0.10, #running-req: 1019, #queue-req: 278, 
[2025-10-27 15:33:58] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:33600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:37424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9495, token usage: 0.10, #running-req: 1011, #queue-req: 265, 
[2025-10-27 15:33:58] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:37170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4297, token usage: 0.10, #running-req: 1018, #queue-req: 259, 
[2025-10-27 15:33:58] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:58 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3592, token usage: 0.10, #running-req: 1019, #queue-req: 254, 
[2025-10-27 15:33:59] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2901, token usage: 0.10, #running-req: 1020, #queue-req: 250, 
[2025-10-27 15:33:59] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5138, token usage: 0.10, #running-req: 1017, #queue-req: 243, 
[2025-10-27 15:33:59] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7342, token usage: 0.10, #running-req: 1014, #queue-req: 233, 
[2025-10-27 15:33:59 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5923, token usage: 0.10, #running-req: 1016, #queue-req: 225, 
[2025-10-27 15:33:59 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5105, token usage: 0.10, #running-req: 1017, #queue-req: 218, 
[2025-10-27 15:33:59] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:33:59] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:33328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:33952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7187, token usage: 0.10, #running-req: 1014, #queue-req: 208, 
[2025-10-27 15:34:00] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7253, token usage: 0.10, #running-req: 1014, #queue-req: 198, 
[2025-10-27 15:34:00] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5116, token usage: 0.10, #running-req: 1017, #queue-req: 191, 
[2025-10-27 15:34:00] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3616, token usage: 0.10, #running-req: 1019, #queue-req: 186, 
[2025-10-27 15:34:00] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:58396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:34838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:36798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:00 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10930, token usage: 0.10, #running-req: 1009, #queue-req: 171, 
[2025-10-27 15:34:01] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:59356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7865, token usage: 0.10, #running-req: 1013, #queue-req: 160, 
[2025-10-27 15:34:01] INFO:     127.0.0.1:57558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5067, token usage: 0.11, #running-req: 1017, #queue-req: 153, 
[2025-10-27 15:34:01] INFO:     127.0.0.1:57086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:34976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5138, token usage: 0.11, #running-req: 1017, #queue-req: 146, 
[2025-10-27 15:34:01] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:34790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3743, token usage: 0.11, #running-req: 1019, #queue-req: 141, 
[2025-10-27 15:34:01] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:35530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:01 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5785, token usage: 0.11, #running-req: 1016, #queue-req: 133, 
[2025-10-27 15:34:02] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8037, token usage: 0.11, #running-req: 1013, #queue-req: 122, 
[2025-10-27 15:34:02] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6636, token usage: 0.11, #running-req: 1015, #queue-req: 113, 
[2025-10-27 15:34:02] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8157, token usage: 0.11, #running-req: 1013, #queue-req: 102, 
[2025-10-27 15:34:02] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7996, token usage: 0.11, #running-req: 1013, #queue-req: 91, 
[2025-10-27 15:34:02] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:02 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7942, token usage: 0.11, #running-req: 1013, #queue-req: 80, 
[2025-10-27 15:34:03] INFO:     127.0.0.1:57102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4380, token usage: 0.11, #running-req: 1018, #queue-req: 74, 
[2025-10-27 15:34:03] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9621, token usage: 0.11, #running-req: 1011, #queue-req: 61, 
[2025-10-27 15:34:03] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12489, token usage: 0.11, #running-req: 1007, #queue-req: 44, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:03] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9444, token usage: 0.11, #running-req: 1011, #queue-req: 31, 
[2025-10-27 15:34:03] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03] INFO:     127.0.0.1:36418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:03 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7263, token usage: 0.11, #running-req: 1014, #queue-req: 21, 
[2025-10-27 15:34:04] INFO:     127.0.0.1:57022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10262, token usage: 0.11, #running-req: 1010, #queue-req: 7, 
[2025-10-27 15:34:04] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5050, token usage: 0.11, #running-req: 1011, #queue-req: 0, 
[2025-10-27 15:34:04] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP1] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP3] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP0] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP6] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP4] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP2] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP7] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP5] [fused_moe] using default for (1001, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP0] Decode batch, #running-req: 1001, #token: 121350, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5430.75, #queue-req: 0, 
[2025-10-27 15:34:04] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:35842 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP4] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP6] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP0] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP3] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP2] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP7] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP5] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP1] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:04] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP3] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP1] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP4] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP5] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP7] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP0] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP2] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:04 TP6] [fused_moe] using default for (974, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (951, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (947, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP1] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP4] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP5] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP3] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP7] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP6] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP0] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05 TP2] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:05] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:35558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:05] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (825, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:38300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:38512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:37366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:06] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP4] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP5] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP1] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP6] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP0] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP2] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP3] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:06 TP7] [fused_moe] using default for (765, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:32804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP7] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP3] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP1] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP4] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP5] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP6] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP0] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP2] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:39130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:39246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP3] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP5] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP1] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP4] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP7] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP6] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP0] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP2] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:07] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:07 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37142 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (583, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] Decode batch, #running-req: 598, #token: 90816, token usage: 0.08, cuda graph: False, gen throughput (token/s): 8039.66, #queue-req: 0, 
[2025-10-27 15:34:08] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:40086 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (572, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39658 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (559, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:35624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:08] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:08 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:09] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:56756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:40336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:09] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:35504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:33280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:10] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:36902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11 TP0] Decode batch, #running-req: 260, #token: 50605, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6049.59, #queue-req: 0, 
[2025-10-27 15:34:11] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:60196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:35692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:37878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:11] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:60840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:37186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:37946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:35928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:40190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:37952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:12] INFO:     127.0.0.1:39420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:33568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:37854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13 TP0] Decode batch, #running-req: 102, #token: 25409, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3726.51, #queue-req: 0, 
[2025-10-27 15:34:13] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:37642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:40238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:33648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:37828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:39780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:13] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14 TP0] Decode batch, #running-req: 38, #token: 11434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1791.67, #queue-req: 0, 
[2025-10-27 15:34:14] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:38482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:14] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:35760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:38418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:38552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:40014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15 TP0] Decode batch, #running-req: 10, #token: 3914, token usage: 0.00, cuda graph: True, gen throughput (token/s): 876.22, #queue-req: 0, 
[2025-10-27 15:34:15] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:15] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:39824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:37800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16 TP0] Decode batch, #running-req: 2, #token: 1322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 228.51, #queue-req: 0, 
[2025-10-27 15:34:16] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:16] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:29] INFO:     127.0.0.1:54126 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:34:29] INFO:     127.0.0.1:54140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 39, #new-token: 39, #cached-token: 28366, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33335, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35835, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[2025-10-27 15:34:29 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39503, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 39861, token usage: 0.01, #running-req: 189, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43715, token usage: 0.02, #running-req: 244, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43720, token usage: 0.02, #running-req: 304, #queue-req: 0, 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48160, token usage: 0.02, #running-req: 364, #queue-req: 0, 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 47965, token usage: 0.03, #running-req: 430, #queue-req: 0, 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 77, #new-token: 77, #cached-token: 55724, token usage: 0.03, #running-req: 496, #queue-req: 0, 
[2025-10-27 15:34:30 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49539, token usage: 0.03, #running-req: 573, #queue-req: 0, 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 83, #new-token: 83, #cached-token: 60411, token usage: 0.04, #running-req: 641, #queue-req: 0, 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51721, token usage: 0.04, #running-req: 724, #queue-req: 0, 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 89, #new-token: 89, #cached-token: 64893, token usage: 0.05, #running-req: 795, #queue-req: 0, 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP4] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP5] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP6] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP7] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP0] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP2] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP1] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP3] [fused_moe] using default for (89, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 62, #new-token: 62, #cached-token: 45194, token usage: 0.05, #running-req: 884, #queue-req: 0, 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP0] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP4] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP1] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP5] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP2] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP6] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP3] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP7] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11723, token usage: 0.05, #running-req: 946, #queue-req: 0, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 44153, token usage: 0.06, #running-req: 962, #queue-req: 0, 
[2025-10-27 15:34:31 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1505, token usage: 0.06, #running-req: 1022, #queue-req: 47, 
[2025-10-27 15:34:34] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.08, #running-req: 1023, #queue-req: 294, 
[2025-10-27 15:34:35 TP0] Decode batch, #running-req: 1024, #token: 95497, token usage: 0.09, cuda graph: False, gen throughput (token/s): 1750.32, #queue-req: 294, 
[2025-10-27 15:34:35] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.09, #running-req: 1023, #queue-req: 293, 
[2025-10-27 15:34:35] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.09, #running-req: 1023, #queue-req: 292, 
[2025-10-27 15:34:36] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2195, token usage: 0.09, #running-req: 1021, #queue-req: 289, 
[2025-10-27 15:34:36] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2138, token usage: 0.09, #running-req: 1021, #queue-req: 286, 
[2025-10-27 15:34:36] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:55916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5149, token usage: 0.09, #running-req: 1017, #queue-req: 279, 
[2025-10-27 15:34:36] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2196, token usage: 0.10, #running-req: 1021, #queue-req: 276, 
[2025-10-27 15:34:36] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:36] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4323, token usage: 0.10, #running-req: 1018, #queue-req: 270, 
[2025-10-27 15:34:37] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2863, token usage: 0.10, #running-req: 1020, #queue-req: 266, 
[2025-10-27 15:34:37] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5783, token usage: 0.10, #running-req: 1016, #queue-req: 258, 
[2025-10-27 15:34:37] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2897, token usage: 0.10, #running-req: 1020, #queue-req: 254, 
[2025-10-27 15:34:37] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5841, token usage: 0.10, #running-req: 1016, #queue-req: 246, 
[2025-10-27 15:34:37] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:60682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:37] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5948, token usage: 0.10, #running-req: 1016, #queue-req: 238, 
[2025-10-27 15:34:38] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:35202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5816, token usage: 0.10, #running-req: 1016, #queue-req: 230, 
[2025-10-27 15:34:38] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5150, token usage: 0.10, #running-req: 1017, #queue-req: 223, 
[2025-10-27 15:34:38] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:60638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:34158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5096, token usage: 0.10, #running-req: 1017, #queue-req: 216, 
[2025-10-27 15:34:38] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:60814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:34992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7873, token usage: 0.10, #running-req: 1013, #queue-req: 205, 
[2025-10-27 15:34:38] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:38] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8012, token usage: 0.10, #running-req: 1013, #queue-req: 194, 
[2025-10-27 15:34:39] INFO:     127.0.0.1:55266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3624, token usage: 0.10, #running-req: 1019, #queue-req: 189, 
[2025-10-27 15:34:39] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8024, token usage: 0.10, #running-req: 1013, #queue-req: 178, 
[2025-10-27 15:34:39] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:59920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7904, token usage: 0.11, #running-req: 1013, #queue-req: 167, 
[2025-10-27 15:34:39] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6515, token usage: 0.11, #running-req: 1015, #queue-req: 158, 
[2025-10-27 15:34:39] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:39] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5807, token usage: 0.11, #running-req: 1016, #queue-req: 150, 
[2025-10-27 15:34:40] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3665, token usage: 0.11, #running-req: 1019, #queue-req: 145, 
[2025-10-27 15:34:40] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:32892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7294, token usage: 0.11, #running-req: 1014, #queue-req: 135, 
[2025-10-27 15:34:40] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:54524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8838, token usage: 0.11, #running-req: 1012, #queue-req: 123, 
[2025-10-27 15:34:40] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7350, token usage: 0.11, #running-req: 1014, #queue-req: 113, 
[2025-10-27 15:34:40] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:40] INFO:     127.0.0.1:33644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3603, token usage: 0.11, #running-req: 1019, #queue-req: 108, 
[2025-10-27 15:34:41] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8938, token usage: 0.11, #running-req: 1012, #queue-req: 96, 
[2025-10-27 15:34:41] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9371, token usage: 0.11, #running-req: 1011, #queue-req: 83, 
[2025-10-27 15:34:41] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:60090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8748, token usage: 0.11, #running-req: 1012, #queue-req: 71, 
[2025-10-27 15:34:41] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8849, token usage: 0.11, #running-req: 1012, #queue-req: 59, 
[2025-10-27 15:34:41] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:59952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:41] INFO:     127.0.0.1:33998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8890, token usage: 0.11, #running-req: 1012, #queue-req: 47, 
[2025-10-27 15:34:42] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10900, token usage: 0.11, #running-req: 1009, #queue-req: 32, 
[2025-10-27 15:34:42] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8753, token usage: 0.11, #running-req: 1012, #queue-req: 20, 
[2025-10-27 15:34:42] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8785, token usage: 0.11, #running-req: 1012, #queue-req: 8, 
[2025-10-27 15:34:42 TP0] Decode batch, #running-req: 1012, #token: 121047, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5528.29, #queue-req: 8, 
[2025-10-27 15:34:42] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:35118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5761, token usage: 0.11, #running-req: 1014, #queue-req: 0, 
[2025-10-27 15:34:42] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:42] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34880 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP4] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP5] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP1] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP6] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP2] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP0] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP7] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP3] [fused_moe] using default for (943, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:43] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:43 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (909, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:32772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP2] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP4] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP6] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP0] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP5] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP1] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP3] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44 TP7] [fused_moe] using default for (850, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:44] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:44] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP4] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP6] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP2] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP0] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP5] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP1] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP3] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP7] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP6] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP4] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP2] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP0] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP5] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP1] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP3] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP7] [fused_moe] using default for (755, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35150 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP4] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP6] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP2] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP0] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP5] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP1] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP3] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP7] [fused_moe] using default for (730, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:55396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:45] INFO:     127.0.0.1:54254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:45] INFO:     127.0.0.1:33840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (692, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:32870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (659, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP2] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP4] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP6] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP0] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP5] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP1] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP3] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46 TP7] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:46] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:58494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46 TP0] Decode batch, #running-req: 628, #token: 94492, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7929.64, #queue-req: 0, 
[2025-10-27 15:34:46] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:46] INFO:     127.0.0.1:36850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:37500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:34:47] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:47] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:34412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:34432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:48] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49 TP0] Decode batch, #running-req: 290, #token: 53619, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6095.73, #queue-req: 0, 
[2025-10-27 15:34:49] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:60652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:33440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:49] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:33392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:34732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:58460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:35370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:36722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:50] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:34238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51 TP0] Decode batch, #running-req: 116, #token: 25933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3971.66, #queue-req: 0, 
[2025-10-27 15:34:51] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:33360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:51] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:60796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:58632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:52] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53 TP0] Decode batch, #running-req: 33, #token: 9267, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1797.51, #queue-req: 0, 
[2025-10-27 15:34:53] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:37550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:35234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:36194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:53] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54 TP0] Decode batch, #running-req: 12, #token: 4219, token usage: 0.00, cuda graph: True, gen throughput (token/s): 795.30, #queue-req: 0, 
[2025-10-27 15:34:54] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:37386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:54] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:55 TP0] Decode batch, #running-req: 2, #token: 1356, token usage: 0.00, cuda graph: True, gen throughput (token/s): 264.20, #queue-req: 0, 
[2025-10-27 15:34:55] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:34:55 TP0] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 58.34, #queue-req: 0, 
[2025-10-27 15:34:55] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:08] INFO:     127.0.0.1:34880 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:35:08] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27551, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 45, #new-token: 45, #cached-token: 32665, token usage: 0.00, #running-req: 39, #queue-req: 0, 
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 35091, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[2025-10-27 15:35:08 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38715, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40040, token usage: 0.01, #running-req: 185, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42124, token usage: 0.02, #running-req: 240, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44464, token usage: 0.02, #running-req: 298, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45896, token usage: 0.02, #running-req: 359, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48126, token usage: 0.03, #running-req: 422, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 50629, token usage: 0.03, #running-req: 488, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52432, token usage: 0.03, #running-req: 558, #queue-req: 0, 
[2025-10-27 15:35:09 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58188, token usage: 0.04, #running-req: 630, #queue-req: 0, 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51735, token usage: 0.04, #running-req: 710, #queue-req: 0, 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 49, #new-token: 49, #cached-token: 35891, token usage: 0.05, #running-req: 781, #queue-req: 0, 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 29, #new-token: 29, #cached-token: 21045, token usage: 0.05, #running-req: 830, #queue-req: 0, 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP4] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP1] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP5] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP3] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP7] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP0] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP2] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP6] [fused_moe] using default for (29, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 51, #new-token: 51, #cached-token: 36881, token usage: 0.05, #running-req: 859, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36664, token usage: 0.05, #running-req: 910, #queue-req: 0, 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 44051, token usage: 0.06, #running-req: 960, #queue-req: 0, 
[2025-10-27 15:35:10 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2938, token usage: 0.06, #running-req: 1020, #queue-req: 53, 
[2025-10-27 15:35:13] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 745, token usage: 0.08, #running-req: 1023, #queue-req: 294, 
[2025-10-27 15:35:14] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.09, #running-req: 1023, #queue-req: 293, 
[2025-10-27 15:35:14] INFO:     127.0.0.1:37742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 746, token usage: 0.09, #running-req: 1023, #queue-req: 292, 
[2025-10-27 15:35:15 TP0] Decode batch, #running-req: 1024, #token: 102111, token usage: 0.09, cuda graph: False, gen throughput (token/s): 2056.10, #queue-req: 292, 
[2025-10-27 15:35:15] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:38468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2984, token usage: 0.09, #running-req: 1020, #queue-req: 288, 
[2025-10-27 15:35:15] INFO:     127.0.0.1:36048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:37568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4443, token usage: 0.09, #running-req: 1018, #queue-req: 282, 
[2025-10-27 15:35:15] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2210, token usage: 0.10, #running-req: 1021, #queue-req: 279, 
[2025-10-27 15:35:15] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:35658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:39750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:15 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5062, token usage: 0.10, #running-req: 1017, #queue-req: 272, 
[2025-10-27 15:35:16] INFO:     127.0.0.1:36506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1457, token usage: 0.10, #running-req: 1022, #queue-req: 270, 
[2025-10-27 15:35:16] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:37080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:38020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:43438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5785, token usage: 0.10, #running-req: 1016, #queue-req: 262, 
[2025-10-27 15:35:16] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:39302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4288, token usage: 0.10, #running-req: 1018, #queue-req: 256, 
[2025-10-27 15:35:16] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:39664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2883, token usage: 0.10, #running-req: 1020, #queue-req: 252, 
[2025-10-27 15:35:16] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:37146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:16 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7310, token usage: 0.10, #running-req: 1014, #queue-req: 242, 
[2025-10-27 15:35:17] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:39424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:43920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6627, token usage: 0.10, #running-req: 1015, #queue-req: 233, 
[2025-10-27 15:35:17] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:39550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4448, token usage: 0.10, #running-req: 1018, #queue-req: 227, 
[2025-10-27 15:35:17] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:39036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:41182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7291, token usage: 0.10, #running-req: 1014, #queue-req: 217, 
[2025-10-27 15:35:17] INFO:     127.0.0.1:35910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5048, token usage: 0.10, #running-req: 1017, #queue-req: 210, 
[2025-10-27 15:35:17] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:17 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5028, token usage: 0.10, #running-req: 1017, #queue-req: 203, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3653, token usage: 0.10, #running-req: 1019, #queue-req: 198, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:34894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:39692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7289, token usage: 0.10, #running-req: 1014, #queue-req: 188, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:35702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:38216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:40452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7987, token usage: 0.11, #running-req: 1013, #queue-req: 177, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:37246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:38150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:39648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6491, token usage: 0.11, #running-req: 1015, #queue-req: 168, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4299, token usage: 0.11, #running-req: 1018, #queue-req: 162, 
[2025-10-27 15:35:18] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:39294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:18] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4352, token usage: 0.11, #running-req: 1018, #queue-req: 156, 
[2025-10-27 15:35:19] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7314, token usage: 0.11, #running-req: 1014, #queue-req: 146, 
[2025-10-27 15:35:19] INFO:     127.0.0.1:35008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:35334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:38332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10951, token usage: 0.11, #running-req: 1009, #queue-req: 131, 
[2025-10-27 15:35:19] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4397, token usage: 0.11, #running-req: 1018, #queue-req: 125, 
[2025-10-27 15:35:19] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:38366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6656, token usage: 0.11, #running-req: 1015, #queue-req: 116, 
[2025-10-27 15:35:19] INFO:     127.0.0.1:35678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:36240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:19] INFO:     127.0.0.1:41284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5199, token usage: 0.11, #running-req: 1017, #queue-req: 109, 
[2025-10-27 15:35:20] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:35480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:38626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10972, token usage: 0.11, #running-req: 1009, #queue-req: 94, 
[2025-10-27 15:35:20] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:35664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:43148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:43944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8653, token usage: 0.11, #running-req: 1012, #queue-req: 82, 
[2025-10-27 15:35:20] INFO:     127.0.0.1:35056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:41244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8818, token usage: 0.11, #running-req: 1012, #queue-req: 70, 
[2025-10-27 15:35:20] INFO:     127.0.0.1:35712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:37280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:38228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:38604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:41702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 11078, token usage: 0.11, #running-req: 1009, #queue-req: 55, 
[2025-10-27 15:35:20] INFO:     127.0.0.1:35718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:20] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21 TP0] Prefill batch, #new-seq: 21, #new-token: 21, #cached-token: 15332, token usage: 0.11, #running-req: 1003, #queue-req: 34, 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP0] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP2] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP3] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP5] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP1] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP7] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP4] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21 TP6] [fused_moe] using default for (21, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:21] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:37634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:37960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6498, token usage: 0.11, #running-req: 1015, #queue-req: 25, 
[2025-10-27 15:35:21] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10332, token usage: 0.11, #running-req: 1010, #queue-req: 11, 
[2025-10-27 15:35:21] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:36672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:37800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7919, token usage: 0.11, #running-req: 1013, #queue-req: 0, 
[2025-10-27 15:35:21] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:35090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:38058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:40858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:21] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43552 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (979, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] Decode batch, #running-req: 974, #token: 120491, token usage: 0.11, cuda graph: False, gen throughput (token/s): 5582.81, #queue-req: 0, 
[2025-10-27 15:35:22] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:37648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:35916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22] INFO:     127.0.0.1:35332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:38478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:42406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:22] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP1] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP5] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP4] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP7] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP3] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP0] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP2] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:22 TP6] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP7] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP4] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP1] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP3] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP5] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP0] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP2] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP6] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP5] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP1] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP7] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP3] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP4] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP0] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP6] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP2] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP7] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP4] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP5] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP1] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP3] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP0] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP2] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP6] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:36422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP5] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP1] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP4] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP3] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP7] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP0] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP2] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP6] [fused_moe] using default for (829, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:36492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:44070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP1] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP3] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP5] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP4] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP7] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP0] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP2] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23 TP6] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:23] INFO:     127.0.0.1:37400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:23] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (800, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:35068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:45076 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:43720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:34978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:36948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:24] INFO:     127.0.0.1:44756 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP7] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP1] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP4] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP5] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP3] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP0] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP6] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:24 TP2] [fused_moe] using default for (694, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:46198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP1] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP3] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP5] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP7] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP4] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP0] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP2] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP6] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45406 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP1] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP5] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP4] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP3] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP7] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP0] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP2] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP6] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP1] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP3] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP5] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP4] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP7] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP0] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP2] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP6] [fused_moe] using default for (596, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25] INFO:     127.0.0.1:35012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:36572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:38894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:39056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:40954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:42814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:25] INFO:     127.0.0.1:44664 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:25 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:46242 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP4] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP3] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP1] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP7] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP5] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP0] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP2] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP6] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP0] Decode batch, #running-req: 551, #token: 85414, token usage: 0.08, cuda graph: False, gen throughput (token/s): 7665.63, #queue-req: 0, 
[2025-10-27 15:35:26] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP5] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP3] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP1] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP4] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP7] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP0] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP2] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP6] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26] INFO:     127.0.0.1:38268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP5] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP1] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP3] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP4] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP7] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP0] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP2] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26 TP6] [fused_moe] using default for (520, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-27 15:35:26] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:41560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:37718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:26] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:35592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:38102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:44922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:27] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:35680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:41152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:43180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:40166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:39018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:43322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:35490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:35958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:46180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:34986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:35498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:37910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:28] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29 TP0] Decode batch, #running-req: 252, #token: 49803, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5966.17, #queue-req: 0, 
[2025-10-27 15:35:29] INFO:     127.0.0.1:46344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:41472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:39144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:41460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:35316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:40404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:35666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:43670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:45632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:29] INFO:     127.0.0.1:46276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:37416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:36394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:39100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:34994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:35848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:38086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30 TP0] Decode batch, #running-req: 95, #token: 23789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3552.54, #queue-req: 0, 
[2025-10-27 15:35:30] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:37286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:30] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:41326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:35578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:44588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:31] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32 TP0] Decode batch, #running-req: 35, #token: 10801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1682.20, #queue-req: 0, 
[2025-10-27 15:35:32] INFO:     127.0.0.1:37542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:36472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:38588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:45328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:32] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:46130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:46088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33 TP0] Decode batch, #running-req: 10, #token: 3983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 783.05, #queue-req: 0, 
[2025-10-27 15:35:33] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:33] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:34] INFO:     127.0.0.1:37816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:34] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:34 TP0] Decode batch, #running-req: 4, #token: 1430, token usage: 0.00, cuda graph: True, gen throughput (token/s): 296.95, #queue-req: 0, 
[2025-10-27 15:35:34] INFO:     127.0.0.1:45376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:34] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:35 TP0] Decode batch, #running-req: 1, #token: 1111, token usage: 0.00, cuda graph: True, gen throughput (token/s): 57.00, #queue-req: 0, 
[2025-10-27 15:35:35 TP0] Decode batch, #running-req: 1, #token: 1151, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.47, #queue-req: 0, 
[2025-10-27 15:35:36 TP0] Decode batch, #running-req: 1, #token: 1191, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.48, #queue-req: 0, 
[2025-10-27 15:35:37 TP0] Decode batch, #running-req: 1, #token: 1231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.46, #queue-req: 0, 
[2025-10-27 15:35:38] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:49] INFO:     127.0.0.1:58134 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-27 15:35:55] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:55 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:35:56 TP0] Decode batch, #running-req: 1, #token: 3209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.14, #queue-req: 0, 
[2025-10-27 15:35:57] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:35:57] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-10-27 15:35:57] INFO:     127.0.0.1:35546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 21, 
[2025-10-27 15:35:57] INFO:     127.0.0.1:35716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:57] INFO:     127.0.0.1:35776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58] INFO:     127.0.0.1:36490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:35:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.03, #running-req: 11, #queue-req: 112, 
[2025-10-27 15:35:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-27 15:36:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.06, #running-req: 21, #queue-req: 102, 
[2025-10-27 15:36:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-10-27 15:36:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.09, #running-req: 31, #queue-req: 92, 
[2025-10-27 15:36:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.10, #running-req: 36, #queue-req: 87, 
[2025-10-27 15:36:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.12, #running-req: 41, #queue-req: 82, 
[2025-10-27 15:36:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 46, #queue-req: 77, 
[2025-10-27 15:36:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.15, #running-req: 51, #queue-req: 72, 
[2025-10-27 15:36:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.16, #running-req: 56, #queue-req: 67, 
[2025-10-27 15:36:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.18, #running-req: 61, #queue-req: 62, 
[2025-10-27 15:36:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.19, #running-req: 66, #queue-req: 57, 
[2025-10-27 15:36:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.21, #running-req: 71, #queue-req: 52, 
[2025-10-27 15:36:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.22, #running-req: 76, #queue-req: 47, 
[2025-10-27 15:36:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.24, #running-req: 81, #queue-req: 42, 
[2025-10-27 15:36:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.25, #running-req: 86, #queue-req: 37, 
[2025-10-27 15:36:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.26, #running-req: 91, #queue-req: 32, 
[2025-10-27 15:36:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.28, #running-req: 96, #queue-req: 27, 
[2025-10-27 15:36:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 101, #queue-req: 22, 
[2025-10-27 15:36:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.31, #running-req: 106, #queue-req: 17, 
[2025-10-27 15:36:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.32, #running-req: 111, #queue-req: 12, 
[2025-10-27 15:36:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.34, #running-req: 116, #queue-req: 7, 
[2025-10-27 15:36:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.35, #running-req: 121, #queue-req: 2, 
[2025-10-27 15:36:17 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.37, #running-req: 126, #queue-req: 0, 
[2025-10-27 15:36:19 TP0] Decode batch, #running-req: 128, #token: 411423, token usage: 0.37, cuda graph: True, gen throughput (token/s): 84.68, #queue-req: 0, 
[2025-10-27 15:36:21 TP0] Decode batch, #running-req: 128, #token: 416543, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2553.90, #queue-req: 0, 
[2025-10-27 15:36:23 TP0] Decode batch, #running-req: 128, #token: 421663, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2524.63, #queue-req: 0, 
[2025-10-27 15:36:25 TP0] Decode batch, #running-req: 128, #token: 426783, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2512.27, #queue-req: 0, 
[2025-10-27 15:36:27 TP0] Decode batch, #running-req: 128, #token: 431903, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2500.27, #queue-req: 0, 
[2025-10-27 15:36:29 TP0] Decode batch, #running-req: 128, #token: 437023, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2493.24, #queue-req: 0, 
[2025-10-27 15:36:31 TP0] Decode batch, #running-req: 128, #token: 442143, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2486.68, #queue-req: 0, 
[2025-10-27 15:36:33 TP0] Decode batch, #running-req: 128, #token: 447263, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2480.92, #queue-req: 0, 
[2025-10-27 15:36:35 TP0] Decode batch, #running-req: 128, #token: 452383, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2475.35, #queue-req: 0, 
[2025-10-27 15:36:37 TP0] Decode batch, #running-req: 128, #token: 457503, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2468.13, #queue-req: 0, 
[2025-10-27 15:36:39 TP0] Decode batch, #running-req: 128, #token: 462623, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2462.20, #queue-req: 0, 
[2025-10-27 15:36:41 TP0] Decode batch, #running-req: 128, #token: 467743, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2459.28, #queue-req: 0, 
[2025-10-27 15:36:43 TP0] Decode batch, #running-req: 128, #token: 472863, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2452.39, #queue-req: 0, 
[2025-10-27 15:36:45 TP0] Decode batch, #running-req: 128, #token: 477983, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2447.19, #queue-req: 0, 
[2025-10-27 15:36:48 TP0] Decode batch, #running-req: 128, #token: 483103, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2441.75, #queue-req: 0, 
[2025-10-27 15:36:50 TP0] Decode batch, #running-req: 128, #token: 488223, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2432.16, #queue-req: 0, 
[2025-10-27 15:36:52 TP0] Decode batch, #running-req: 128, #token: 493343, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2430.35, #queue-req: 0, 
[2025-10-27 15:36:54 TP0] Decode batch, #running-req: 128, #token: 498463, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2426.37, #queue-req: 0, 
[2025-10-27 15:36:56 TP0] Decode batch, #running-req: 128, #token: 503583, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2422.56, #queue-req: 0, 
[2025-10-27 15:36:58 TP0] Decode batch, #running-req: 128, #token: 508703, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2417.16, #queue-req: 0, 
[2025-10-27 15:36:59] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:36:59] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:36:59] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-10-27 15:37:00] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-10-27 15:37:00] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:00] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:37:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.03, #running-req: 11, #queue-req: 112, 
[2025-10-27 15:37:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-27 15:37:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.06, #running-req: 21, #queue-req: 102, 
[2025-10-27 15:37:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-10-27 15:37:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 31, #queue-req: 92, 
[2025-10-27 15:37:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 36, #queue-req: 87, 
[2025-10-27 15:37:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.12, #running-req: 41, #queue-req: 82, 
[2025-10-27 15:37:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 46, #queue-req: 77, 
[2025-10-27 15:37:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 51, #queue-req: 72, 
[2025-10-27 15:37:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.16, #running-req: 56, #queue-req: 67, 
[2025-10-27 15:37:09 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.18, #running-req: 61, #queue-req: 61, 
[2025-10-27 15:37:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.19, #running-req: 67, #queue-req: 56, 
[2025-10-27 15:37:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.21, #running-req: 72, #queue-req: 51, 
[2025-10-27 15:37:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 77, #queue-req: 46, 
[2025-10-27 15:37:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.24, #running-req: 82, #queue-req: 41, 
[2025-10-27 15:37:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.25, #running-req: 87, #queue-req: 36, 
[2025-10-27 15:37:14 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.27, #running-req: 92, #queue-req: 30, 
[2025-10-27 15:37:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 98, #queue-req: 25, 
[2025-10-27 15:37:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.30, #running-req: 103, #queue-req: 20, 
[2025-10-27 15:37:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.31, #running-req: 108, #queue-req: 15, 
[2025-10-27 15:37:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.33, #running-req: 113, #queue-req: 10, 
[2025-10-27 15:37:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.34, #running-req: 118, #queue-req: 5, 
[2025-10-27 15:37:19 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.35, #running-req: 123, #queue-req: 0, 
[2025-10-27 15:37:21 TP0] Decode batch, #running-req: 128, #token: 405021, token usage: 0.37, cuda graph: True, gen throughput (token/s): 226.27, #queue-req: 0, 
[2025-10-27 15:37:23 TP0] Decode batch, #running-req: 128, #token: 410141, token usage: 0.37, cuda graph: True, gen throughput (token/s): 2533.10, #queue-req: 0, 
[2025-10-27 15:37:25 TP0] Decode batch, #running-req: 128, #token: 415261, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2508.29, #queue-req: 0, 
[2025-10-27 15:37:27 TP0] Decode batch, #running-req: 128, #token: 420381, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2496.87, #queue-req: 0, 
[2025-10-27 15:37:29 TP0] Decode batch, #running-req: 128, #token: 425501, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2490.15, #queue-req: 0, 
[2025-10-27 15:37:31 TP0] Decode batch, #running-req: 128, #token: 430621, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2481.47, #queue-req: 0, 
[2025-10-27 15:37:33 TP0] Decode batch, #running-req: 128, #token: 435741, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2475.61, #queue-req: 0, 
[2025-10-27 15:37:35 TP0] Decode batch, #running-req: 128, #token: 440861, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2470.92, #queue-req: 0, 
[2025-10-27 15:37:37 TP0] Decode batch, #running-req: 128, #token: 445981, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2465.10, #queue-req: 0, 
[2025-10-27 15:37:39 TP0] Decode batch, #running-req: 128, #token: 451101, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2460.99, #queue-req: 0, 
[2025-10-27 15:37:41 TP0] Decode batch, #running-req: 128, #token: 456221, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2455.32, #queue-req: 0, 
[2025-10-27 15:37:43 TP0] Decode batch, #running-req: 128, #token: 461341, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2451.49, #queue-req: 0, 
[2025-10-27 15:37:45 TP0] Decode batch, #running-req: 128, #token: 466461, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2446.18, #queue-req: 0, 
[2025-10-27 15:37:48 TP0] Decode batch, #running-req: 128, #token: 471581, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2442.20, #queue-req: 0, 
[2025-10-27 15:37:50 TP0] Decode batch, #running-req: 128, #token: 476701, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2438.17, #queue-req: 0, 
[2025-10-27 15:37:52 TP0] Decode batch, #running-req: 128, #token: 481821, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2429.40, #queue-req: 0, 
[2025-10-27 15:37:54 TP0] Decode batch, #running-req: 128, #token: 486941, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2428.59, #queue-req: 0, 
[2025-10-27 15:37:56 TP0] Decode batch, #running-req: 128, #token: 492061, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2424.44, #queue-req: 0, 
[2025-10-27 15:37:58 TP0] Decode batch, #running-req: 128, #token: 497181, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2420.32, #queue-req: 0, 
[2025-10-27 15:38:00 TP0] Decode batch, #running-req: 128, #token: 502301, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2418.71, #queue-req: 0, 
[2025-10-27 15:38:02] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:38:02] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-27 15:38:02] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-27 15:38:02] INFO:     127.0.0.1:42992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:02] INFO:     127.0.0.1:43678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:38:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.03, #running-req: 11, #queue-req: 112, 
[2025-10-27 15:38:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-10-27 15:38:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 21, #queue-req: 102, 
[2025-10-27 15:38:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-10-27 15:38:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.09, #running-req: 31, #queue-req: 92, 
[2025-10-27 15:38:07 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.11, #running-req: 36, #queue-req: 86, 
[2025-10-27 15:38:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 42, #queue-req: 81, 
[2025-10-27 15:38:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 47, #queue-req: 76, 
[2025-10-27 15:38:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.15, #running-req: 52, #queue-req: 71, 
[2025-10-27 15:38:10 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.17, #running-req: 57, #queue-req: 65, 
[2025-10-27 15:38:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.18, #running-req: 63, #queue-req: 60, 
[2025-10-27 15:38:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.20, #running-req: 68, #queue-req: 55, 
[2025-10-27 15:38:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.21, #running-req: 73, #queue-req: 50, 
[2025-10-27 15:38:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.23, #running-req: 78, #queue-req: 45, 
[2025-10-27 15:38:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.24, #running-req: 83, #queue-req: 40, 
[2025-10-27 15:38:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.26, #running-req: 88, #queue-req: 35, 
[2025-10-27 15:38:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.27, #running-req: 93, #queue-req: 30, 
[2025-10-27 15:38:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.28, #running-req: 98, #queue-req: 25, 
[2025-10-27 15:38:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 103, #queue-req: 20, 
[2025-10-27 15:38:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.31, #running-req: 108, #queue-req: 15, 
[2025-10-27 15:38:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.33, #running-req: 113, #queue-req: 10, 
[2025-10-27 15:38:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.34, #running-req: 118, #queue-req: 5, 
[2025-10-27 15:38:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.36, #running-req: 123, #queue-req: 0, 
[2025-10-27 15:38:23 TP0] Decode batch, #running-req: 128, #token: 411420, token usage: 0.37, cuda graph: True, gen throughput (token/s): 225.25, #queue-req: 0, 
[2025-10-27 15:38:25 TP0] Decode batch, #running-req: 128, #token: 416540, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2536.82, #queue-req: 0, 
[2025-10-27 15:38:27 TP0] Decode batch, #running-req: 128, #token: 421660, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2507.18, #queue-req: 0, 
[2025-10-27 15:38:29 TP0] Decode batch, #running-req: 128, #token: 426780, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2496.36, #queue-req: 0, 
[2025-10-27 15:38:31 TP0] Decode batch, #running-req: 128, #token: 431900, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2485.74, #queue-req: 0, 
[2025-10-27 15:38:33 TP0] Decode batch, #running-req: 128, #token: 437020, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2484.41, #queue-req: 0, 
[2025-10-27 15:38:35 TP0] Decode batch, #running-req: 128, #token: 442140, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2476.83, #queue-req: 0, 
[2025-10-27 15:38:37 TP0] Decode batch, #running-req: 128, #token: 447260, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2473.75, #queue-req: 0, 
[2025-10-27 15:38:39 TP0] Decode batch, #running-req: 128, #token: 452380, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2467.62, #queue-req: 0, 
[2025-10-27 15:38:41 TP0] Decode batch, #running-req: 128, #token: 457500, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2462.04, #queue-req: 0, 
[2025-10-27 15:38:44 TP0] Decode batch, #running-req: 128, #token: 462620, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2459.17, #queue-req: 0, 
[2025-10-27 15:38:46 TP0] Decode batch, #running-req: 128, #token: 467740, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2457.24, #queue-req: 0, 
[2025-10-27 15:38:48 TP0] Decode batch, #running-req: 128, #token: 472860, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2449.95, #queue-req: 0, 
[2025-10-27 15:38:50 TP0] Decode batch, #running-req: 128, #token: 477980, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2447.11, #queue-req: 0, 
[2025-10-27 15:38:52 TP0] Decode batch, #running-req: 128, #token: 483100, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2438.35, #queue-req: 0, 
[2025-10-27 15:38:54 TP0] Decode batch, #running-req: 128, #token: 488220, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2436.14, #queue-req: 0, 
[2025-10-27 15:38:56 TP0] Decode batch, #running-req: 128, #token: 493340, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2431.12, #queue-req: 0, 
[2025-10-27 15:38:58 TP0] Decode batch, #running-req: 128, #token: 498460, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2426.68, #queue-req: 0, 
[2025-10-27 15:39:00 TP0] Decode batch, #running-req: 128, #token: 503580, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2427.04, #queue-req: 0, 
[2025-10-27 15:39:02 TP0] Decode batch, #running-req: 128, #token: 508700, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2420.47, #queue-req: 0, 
[2025-10-27 15:39:04] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:39:04] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-27 15:39:04] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-10-27 15:39:04] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:04] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:39:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.03, #running-req: 11, #queue-req: 100, 
[2025-10-27 15:39:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 95, 
[2025-10-27 15:39:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 21, #queue-req: 90, 
[2025-10-27 15:39:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 26, #queue-req: 85, 
[2025-10-27 15:39:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 31, #queue-req: 80, 
[2025-10-27 15:39:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 36, #queue-req: 75, 
[2025-10-27 15:39:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 41, #queue-req: 70, 
[2025-10-27 15:39:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.13, #running-req: 46, #queue-req: 65, 
[2025-10-27 15:39:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.15, #running-req: 51, #queue-req: 60, 
[2025-10-27 15:39:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.16, #running-req: 56, #queue-req: 55, 
[2025-10-27 15:39:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.18, #running-req: 61, #queue-req: 50, 
[2025-10-27 15:39:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.19, #running-req: 66, #queue-req: 45, 
[2025-10-27 15:39:15 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.21, #running-req: 71, #queue-req: 39, 
[2025-10-27 15:39:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.22, #running-req: 77, #queue-req: 34, 
[2025-10-27 15:39:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.24, #running-req: 82, #queue-req: 29, 
[2025-10-27 15:39:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.25, #running-req: 87, #queue-req: 24, 
[2025-10-27 15:39:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.27, #running-req: 92, #queue-req: 19, 
[2025-10-27 15:39:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.28, #running-req: 97, #queue-req: 14, 
[2025-10-27 15:39:20 TP0] Prefill batch, #new-seq: 6, #new-token: 15993, #cached-token: 3213, token usage: 0.30, #running-req: 102, #queue-req: 8, 
[2025-10-27 15:39:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.31, #running-req: 108, #queue-req: 3, 
[2025-10-27 15:39:21 TP0] Prefill batch, #new-seq: 3, #new-token: 9590, #cached-token: 13, token usage: 0.33, #running-req: 113, #queue-req: 0, 
[2025-10-27 15:39:23 TP0] Decode batch, #running-req: 116, #token: 372854, token usage: 0.34, cuda graph: True, gen throughput (token/s): 237.83, #queue-req: 0, 
[2025-10-27 15:39:25 TP0] Decode batch, #running-req: 116, #token: 377494, token usage: 0.34, cuda graph: True, gen throughput (token/s): 2347.45, #queue-req: 0, 
[2025-10-27 15:39:27 TP0] Decode batch, #running-req: 116, #token: 382134, token usage: 0.35, cuda graph: True, gen throughput (token/s): 2333.35, #queue-req: 0, 
[2025-10-27 15:39:29 TP0] Decode batch, #running-req: 116, #token: 386774, token usage: 0.35, cuda graph: True, gen throughput (token/s): 2324.54, #queue-req: 0, 
[2025-10-27 15:39:31 TP0] Decode batch, #running-req: 116, #token: 391414, token usage: 0.36, cuda graph: True, gen throughput (token/s): 2317.70, #queue-req: 0, 
[2025-10-27 15:39:33 TP0] Decode batch, #running-req: 116, #token: 396054, token usage: 0.36, cuda graph: True, gen throughput (token/s): 2309.76, #queue-req: 0, 
[2025-10-27 15:39:35 TP0] Decode batch, #running-req: 116, #token: 400694, token usage: 0.36, cuda graph: True, gen throughput (token/s): 2305.33, #queue-req: 0, 
[2025-10-27 15:39:37 TP0] Decode batch, #running-req: 116, #token: 405334, token usage: 0.37, cuda graph: True, gen throughput (token/s): 2304.28, #queue-req: 0, 
[2025-10-27 15:39:39 TP0] Decode batch, #running-req: 116, #token: 409974, token usage: 0.37, cuda graph: True, gen throughput (token/s): 2297.87, #queue-req: 0, 
[2025-10-27 15:39:41 TP0] Decode batch, #running-req: 116, #token: 414614, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2295.33, #queue-req: 0, 
[2025-10-27 15:39:43 TP0] Decode batch, #running-req: 116, #token: 419254, token usage: 0.38, cuda graph: True, gen throughput (token/s): 2290.62, #queue-req: 0, 
[2025-10-27 15:39:45 TP0] Decode batch, #running-req: 116, #token: 423894, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2286.26, #queue-req: 0, 
[2025-10-27 15:39:47 TP0] Decode batch, #running-req: 116, #token: 428534, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2283.87, #queue-req: 0, 
[2025-10-27 15:39:49 TP0] Decode batch, #running-req: 116, #token: 433174, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2281.02, #queue-req: 0, 
[2025-10-27 15:39:51 TP0] Decode batch, #running-req: 116, #token: 437814, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2275.26, #queue-req: 0, 
[2025-10-27 15:39:53 TP0] Decode batch, #running-req: 116, #token: 442454, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2268.53, #queue-req: 0, 
[2025-10-27 15:39:56 TP0] Decode batch, #running-req: 116, #token: 447094, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2265.00, #queue-req: 0, 
[2025-10-27 15:39:58 TP0] Decode batch, #running-req: 116, #token: 451734, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2258.11, #queue-req: 0, 
[2025-10-27 15:40:00 TP0] Decode batch, #running-req: 116, #token: 456374, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2255.75, #queue-req: 0, 
[2025-10-27 15:40:02 TP0] Decode batch, #running-req: 116, #token: 461014, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2254.65, #queue-req: 0, 
[2025-10-27 15:40:03] INFO:     127.0.0.1:59616 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-27 15:40:20] INFO:     127.0.0.1:58662 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-27 15:40:27] INFO:     127.0.0.1:39376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:40:27 TP0] Decode batch, #running-req: 1, #token: 3217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 114.74, #queue-req: 0, 
[2025-10-27 15:40:29] INFO:     127.0.0.1:39388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:40:29] INFO:     127.0.0.1:39392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-27 15:40:29] INFO:     127.0.0.1:39498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 22, 
[2025-10-27 15:40:29] INFO:     127.0.0.1:39670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:29] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:40:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.03, #running-req: 11, #queue-req: 48, 
[2025-10-27 15:40:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-27 15:40:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 21, #queue-req: 38, 
[2025-10-27 15:40:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-10-27 15:40:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[2025-10-27 15:40:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 36, #queue-req: 23, 
[2025-10-27 15:40:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.12, #running-req: 41, #queue-req: 18, 
[2025-10-27 15:40:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.13, #running-req: 46, #queue-req: 13, 
[2025-10-27 15:40:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 51, #queue-req: 8, 
[2025-10-27 15:40:37 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.17, #running-req: 56, #queue-req: 2, 
[2025-10-27 15:40:38 TP0] Prefill batch, #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.18, #running-req: 62, #queue-req: 0, 
[2025-10-27 15:40:40 TP0] Decode batch, #running-req: 64, #token: 206248, token usage: 0.19, cuda graph: True, gen throughput (token/s): 119.32, #queue-req: 0, 
[2025-10-27 15:40:41 TP0] Decode batch, #running-req: 64, #token: 208808, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1538.84, #queue-req: 0, 
[2025-10-27 15:40:43 TP0] Decode batch, #running-req: 64, #token: 211368, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1523.64, #queue-req: 0, 
[2025-10-27 15:40:45 TP0] Decode batch, #running-req: 64, #token: 213928, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1516.29, #queue-req: 0, 
[2025-10-27 15:40:46 TP0] Decode batch, #running-req: 64, #token: 216488, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1509.85, #queue-req: 0, 
[2025-10-27 15:40:48 TP0] Decode batch, #running-req: 64, #token: 219048, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1505.16, #queue-req: 0, 
[2025-10-27 15:40:50 TP0] Decode batch, #running-req: 64, #token: 221608, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1500.12, #queue-req: 0, 
[2025-10-27 15:40:51 TP0] Decode batch, #running-req: 64, #token: 224168, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1499.42, #queue-req: 0, 
[2025-10-27 15:40:53 TP0] Decode batch, #running-req: 64, #token: 226728, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1496.38, #queue-req: 0, 
[2025-10-27 15:40:55 TP0] Decode batch, #running-req: 64, #token: 229288, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.55, #queue-req: 0, 
[2025-10-27 15:40:57 TP0] Decode batch, #running-req: 64, #token: 231848, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.64, #queue-req: 0, 
[2025-10-27 15:40:58 TP0] Decode batch, #running-req: 64, #token: 234408, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.77, #queue-req: 0, 
[2025-10-27 15:41:00 TP0] Decode batch, #running-req: 64, #token: 236968, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.87, #queue-req: 0, 
[2025-10-27 15:41:02 TP0] Decode batch, #running-req: 64, #token: 239528, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1491.31, #queue-req: 0, 
[2025-10-27 15:41:03 TP0] Decode batch, #running-req: 64, #token: 242088, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1491.96, #queue-req: 0, 
[2025-10-27 15:41:05 TP0] Decode batch, #running-req: 64, #token: 244648, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1489.16, #queue-req: 0, 
[2025-10-27 15:41:07 TP0] Decode batch, #running-req: 64, #token: 247208, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1485.01, #queue-req: 0, 
[2025-10-27 15:41:09 TP0] Decode batch, #running-req: 64, #token: 249768, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1480.36, #queue-req: 0, 
[2025-10-27 15:41:10 TP0] Decode batch, #running-req: 64, #token: 252328, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1482.45, #queue-req: 0, 
[2025-10-27 15:41:12 TP0] Decode batch, #running-req: 64, #token: 254888, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1476.93, #queue-req: 0, 
[2025-10-27 15:41:13] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:41:13] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:41:13] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 43, 
[2025-10-27 15:41:13] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:13] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.03, #running-req: 11, #queue-req: 48, 
[2025-10-27 15:41:15 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.05, #running-req: 16, #queue-req: 42, 
[2025-10-27 15:41:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 22, #queue-req: 37, 
[2025-10-27 15:41:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.08, #running-req: 27, #queue-req: 32, 
[2025-10-27 15:41:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 32, #queue-req: 27, 
[2025-10-27 15:41:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.11, #running-req: 37, #queue-req: 22, 
[2025-10-27 15:41:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 42, #queue-req: 17, 
[2025-10-27 15:41:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.14, #running-req: 47, #queue-req: 12, 
[2025-10-27 15:41:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 52, #queue-req: 7, 
[2025-10-27 15:41:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 57, #queue-req: 2, 
[2025-10-27 15:41:22 TP0] Prefill batch, #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.18, #running-req: 62, #queue-req: 0, 
[2025-10-27 15:41:24 TP0] Decode batch, #running-req: 64, #token: 206231, token usage: 0.19, cuda graph: True, gen throughput (token/s): 212.15, #queue-req: 0, 
[2025-10-27 15:41:26 TP0] Decode batch, #running-req: 64, #token: 208791, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1539.44, #queue-req: 0, 
[2025-10-27 15:41:27 TP0] Decode batch, #running-req: 64, #token: 211351, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1523.73, #queue-req: 0, 
[2025-10-27 15:41:29 TP0] Decode batch, #running-req: 64, #token: 213911, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1521.60, #queue-req: 0, 
[2025-10-27 15:41:31 TP0] Decode batch, #running-req: 64, #token: 216471, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1513.30, #queue-req: 0, 
[2025-10-27 15:41:33 TP0] Decode batch, #running-req: 64, #token: 219031, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1508.85, #queue-req: 0, 
[2025-10-27 15:41:34 TP0] Decode batch, #running-req: 64, #token: 221591, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1506.67, #queue-req: 0, 
[2025-10-27 15:41:36 TP0] Decode batch, #running-req: 64, #token: 224151, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1502.41, #queue-req: 0, 
[2025-10-27 15:41:38 TP0] Decode batch, #running-req: 64, #token: 226711, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1500.43, #queue-req: 0, 
[2025-10-27 15:41:39 TP0] Decode batch, #running-req: 64, #token: 229271, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1496.24, #queue-req: 0, 
[2025-10-27 15:41:41 TP0] Decode batch, #running-req: 64, #token: 231831, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1494.25, #queue-req: 0, 
[2025-10-27 15:41:43 TP0] Decode batch, #running-req: 64, #token: 234391, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1495.85, #queue-req: 0, 
[2025-10-27 15:41:45 TP0] Decode batch, #running-req: 64, #token: 236951, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.63, #queue-req: 0, 
[2025-10-27 15:41:46 TP0] Decode batch, #running-req: 64, #token: 239511, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1493.99, #queue-req: 0, 
[2025-10-27 15:41:48 TP0] Decode batch, #running-req: 64, #token: 242071, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.99, #queue-req: 0, 
[2025-10-27 15:41:50 TP0] Decode batch, #running-req: 64, #token: 244631, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.79, #queue-req: 0, 
[2025-10-27 15:41:51 TP0] Decode batch, #running-req: 64, #token: 247191, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1489.31, #queue-req: 0, 
[2025-10-27 15:41:53 TP0] Decode batch, #running-req: 64, #token: 249751, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.95, #queue-req: 0, 
[2025-10-27 15:41:55 TP0] Decode batch, #running-req: 64, #token: 252311, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.33, #queue-req: 0, 
[2025-10-27 15:41:57 TP0] Decode batch, #running-req: 64, #token: 254871, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.75, #queue-req: 0, 
[2025-10-27 15:41:57] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:57 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:41:58] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58 TP0] Prefill batch, #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:41:58] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.01, #running-req: 3, #queue-req: 13, 
[2025-10-27 15:41:58] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:41:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 9, #queue-req: 50, 
[2025-10-27 15:41:59 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.04, #running-req: 14, #queue-req: 44, 
[2025-10-27 15:42:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.06, #running-req: 20, #queue-req: 39, 
[2025-10-27 15:42:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 25, #queue-req: 34, 
[2025-10-27 15:42:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 30, #queue-req: 29, 
[2025-10-27 15:42:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 35, #queue-req: 24, 
[2025-10-27 15:42:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 40, #queue-req: 19, 
[2025-10-27 15:42:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.13, #running-req: 45, #queue-req: 14, 
[2025-10-27 15:42:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.15, #running-req: 50, #queue-req: 9, 
[2025-10-27 15:42:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.16, #running-req: 55, #queue-req: 4, 
[2025-10-27 15:42:06 TP0] Prefill batch, #new-seq: 4, #new-token: 12791, #cached-token: 13, token usage: 0.17, #running-req: 60, #queue-req: 0, 
[2025-10-27 15:42:08 TP0] Decode batch, #running-req: 64, #token: 206182, token usage: 0.19, cuda graph: True, gen throughput (token/s): 210.37, #queue-req: 0, 
[2025-10-27 15:42:10 TP0] Decode batch, #running-req: 64, #token: 208742, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1535.18, #queue-req: 0, 
[2025-10-27 15:42:12 TP0] Decode batch, #running-req: 64, #token: 211302, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1521.66, #queue-req: 0, 
[2025-10-27 15:42:13 TP0] Decode batch, #running-req: 64, #token: 213862, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1513.16, #queue-req: 0, 
[2025-10-27 15:42:15 TP0] Decode batch, #running-req: 64, #token: 216422, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1503.88, #queue-req: 0, 
[2025-10-27 15:42:17 TP0] Decode batch, #running-req: 64, #token: 218982, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1499.83, #queue-req: 0, 
[2025-10-27 15:42:19 TP0] Decode batch, #running-req: 64, #token: 221542, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1493.12, #queue-req: 0, 
[2025-10-27 15:42:20 TP0] Decode batch, #running-req: 64, #token: 224102, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1490.92, #queue-req: 0, 
[2025-10-27 15:42:22 TP0] Decode batch, #running-req: 64, #token: 226662, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1489.35, #queue-req: 0, 
[2025-10-27 15:42:24 TP0] Decode batch, #running-req: 64, #token: 229222, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1490.17, #queue-req: 0, 
[2025-10-27 15:42:25 TP0] Decode batch, #running-req: 64, #token: 231782, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1487.73, #queue-req: 0, 
[2025-10-27 15:42:27 TP0] Decode batch, #running-req: 64, #token: 234342, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1487.91, #queue-req: 0, 
[2025-10-27 15:42:29 TP0] Decode batch, #running-req: 64, #token: 236902, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1490.02, #queue-req: 0, 
[2025-10-27 15:42:31 TP0] Decode batch, #running-req: 64, #token: 239462, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.49, #queue-req: 0, 
[2025-10-27 15:42:32 TP0] Decode batch, #running-req: 64, #token: 242022, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.10, #queue-req: 0, 
[2025-10-27 15:42:34 TP0] Decode batch, #running-req: 64, #token: 244582, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1481.71, #queue-req: 0, 
[2025-10-27 15:42:36 TP0] Decode batch, #running-req: 64, #token: 247142, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1479.36, #queue-req: 0, 
[2025-10-27 15:42:38 TP0] Decode batch, #running-req: 64, #token: 249702, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1478.60, #queue-req: 0, 
[2025-10-27 15:42:39 TP0] Decode batch, #running-req: 64, #token: 252262, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1476.62, #queue-req: 0, 
[2025-10-27 15:42:41 TP0] Decode batch, #running-req: 64, #token: 254822, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1478.36, #queue-req: 0, 
[2025-10-27 15:42:42] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 63, #queue-req: 0, 
[2025-10-27 15:42:42] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 64, #queue-req: 7, 
[2025-10-27 15:42:42] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 70, #queue-req: 38, 
[2025-10-27 15:42:42] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:52992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:42] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:42:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.03, #running-req: 75, #queue-req: 47, 
[2025-10-27 15:42:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.05, #running-req: 80, #queue-req: 42, 
[2025-10-27 15:42:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.06, #running-req: 85, #queue-req: 37, 
[2025-10-27 15:42:45 TP0] Prefill batch, #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.08, #running-req: 90, #queue-req: 30, 
[2025-10-27 15:42:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.10, #running-req: 97, #queue-req: 25, 
[2025-10-27 15:42:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.11, #running-req: 102, #queue-req: 20, 
[2025-10-27 15:42:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.13, #running-req: 107, #queue-req: 15, 
[2025-10-27 15:42:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.14, #running-req: 112, #queue-req: 10, 
[2025-10-27 15:42:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.16, #running-req: 117, #queue-req: 5, 
[2025-10-27 15:42:50 TP0] Prefill batch, #new-seq: 5, #new-token: 12790, #cached-token: 3215, token usage: 0.17, #running-req: 122, #queue-req: 0, 
[2025-10-27 15:42:53 TP0] Decode batch, #running-req: 64, #token: 202970, token usage: 0.18, cuda graph: True, gen throughput (token/s): 221.73, #queue-req: 0, 
[2025-10-27 15:42:54 TP0] Decode batch, #running-req: 64, #token: 205530, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1526.68, #queue-req: 0, 
[2025-10-27 15:42:56 TP0] Decode batch, #running-req: 64, #token: 208090, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1512.51, #queue-req: 0, 
[2025-10-27 15:42:58 TP0] Decode batch, #running-req: 64, #token: 210650, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1508.48, #queue-req: 0, 
[2025-10-27 15:42:59 TP0] Decode batch, #running-req: 64, #token: 213210, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1502.76, #queue-req: 0, 
[2025-10-27 15:43:01 TP0] Decode batch, #running-req: 64, #token: 215770, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1501.26, #queue-req: 0, 
[2025-10-27 15:43:03 TP0] Decode batch, #running-req: 64, #token: 218330, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1499.11, #queue-req: 0, 
[2025-10-27 15:43:04 TP0] Decode batch, #running-req: 64, #token: 220890, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1495.33, #queue-req: 0, 
[2025-10-27 15:43:06 TP0] Decode batch, #running-req: 64, #token: 223450, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1494.52, #queue-req: 0, 
[2025-10-27 15:43:08 TP0] Decode batch, #running-req: 64, #token: 226010, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1495.17, #queue-req: 0, 
[2025-10-27 15:43:10 TP0] Decode batch, #running-req: 64, #token: 228570, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.40, #queue-req: 0, 
[2025-10-27 15:43:11 TP0] Decode batch, #running-req: 64, #token: 231130, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.70, #queue-req: 0, 
[2025-10-27 15:43:13 TP0] Decode batch, #running-req: 64, #token: 233690, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.51, #queue-req: 0, 
[2025-10-27 15:43:15 TP0] Decode batch, #running-req: 64, #token: 236250, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1489.28, #queue-req: 0, 
[2025-10-27 15:43:16 TP0] Decode batch, #running-req: 64, #token: 238810, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1491.38, #queue-req: 0, 
[2025-10-27 15:43:18 TP0] Decode batch, #running-req: 64, #token: 241370, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.61, #queue-req: 0, 
[2025-10-27 15:43:20 TP0] Decode batch, #running-req: 64, #token: 243930, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.91, #queue-req: 0, 
[2025-10-27 15:43:22 TP0] Decode batch, #running-req: 64, #token: 246490, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1485.84, #queue-req: 0, 
[2025-10-27 15:43:23 TP0] Decode batch, #running-req: 64, #token: 249050, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1480.94, #queue-req: 0, 
[2025-10-27 15:43:25 TP0] Decode batch, #running-req: 64, #token: 251610, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1482.31, #queue-req: 0, 
[2025-10-27 15:43:26] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:43:26] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 11, 
[2025-10-27 15:43:26] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-27 15:43:26] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:26] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:43:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.03, #running-req: 11, #queue-req: 48, 
[2025-10-27 15:43:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-27 15:43:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 21, #queue-req: 38, 
[2025-10-27 15:43:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-10-27 15:43:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[2025-10-27 15:43:31 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.11, #running-req: 36, #queue-req: 22, 
[2025-10-27 15:43:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 42, #queue-req: 17, 
[2025-10-27 15:43:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 47, #queue-req: 12, 
[2025-10-27 15:43:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.15, #running-req: 52, #queue-req: 7, 
[2025-10-27 15:43:34 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.17, #running-req: 57, #queue-req: 1, 
[2025-10-27 15:43:35 TP0] Prefill batch, #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.18, #running-req: 63, #queue-req: 0, 
[2025-10-27 15:43:37 TP0] Decode batch, #running-req: 64, #token: 206170, token usage: 0.19, cuda graph: True, gen throughput (token/s): 215.23, #queue-req: 0, 
[2025-10-27 15:43:39 TP0] Decode batch, #running-req: 64, #token: 208730, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1526.43, #queue-req: 0, 
[2025-10-27 15:43:40 TP0] Decode batch, #running-req: 64, #token: 211290, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1515.11, #queue-req: 0, 
[2025-10-27 15:43:42 TP0] Decode batch, #running-req: 64, #token: 213850, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1515.96, #queue-req: 0, 
[2025-10-27 15:43:44 TP0] Decode batch, #running-req: 64, #token: 216410, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1508.79, #queue-req: 0, 
[2025-10-27 15:43:45 TP0] Decode batch, #running-req: 64, #token: 218970, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1501.58, #queue-req: 0, 
[2025-10-27 15:43:47 TP0] Decode batch, #running-req: 64, #token: 221530, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1500.47, #queue-req: 0, 
[2025-10-27 15:43:49 TP0] Decode batch, #running-req: 64, #token: 224090, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1497.09, #queue-req: 0, 
[2025-10-27 15:43:51 TP0] Decode batch, #running-req: 64, #token: 226650, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1493.88, #queue-req: 0, 
[2025-10-27 15:43:52 TP0] Decode batch, #running-req: 64, #token: 229210, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1492.74, #queue-req: 0, 
[2025-10-27 15:43:54 TP0] Decode batch, #running-req: 64, #token: 231770, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1491.37, #queue-req: 0, 
[2025-10-27 15:43:56 TP0] Decode batch, #running-req: 64, #token: 234330, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1495.12, #queue-req: 0, 
[2025-10-27 15:43:57 TP0] Decode batch, #running-req: 64, #token: 236890, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1498.69, #queue-req: 0, 
[2025-10-27 15:43:59 TP0] Decode batch, #running-req: 64, #token: 239450, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.39, #queue-req: 0, 
[2025-10-27 15:44:01 TP0] Decode batch, #running-req: 64, #token: 242010, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.93, #queue-req: 0, 
[2025-10-27 15:44:03 TP0] Decode batch, #running-req: 64, #token: 244570, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.01, #queue-req: 0, 
[2025-10-27 15:44:04 TP0] Decode batch, #running-req: 64, #token: 247130, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.68, #queue-req: 0, 
[2025-10-27 15:44:06 TP0] Decode batch, #running-req: 64, #token: 249690, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.02, #queue-req: 0, 
[2025-10-27 15:44:08 TP0] Decode batch, #running-req: 64, #token: 252250, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.42, #queue-req: 0, 
[2025-10-27 15:44:09 TP0] Decode batch, #running-req: 64, #token: 254810, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.62, #queue-req: 0, 
[2025-10-27 15:44:10] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:44:10] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-27 15:44:10] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:10] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-27 15:44:11] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.03, #running-req: 11, #queue-req: 48, 
[2025-10-27 15:44:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-27 15:44:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.06, #running-req: 21, #queue-req: 38, 
[2025-10-27 15:44:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-10-27 15:44:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[2025-10-27 15:44:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.10, #running-req: 36, #queue-req: 23, 
[2025-10-27 15:44:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 41, #queue-req: 18, 
[2025-10-27 15:44:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 46, #queue-req: 13, 
[2025-10-27 15:44:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.15, #running-req: 51, #queue-req: 8, 
[2025-10-27 15:44:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.16, #running-req: 56, #queue-req: 3, 
[2025-10-27 15:44:19 TP0] Prefill batch, #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.18, #running-req: 61, #queue-req: 0, 
[2025-10-27 15:44:22 TP0] Decode batch, #running-req: 64, #token: 206184, token usage: 0.19, cuda graph: True, gen throughput (token/s): 210.11, #queue-req: 0, 
[2025-10-27 15:44:23 TP0] Decode batch, #running-req: 64, #token: 208744, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1534.91, #queue-req: 0, 
[2025-10-27 15:44:25 TP0] Decode batch, #running-req: 64, #token: 211304, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1516.03, #queue-req: 0, 
[2025-10-27 15:44:27 TP0] Decode batch, #running-req: 64, #token: 213864, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1510.64, #queue-req: 0, 
[2025-10-27 15:44:28 TP0] Decode batch, #running-req: 64, #token: 216424, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1505.91, #queue-req: 0, 
[2025-10-27 15:44:30 TP0] Decode batch, #running-req: 64, #token: 218984, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1506.30, #queue-req: 0, 
[2025-10-27 15:44:32 TP0] Decode batch, #running-req: 64, #token: 221544, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1502.21, #queue-req: 0, 
[2025-10-27 15:44:33 TP0] Decode batch, #running-req: 64, #token: 224104, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1500.97, #queue-req: 0, 
[2025-10-27 15:44:35 TP0] Decode batch, #running-req: 64, #token: 226664, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1502.45, #queue-req: 0, 
[2025-10-27 15:44:37 TP0] Decode batch, #running-req: 64, #token: 229224, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1501.21, #queue-req: 0, 
[2025-10-27 15:44:39 TP0] Decode batch, #running-req: 64, #token: 231784, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1502.81, #queue-req: 0, 
[2025-10-27 15:44:40 TP0] Decode batch, #running-req: 64, #token: 234344, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1505.58, #queue-req: 0, 
[2025-10-27 15:44:42 TP0] Decode batch, #running-req: 64, #token: 236904, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1504.98, #queue-req: 0, 
[2025-10-27 15:44:44 TP0] Decode batch, #running-req: 64, #token: 239464, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.67, #queue-req: 0, 
[2025-10-27 15:44:45 TP0] Decode batch, #running-req: 64, #token: 242024, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.76, #queue-req: 0, 
[2025-10-27 15:44:47 TP0] Decode batch, #running-req: 64, #token: 244584, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.80, #queue-req: 0, 
[2025-10-27 15:44:49 TP0] Decode batch, #running-req: 64, #token: 247144, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1493.56, #queue-req: 0, 
[2025-10-27 15:44:51 TP0] Decode batch, #running-req: 64, #token: 249704, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.88, #queue-req: 0, 
[2025-10-27 15:44:52 TP0] Decode batch, #running-req: 64, #token: 252264, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.45, #queue-req: 0, 
[2025-10-27 15:44:54 TP0] Decode batch, #running-req: 64, #token: 254824, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.65, #queue-req: 0, 
[2025-10-27 15:44:55] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:44:55] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-27 15:44:55] INFO:     127.0.0.1:44352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-10-27 15:44:55] INFO:     127.0.0.1:44714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:55] INFO:     127.0.0.1:44854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:44:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.03, #running-req: 11, #queue-req: 48, 
[2025-10-27 15:44:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-10-27 15:44:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 21, #queue-req: 38, 
[2025-10-27 15:44:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-10-27 15:44:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 31, #queue-req: 28, 
[2025-10-27 15:45:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 36, #queue-req: 23, 
[2025-10-27 15:45:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 41, #queue-req: 18, 
[2025-10-27 15:45:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.13, #running-req: 46, #queue-req: 13, 
[2025-10-27 15:45:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.15, #running-req: 51, #queue-req: 8, 
[2025-10-27 15:45:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.16, #running-req: 56, #queue-req: 3, 
[2025-10-27 15:45:04 TP0] Prefill batch, #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.18, #running-req: 61, #queue-req: 0, 
[2025-10-27 15:45:06 TP0] Decode batch, #running-req: 64, #token: 206174, token usage: 0.19, cuda graph: True, gen throughput (token/s): 209.57, #queue-req: 0, 
[2025-10-27 15:45:08 TP0] Decode batch, #running-req: 64, #token: 208734, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1536.84, #queue-req: 0, 
[2025-10-27 15:45:10 TP0] Decode batch, #running-req: 64, #token: 211294, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1522.69, #queue-req: 0, 
[2025-10-27 15:45:11 TP0] Decode batch, #running-req: 64, #token: 213854, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1513.41, #queue-req: 0, 
[2025-10-27 15:45:13 TP0] Decode batch, #running-req: 64, #token: 216414, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1508.74, #queue-req: 0, 
[2025-10-27 15:45:15 TP0] Decode batch, #running-req: 64, #token: 218974, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1501.46, #queue-req: 0, 
[2025-10-27 15:45:16 TP0] Decode batch, #running-req: 64, #token: 221534, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1496.11, #queue-req: 0, 
[2025-10-27 15:45:18 TP0] Decode batch, #running-req: 64, #token: 224094, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1495.63, #queue-req: 0, 
[2025-10-27 15:45:20 TP0] Decode batch, #running-req: 64, #token: 226654, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1494.01, #queue-req: 0, 
[2025-10-27 15:45:21 TP0] Decode batch, #running-req: 64, #token: 229214, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1496.11, #queue-req: 0, 
[2025-10-27 15:45:23 TP0] Decode batch, #running-req: 64, #token: 231774, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1495.39, #queue-req: 0, 
[2025-10-27 15:45:25 TP0] Decode batch, #running-req: 64, #token: 234334, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1497.08, #queue-req: 0, 
[2025-10-27 15:45:27 TP0] Decode batch, #running-req: 64, #token: 236894, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1496.36, #queue-req: 0, 
[2025-10-27 15:45:28 TP0] Decode batch, #running-req: 64, #token: 239454, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1496.31, #queue-req: 0, 
[2025-10-27 15:45:30 TP0] Decode batch, #running-req: 64, #token: 242014, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.48, #queue-req: 0, 
[2025-10-27 15:45:32 TP0] Decode batch, #running-req: 64, #token: 244574, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1486.05, #queue-req: 0, 
[2025-10-27 15:45:33 TP0] Decode batch, #running-req: 64, #token: 247134, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1487.26, #queue-req: 0, 
[2025-10-27 15:45:35 TP0] Decode batch, #running-req: 64, #token: 249694, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1483.45, #queue-req: 0, 
[2025-10-27 15:45:37 TP0] Decode batch, #running-req: 64, #token: 252254, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1482.40, #queue-req: 0, 
[2025-10-27 15:45:39 TP0] Decode batch, #running-req: 64, #token: 254814, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1480.25, #queue-req: 0, 
[2025-10-27 15:45:39] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:45:39] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:39] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-10-27 15:45:40] INFO:     127.0.0.1:43298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 6, #queue-req: 36, 
[2025-10-27 15:45:40] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:40] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:45:41 TP0] Prefill batch, #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.03, #running-req: 11, #queue-req: 35, 
[2025-10-27 15:45:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 17, #queue-req: 30, 
[2025-10-27 15:45:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.06, #running-req: 22, #queue-req: 25, 
[2025-10-27 15:45:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.08, #running-req: 27, #queue-req: 20, 
[2025-10-27 15:45:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 32, #queue-req: 15, 
[2025-10-27 15:45:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 37, #queue-req: 10, 
[2025-10-27 15:45:45 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.12, #running-req: 42, #queue-req: 4, 
[2025-10-27 15:45:46 TP0] Prefill batch, #new-seq: 4, #new-token: 12787, #cached-token: 17, token usage: 0.14, #running-req: 48, #queue-req: 0, 
[2025-10-27 15:45:49 TP0] Decode batch, #running-req: 52, #token: 167518, token usage: 0.15, cuda graph: True, gen throughput (token/s): 232.64, #queue-req: 0, 
[2025-10-27 15:45:50 TP0] Decode batch, #running-req: 52, #token: 169598, token usage: 0.15, cuda graph: True, gen throughput (token/s): 1295.95, #queue-req: 0, 
[2025-10-27 15:45:52 TP0] Decode batch, #running-req: 52, #token: 171678, token usage: 0.16, cuda graph: True, gen throughput (token/s): 1282.47, #queue-req: 0, 
[2025-10-27 15:45:53 TP0] Decode batch, #running-req: 52, #token: 173758, token usage: 0.16, cuda graph: True, gen throughput (token/s): 1276.60, #queue-req: 0, 
[2025-10-27 15:45:55 TP0] Decode batch, #running-req: 52, #token: 175838, token usage: 0.16, cuda graph: True, gen throughput (token/s): 1275.71, #queue-req: 0, 
[2025-10-27 15:45:57 TP0] Decode batch, #running-req: 52, #token: 177918, token usage: 0.16, cuda graph: True, gen throughput (token/s): 1268.93, #queue-req: 0, 
[2025-10-27 15:45:58 TP0] Decode batch, #running-req: 52, #token: 179998, token usage: 0.16, cuda graph: True, gen throughput (token/s): 1268.05, #queue-req: 0, 
[2025-10-27 15:46:00 TP0] Decode batch, #running-req: 52, #token: 182078, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1270.84, #queue-req: 0, 
[2025-10-27 15:46:02 TP0] Decode batch, #running-req: 52, #token: 184158, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1265.82, #queue-req: 0, 
[2025-10-27 15:46:03 TP0] Decode batch, #running-req: 52, #token: 186238, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1267.81, #queue-req: 0, 
[2025-10-27 15:46:05 TP0] Decode batch, #running-req: 52, #token: 188318, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1265.99, #queue-req: 0, 
[2025-10-27 15:46:06 TP0] Decode batch, #running-req: 52, #token: 190398, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1265.14, #queue-req: 0, 
[2025-10-27 15:46:08 TP0] Decode batch, #running-req: 52, #token: 192478, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1266.65, #queue-req: 0, 
[2025-10-27 15:46:10 TP0] Decode batch, #running-req: 52, #token: 194558, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1260.30, #queue-req: 0, 
[2025-10-27 15:46:11 TP0] Decode batch, #running-req: 52, #token: 196638, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1260.12, #queue-req: 0, 
[2025-10-27 15:46:13 TP0] Decode batch, #running-req: 52, #token: 198718, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1256.30, #queue-req: 0, 
[2025-10-27 15:46:15 TP0] Decode batch, #running-req: 52, #token: 200798, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1259.79, #queue-req: 0, 
[2025-10-27 15:46:16 TP0] Decode batch, #running-req: 52, #token: 202878, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1256.31, #queue-req: 0, 
[2025-10-27 15:46:18 TP0] Decode batch, #running-req: 52, #token: 204958, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1260.24, #queue-req: 0, 
[2025-10-27 15:46:20 TP0] Decode batch, #running-req: 52, #token: 207038, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1255.62, #queue-req: 0, 
[2025-10-27 15:46:20] INFO:     127.0.0.1:59548 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-27 15:46:38] INFO:     127.0.0.1:35578 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-27 15:46:43] INFO:     127.0.0.1:35586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:46:44 TP0] Decode batch, #running-req: 1, #token: 3224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 39.26, #queue-req: 0, 
[2025-10-27 15:46:45] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:46:45] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:46:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-10-27 15:46:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:46:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:46:49 TP0] Decode batch, #running-req: 16, #token: 51682, token usage: 0.05, cuda graph: True, gen throughput (token/s): 109.16, #queue-req: 0, 
[2025-10-27 15:46:50 TP0] Decode batch, #running-req: 16, #token: 52322, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.50, #queue-req: 0, 
[2025-10-27 15:46:51 TP0] Decode batch, #running-req: 16, #token: 52962, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.51, #queue-req: 0, 
[2025-10-27 15:46:52 TP0] Decode batch, #running-req: 16, #token: 53602, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.06, #queue-req: 0, 
[2025-10-27 15:46:53 TP0] Decode batch, #running-req: 16, #token: 54242, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.42, #queue-req: 0, 
[2025-10-27 15:46:54 TP0] Decode batch, #running-req: 16, #token: 54882, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.18, #queue-req: 0, 
[2025-10-27 15:46:55 TP0] Decode batch, #running-req: 16, #token: 55522, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.67, #queue-req: 0, 
[2025-10-27 15:46:56 TP0] Decode batch, #running-req: 16, #token: 56162, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.01, #queue-req: 0, 
[2025-10-27 15:46:57 TP0] Decode batch, #running-req: 16, #token: 56802, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.57, #queue-req: 0, 
[2025-10-27 15:46:58 TP0] Decode batch, #running-req: 16, #token: 57442, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.87, #queue-req: 0, 
[2025-10-27 15:46:59 TP0] Decode batch, #running-req: 16, #token: 58082, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.42, #queue-req: 0, 
[2025-10-27 15:47:00 TP0] Decode batch, #running-req: 16, #token: 58722, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.89, #queue-req: 0, 
[2025-10-27 15:47:01 TP0] Decode batch, #running-req: 16, #token: 59362, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.94, #queue-req: 0, 
[2025-10-27 15:47:02 TP0] Decode batch, #running-req: 16, #token: 60002, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.81, #queue-req: 0, 
[2025-10-27 15:47:03 TP0] Decode batch, #running-req: 16, #token: 60642, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.99, #queue-req: 0, 
[2025-10-27 15:47:04 TP0] Decode batch, #running-req: 16, #token: 61282, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.93, #queue-req: 0, 
[2025-10-27 15:47:05 TP0] Decode batch, #running-req: 16, #token: 61922, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.41, #queue-req: 0, 
[2025-10-27 15:47:06 TP0] Decode batch, #running-req: 16, #token: 62562, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.10, #queue-req: 0, 
[2025-10-27 15:47:07 TP0] Decode batch, #running-req: 16, #token: 63202, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.60, #queue-req: 0, 
[2025-10-27 15:47:08 TP0] Decode batch, #running-req: 16, #token: 63842, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.20, #queue-req: 0, 
[2025-10-27 15:47:09] INFO:     127.0.0.1:57164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:47:09] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:47:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:47:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:47:12 TP0] Decode batch, #running-req: 16, #token: 51676, token usage: 0.05, cuda graph: True, gen throughput (token/s): 174.61, #queue-req: 0, 
[2025-10-27 15:47:13 TP0] Decode batch, #running-req: 16, #token: 52316, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.32, #queue-req: 0, 
[2025-10-27 15:47:14 TP0] Decode batch, #running-req: 16, #token: 52956, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.61, #queue-req: 0, 
[2025-10-27 15:47:15 TP0] Decode batch, #running-req: 16, #token: 53596, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.21, #queue-req: 0, 
[2025-10-27 15:47:16 TP0] Decode batch, #running-req: 16, #token: 54236, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.66, #queue-req: 0, 
[2025-10-27 15:47:17 TP0] Decode batch, #running-req: 16, #token: 54876, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.88, #queue-req: 0, 
[2025-10-27 15:47:18 TP0] Decode batch, #running-req: 16, #token: 55516, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.11, #queue-req: 0, 
[2025-10-27 15:47:19 TP0] Decode batch, #running-req: 16, #token: 56156, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.19, #queue-req: 0, 
[2025-10-27 15:47:20 TP0] Decode batch, #running-req: 16, #token: 56796, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.06, #queue-req: 0, 
[2025-10-27 15:47:21 TP0] Decode batch, #running-req: 16, #token: 57436, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.29, #queue-req: 0, 
[2025-10-27 15:47:23 TP0] Decode batch, #running-req: 16, #token: 58076, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.81, #queue-req: 0, 
[2025-10-27 15:47:24 TP0] Decode batch, #running-req: 16, #token: 58716, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.25, #queue-req: 0, 
[2025-10-27 15:47:25 TP0] Decode batch, #running-req: 16, #token: 59356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 608.99, #queue-req: 0, 
[2025-10-27 15:47:26 TP0] Decode batch, #running-req: 16, #token: 59996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.08, #queue-req: 0, 
[2025-10-27 15:47:27 TP0] Decode batch, #running-req: 16, #token: 60636, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.93, #queue-req: 0, 
[2025-10-27 15:47:28 TP0] Decode batch, #running-req: 16, #token: 61276, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.06, #queue-req: 0, 
[2025-10-27 15:47:29 TP0] Decode batch, #running-req: 16, #token: 61916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.01, #queue-req: 0, 
[2025-10-27 15:47:30 TP0] Decode batch, #running-req: 16, #token: 62556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.87, #queue-req: 0, 
[2025-10-27 15:47:31 TP0] Decode batch, #running-req: 16, #token: 63196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.91, #queue-req: 0, 
[2025-10-27 15:47:32 TP0] Decode batch, #running-req: 16, #token: 63836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.01, #queue-req: 0, 
[2025-10-27 15:47:32] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:47:32] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:47:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:47:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:47:36 TP0] Decode batch, #running-req: 16, #token: 51678, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.65, #queue-req: 0, 
[2025-10-27 15:47:37 TP0] Decode batch, #running-req: 16, #token: 52318, token usage: 0.05, cuda graph: True, gen throughput (token/s): 626.11, #queue-req: 0, 
[2025-10-27 15:47:38 TP0] Decode batch, #running-req: 16, #token: 52958, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.57, #queue-req: 0, 
[2025-10-27 15:47:39 TP0] Decode batch, #running-req: 16, #token: 53598, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.20, #queue-req: 0, 
[2025-10-27 15:47:40 TP0] Decode batch, #running-req: 16, #token: 54238, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.26, #queue-req: 0, 
[2025-10-27 15:47:41 TP0] Decode batch, #running-req: 16, #token: 54878, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.16, #queue-req: 0, 
[2025-10-27 15:47:42 TP0] Decode batch, #running-req: 16, #token: 55518, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.30, #queue-req: 0, 
[2025-10-27 15:47:43 TP0] Decode batch, #running-req: 16, #token: 56158, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.86, #queue-req: 0, 
[2025-10-27 15:47:44 TP0] Decode batch, #running-req: 16, #token: 56798, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.12, #queue-req: 0, 
[2025-10-27 15:47:45 TP0] Decode batch, #running-req: 16, #token: 57438, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.77, #queue-req: 0, 
[2025-10-27 15:47:46 TP0] Decode batch, #running-req: 16, #token: 58078, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.26, #queue-req: 0, 
[2025-10-27 15:47:47 TP0] Decode batch, #running-req: 16, #token: 58718, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.05, #queue-req: 0, 
[2025-10-27 15:47:48 TP0] Decode batch, #running-req: 16, #token: 59358, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.10, #queue-req: 0, 
[2025-10-27 15:47:49 TP0] Decode batch, #running-req: 16, #token: 59998, token usage: 0.05, cuda graph: True, gen throughput (token/s): 608.60, #queue-req: 0, 
[2025-10-27 15:47:50 TP0] Decode batch, #running-req: 16, #token: 60638, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.21, #queue-req: 0, 
[2025-10-27 15:47:51 TP0] Decode batch, #running-req: 16, #token: 61278, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.65, #queue-req: 0, 
[2025-10-27 15:47:52 TP0] Decode batch, #running-req: 16, #token: 61918, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.79, #queue-req: 0, 
[2025-10-27 15:47:53 TP0] Decode batch, #running-req: 16, #token: 62558, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.03, #queue-req: 0, 
[2025-10-27 15:47:54 TP0] Decode batch, #running-req: 16, #token: 63198, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.78, #queue-req: 0, 
[2025-10-27 15:47:56 TP0] Decode batch, #running-req: 16, #token: 63838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.75, #queue-req: 0, 
[2025-10-27 15:47:56] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:47:56] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:47:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:47:56 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 6, #queue-req: 4, 
[2025-10-27 15:47:57 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.03, #running-req: 12, #queue-req: 0, 
[2025-10-27 15:47:59 TP0] Decode batch, #running-req: 16, #token: 51681, token usage: 0.05, cuda graph: True, gen throughput (token/s): 180.98, #queue-req: 0, 
[2025-10-27 15:48:00 TP0] Decode batch, #running-req: 16, #token: 52321, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.44, #queue-req: 0, 
[2025-10-27 15:48:01 TP0] Decode batch, #running-req: 16, #token: 52961, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.14, #queue-req: 0, 
[2025-10-27 15:48:02 TP0] Decode batch, #running-req: 16, #token: 53601, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.25, #queue-req: 0, 
[2025-10-27 15:48:03 TP0] Decode batch, #running-req: 16, #token: 54241, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.56, #queue-req: 0, 
[2025-10-27 15:48:04 TP0] Decode batch, #running-req: 16, #token: 54881, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.42, #queue-req: 0, 
[2025-10-27 15:48:05 TP0] Decode batch, #running-req: 16, #token: 55521, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.37, #queue-req: 0, 
[2025-10-27 15:48:06 TP0] Decode batch, #running-req: 16, #token: 56161, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.71, #queue-req: 0, 
[2025-10-27 15:48:07 TP0] Decode batch, #running-req: 16, #token: 56801, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.13, #queue-req: 0, 
[2025-10-27 15:48:08 TP0] Decode batch, #running-req: 16, #token: 57441, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.18, #queue-req: 0, 
[2025-10-27 15:48:09 TP0] Decode batch, #running-req: 16, #token: 58081, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.92, #queue-req: 0, 
[2025-10-27 15:48:11 TP0] Decode batch, #running-req: 16, #token: 58721, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.41, #queue-req: 0, 
[2025-10-27 15:48:12 TP0] Decode batch, #running-req: 16, #token: 59361, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.24, #queue-req: 0, 
[2025-10-27 15:48:13 TP0] Decode batch, #running-req: 16, #token: 60001, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.61, #queue-req: 0, 
[2025-10-27 15:48:14 TP0] Decode batch, #running-req: 16, #token: 60641, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-10-27 15:48:15 TP0] Decode batch, #running-req: 16, #token: 61281, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.58, #queue-req: 0, 
[2025-10-27 15:48:16 TP0] Decode batch, #running-req: 16, #token: 61921, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.32, #queue-req: 0, 
[2025-10-27 15:48:17 TP0] Decode batch, #running-req: 16, #token: 62561, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.73, #queue-req: 0, 
[2025-10-27 15:48:18 TP0] Decode batch, #running-req: 16, #token: 63201, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.10, #queue-req: 0, 
[2025-10-27 15:48:19 TP0] Decode batch, #running-req: 16, #token: 63841, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.05, #queue-req: 0, 
[2025-10-27 15:48:19] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:48:19] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:48:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:48:20 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:48:23 TP0] Decode batch, #running-req: 16, #token: 51676, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.92, #queue-req: 0, 
[2025-10-27 15:48:24 TP0] Decode batch, #running-req: 16, #token: 52316, token usage: 0.05, cuda graph: True, gen throughput (token/s): 624.54, #queue-req: 0, 
[2025-10-27 15:48:25 TP0] Decode batch, #running-req: 16, #token: 52956, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.74, #queue-req: 0, 
[2025-10-27 15:48:26 TP0] Decode batch, #running-req: 16, #token: 53596, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.13, #queue-req: 0, 
[2025-10-27 15:48:27 TP0] Decode batch, #running-req: 16, #token: 54236, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.05, #queue-req: 0, 
[2025-10-27 15:48:28 TP0] Decode batch, #running-req: 16, #token: 54876, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.11, #queue-req: 0, 
[2025-10-27 15:48:29 TP0] Decode batch, #running-req: 16, #token: 55516, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.77, #queue-req: 0, 
[2025-10-27 15:48:30 TP0] Decode batch, #running-req: 16, #token: 56156, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.50, #queue-req: 0, 
[2025-10-27 15:48:31 TP0] Decode batch, #running-req: 16, #token: 56796, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.73, #queue-req: 0, 
[2025-10-27 15:48:32 TP0] Decode batch, #running-req: 16, #token: 57436, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.38, #queue-req: 0, 
[2025-10-27 15:48:33 TP0] Decode batch, #running-req: 16, #token: 58076, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.62, #queue-req: 0, 
[2025-10-27 15:48:34 TP0] Decode batch, #running-req: 16, #token: 58716, token usage: 0.05, cuda graph: True, gen throughput (token/s): 618.70, #queue-req: 0, 
[2025-10-27 15:48:35 TP0] Decode batch, #running-req: 16, #token: 59356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.20, #queue-req: 0, 
[2025-10-27 15:48:36 TP0] Decode batch, #running-req: 16, #token: 59996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.98, #queue-req: 0, 
[2025-10-27 15:48:37 TP0] Decode batch, #running-req: 16, #token: 60636, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.77, #queue-req: 0, 
[2025-10-27 15:48:38 TP0] Decode batch, #running-req: 16, #token: 61276, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.48, #queue-req: 0, 
[2025-10-27 15:48:39 TP0] Decode batch, #running-req: 16, #token: 61916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.44, #queue-req: 0, 
[2025-10-27 15:48:40 TP0] Decode batch, #running-req: 16, #token: 62556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.19, #queue-req: 0, 
[2025-10-27 15:48:41 TP0] Decode batch, #running-req: 16, #token: 63196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.40, #queue-req: 0, 
[2025-10-27 15:48:42 TP0] Decode batch, #running-req: 16, #token: 63836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.43, #queue-req: 0, 
[2025-10-27 15:48:43] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:48:43] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:48:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:48:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:48:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:48:46 TP0] Decode batch, #running-req: 16, #token: 51676, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.02, #queue-req: 0, 
[2025-10-27 15:48:47 TP0] Decode batch, #running-req: 16, #token: 52316, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.36, #queue-req: 0, 
[2025-10-27 15:48:48 TP0] Decode batch, #running-req: 16, #token: 52956, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.80, #queue-req: 0, 
[2025-10-27 15:48:49 TP0] Decode batch, #running-req: 16, #token: 53596, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.61, #queue-req: 0, 
[2025-10-27 15:48:50 TP0] Decode batch, #running-req: 16, #token: 54236, token usage: 0.05, cuda graph: True, gen throughput (token/s): 613.94, #queue-req: 0, 
[2025-10-27 15:48:51 TP0] Decode batch, #running-req: 16, #token: 54876, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.64, #queue-req: 0, 
[2025-10-27 15:48:52 TP0] Decode batch, #running-req: 16, #token: 55516, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.34, #queue-req: 0, 
[2025-10-27 15:48:53 TP0] Decode batch, #running-req: 16, #token: 56156, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.38, #queue-req: 0, 
[2025-10-27 15:48:54 TP0] Decode batch, #running-req: 16, #token: 56796, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.15, #queue-req: 0, 
[2025-10-27 15:48:55 TP0] Decode batch, #running-req: 16, #token: 57436, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.02, #queue-req: 0, 
[2025-10-27 15:48:56 TP0] Decode batch, #running-req: 16, #token: 58076, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.87, #queue-req: 0, 
[2025-10-27 15:48:57 TP0] Decode batch, #running-req: 16, #token: 58716, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.54, #queue-req: 0, 
[2025-10-27 15:48:59 TP0] Decode batch, #running-req: 16, #token: 59356, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.78, #queue-req: 0, 
[2025-10-27 15:49:00 TP0] Decode batch, #running-req: 16, #token: 59996, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.92, #queue-req: 0, 
[2025-10-27 15:49:01 TP0] Decode batch, #running-req: 16, #token: 60636, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.10, #queue-req: 0, 
[2025-10-27 15:49:02 TP0] Decode batch, #running-req: 16, #token: 61276, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.15, #queue-req: 0, 
[2025-10-27 15:49:03 TP0] Decode batch, #running-req: 16, #token: 61916, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.29, #queue-req: 0, 
[2025-10-27 15:49:04 TP0] Decode batch, #running-req: 16, #token: 62556, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.98, #queue-req: 0, 
[2025-10-27 15:49:05 TP0] Decode batch, #running-req: 16, #token: 63196, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.63, #queue-req: 0, 
[2025-10-27 15:49:06 TP0] Decode batch, #running-req: 16, #token: 63836, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.98, #queue-req: 0, 
[2025-10-27 15:49:06] INFO:     127.0.0.1:36478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:49:06] INFO:     127.0.0.1:36500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06] INFO:     127.0.0.1:36630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:49:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:49:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:49:10 TP0] Decode batch, #running-req: 16, #token: 51678, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.49, #queue-req: 0, 
[2025-10-27 15:49:11 TP0] Decode batch, #running-req: 16, #token: 52318, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.88, #queue-req: 0, 
[2025-10-27 15:49:12 TP0] Decode batch, #running-req: 16, #token: 52958, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.33, #queue-req: 0, 
[2025-10-27 15:49:13 TP0] Decode batch, #running-req: 16, #token: 53598, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.15, #queue-req: 0, 
[2025-10-27 15:49:14 TP0] Decode batch, #running-req: 16, #token: 54238, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.09, #queue-req: 0, 
[2025-10-27 15:49:15 TP0] Decode batch, #running-req: 16, #token: 54878, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.38, #queue-req: 0, 
[2025-10-27 15:49:16 TP0] Decode batch, #running-req: 16, #token: 55518, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.49, #queue-req: 0, 
[2025-10-27 15:49:17 TP0] Decode batch, #running-req: 16, #token: 56158, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.11, #queue-req: 0, 
[2025-10-27 15:49:18 TP0] Decode batch, #running-req: 16, #token: 56798, token usage: 0.05, cuda graph: True, gen throughput (token/s): 608.64, #queue-req: 0, 
[2025-10-27 15:49:19 TP0] Decode batch, #running-req: 16, #token: 57438, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.74, #queue-req: 0, 
[2025-10-27 15:49:20 TP0] Decode batch, #running-req: 16, #token: 58078, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.05, #queue-req: 0, 
[2025-10-27 15:49:21 TP0] Decode batch, #running-req: 16, #token: 58718, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.29, #queue-req: 0, 
[2025-10-27 15:49:22 TP0] Decode batch, #running-req: 16, #token: 59358, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.91, #queue-req: 0, 
[2025-10-27 15:49:23 TP0] Decode batch, #running-req: 16, #token: 59998, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.16, #queue-req: 0, 
[2025-10-27 15:49:24 TP0] Decode batch, #running-req: 16, #token: 60638, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.41, #queue-req: 0, 
[2025-10-27 15:49:25 TP0] Decode batch, #running-req: 16, #token: 61278, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.61, #queue-req: 0, 
[2025-10-27 15:49:26 TP0] Decode batch, #running-req: 16, #token: 61918, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.58, #queue-req: 0, 
[2025-10-27 15:49:27 TP0] Decode batch, #running-req: 16, #token: 62558, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.48, #queue-req: 0, 
[2025-10-27 15:49:28 TP0] Decode batch, #running-req: 16, #token: 63198, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.31, #queue-req: 0, 
[2025-10-27 15:49:29 TP0] Decode batch, #running-req: 16, #token: 63838, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.37, #queue-req: 0, 
[2025-10-27 15:49:30] INFO:     127.0.0.1:47940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:49:30] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30] INFO:     127.0.0.1:48078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:49:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-10-27 15:49:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-10-27 15:49:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.03, #running-req: 11, #queue-req: 0, 
[2025-10-27 15:49:33 TP0] Decode batch, #running-req: 16, #token: 51679, token usage: 0.05, cuda graph: True, gen throughput (token/s): 173.66, #queue-req: 0, 
[2025-10-27 15:49:34 TP0] Decode batch, #running-req: 16, #token: 52319, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.17, #queue-req: 0, 
[2025-10-27 15:49:35 TP0] Decode batch, #running-req: 16, #token: 52959, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.12, #queue-req: 0, 
[2025-10-27 15:49:36 TP0] Decode batch, #running-req: 16, #token: 53599, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.52, #queue-req: 0, 
[2025-10-27 15:49:37 TP0] Decode batch, #running-req: 16, #token: 54239, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.55, #queue-req: 0, 
[2025-10-27 15:49:38 TP0] Decode batch, #running-req: 16, #token: 54879, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.44, #queue-req: 0, 
[2025-10-27 15:49:39 TP0] Decode batch, #running-req: 16, #token: 55519, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.43, #queue-req: 0, 
[2025-10-27 15:49:40 TP0] Decode batch, #running-req: 16, #token: 56159, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.35, #queue-req: 0, 
[2025-10-27 15:49:42 TP0] Decode batch, #running-req: 16, #token: 56799, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.26, #queue-req: 0, 
[2025-10-27 15:49:43 TP0] Decode batch, #running-req: 16, #token: 57439, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.85, #queue-req: 0, 
[2025-10-27 15:49:44 TP0] Decode batch, #running-req: 16, #token: 58079, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.63, #queue-req: 0, 
[2025-10-27 15:49:45 TP0] Decode batch, #running-req: 16, #token: 58719, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.04, #queue-req: 0, 
[2025-10-27 15:49:46 TP0] Decode batch, #running-req: 16, #token: 59359, token usage: 0.05, cuda graph: True, gen throughput (token/s): 608.52, #queue-req: 0, 
[2025-10-27 15:49:47 TP0] Decode batch, #running-req: 16, #token: 59999, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.00, #queue-req: 0, 
[2025-10-27 15:49:48 TP0] Decode batch, #running-req: 16, #token: 60639, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.55, #queue-req: 0, 
[2025-10-27 15:49:49 TP0] Decode batch, #running-req: 16, #token: 61279, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.39, #queue-req: 0, 
[2025-10-27 15:49:50 TP0] Decode batch, #running-req: 16, #token: 61919, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.06, #queue-req: 0, 
[2025-10-27 15:49:51 TP0] Decode batch, #running-req: 16, #token: 62559, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.98, #queue-req: 0, 
[2025-10-27 15:49:52 TP0] Decode batch, #running-req: 16, #token: 63199, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.78, #queue-req: 0, 
[2025-10-27 15:49:53 TP0] Decode batch, #running-req: 16, #token: 63839, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.95, #queue-req: 0, 
[2025-10-27 15:49:53] INFO:     127.0.0.1:42690 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-27 15:50:10] INFO:     127.0.0.1:53670 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-27 15:50:16] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:50:17 TP0] Decode batch, #running-req: 1, #token: 3232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 8.06, #queue-req: 0, 
[2025-10-27 15:50:18] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:18] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:50:18] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:18] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:18 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:50:19 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 77.17, #queue-req: 0, 
[2025-10-27 15:50:19 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:50:20 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:50:21 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:50:22 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:50:23 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:50:24 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:50:24 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:50:25 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:50:26 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:50:27 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:50:28 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:50:29 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:50:29 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:50:30 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:50:31 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:50:32 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:50:33 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:50:34 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:50:34 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:50:34] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:34] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:34] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:50:34] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:34 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:50:35 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.55, #queue-req: 0, 
[2025-10-27 15:50:36 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:50:37 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:50:38 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:50:39 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:50:39 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:50:40 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-27 15:50:41 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:50:42 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:50:43 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:50:44 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:50:44 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:50:45 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:50:46 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:50:47 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:50:48 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:50:49 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:50:49 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:50:50 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:50:51 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:50:51] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:51] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:51] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:50:51] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:50:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-27 15:50:52 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.63, #queue-req: 0, 
[2025-10-27 15:50:53 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:50:54 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:50:55 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:50:55 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:50:56 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:50:57 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:50:58 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:50:59 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:50:59 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:51:00 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:51:01 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:51:02 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:51:03 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:51:04 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:51:04 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-27 15:51:05 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:51:06 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:07 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:51:08 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:51:08] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:08] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:08] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:51:08] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-27 15:51:09 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.48, #queue-req: 0, 
[2025-10-27 15:51:10 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:51:10 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:51:11 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:51:12 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:51:13 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:51:14 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-27 15:51:15 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:51:15 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:51:16 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:17 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:51:18 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:51:19 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:20 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:20 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:51:21 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:51:22 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:51:23 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:24 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:51:24 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:51:25] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:25] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:25] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:51:25] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:25 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:51:25 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.64, #queue-req: 0, 
[2025-10-27 15:51:26 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:51:27 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:51:28 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-27 15:51:29 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-10-27 15:51:30 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:51:30 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:51:31 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:51:32 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:51:33 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-27 15:51:34 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:51:35 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:51:35 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-27 15:51:36 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:51:37 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-27 15:51:38 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:51:39 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:51:40 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:51:40 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:51:41 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-27 15:51:41] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:41] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:51:41] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:41] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:51:42 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.41, #queue-req: 0, 
[2025-10-27 15:51:43 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-27 15:51:44 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:51:45 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:51:46 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:51:46 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:51:47 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:51:48 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:51:49 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:51:50 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:51:50 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:51:51 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:51:52 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:51:53 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:51:54 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:51:55 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:51:55 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:51:56 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:51:57 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:51:58 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:51:58] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:58] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:51:58] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:58] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:51:58 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:51:59 TP0] Decode batch, #running-req: 4, #token: 12952, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.89, #queue-req: 0, 
[2025-10-27 15:52:00 TP0] Decode batch, #running-req: 4, #token: 13112, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-27 15:52:01 TP0] Decode batch, #running-req: 4, #token: 13272, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.81, #queue-req: 0, 
[2025-10-27 15:52:01 TP0] Decode batch, #running-req: 4, #token: 13432, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-27 15:52:02 TP0] Decode batch, #running-req: 4, #token: 13592, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:52:03 TP0] Decode batch, #running-req: 4, #token: 13752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:52:04 TP0] Decode batch, #running-req: 4, #token: 13912, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-27 15:52:05 TP0] Decode batch, #running-req: 4, #token: 14072, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-27 15:52:06 TP0] Decode batch, #running-req: 4, #token: 14232, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-27 15:52:06 TP0] Decode batch, #running-req: 4, #token: 14392, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:52:07 TP0] Decode batch, #running-req: 4, #token: 14552, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-27 15:52:08 TP0] Decode batch, #running-req: 4, #token: 14712, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:52:09 TP0] Decode batch, #running-req: 4, #token: 14872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-27 15:52:10 TP0] Decode batch, #running-req: 4, #token: 15032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-27 15:52:10 TP0] Decode batch, #running-req: 4, #token: 15192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-27 15:52:11 TP0] Decode batch, #running-req: 4, #token: 15352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-27 15:52:12 TP0] Decode batch, #running-req: 4, #token: 15512, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:52:13 TP0] Decode batch, #running-req: 4, #token: 15672, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:52:14 TP0] Decode batch, #running-req: 4, #token: 15832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:52:15 TP0] Decode batch, #running-req: 4, #token: 15992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:52:15] INFO:     127.0.0.1:34064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:15] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:52:15] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:15] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:15 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:52:16 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.48, #queue-req: 0, 
[2025-10-27 15:52:16 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-27 15:52:17 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:52:18 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:52:19 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:52:20 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:52:21 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:52:21 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:52:22 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:52:23 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:52:24 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:52:25 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:52:26 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:52:26 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:52:27 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:52:28 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:52:29 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:52:30 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:52:30 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:52:31 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:52:31] INFO:     127.0.0.1:60518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:31] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:31] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:52:31] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:31 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:52:32 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.13, #queue-req: 0, 
[2025-10-27 15:52:33 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:52:34 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:52:35 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:52:36 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:52:36 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:52:37 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:52:38 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:52:39 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:52:40 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:52:41 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:52:41 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:52:42 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:52:43 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:52:44 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:52:45 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-27 15:52:46 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-27 15:52:46 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:52:47 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-27 15:52:48 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:52:48] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:48] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:48] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:52:48] INFO:     127.0.0.1:46934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:52:48 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:52:49 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-27 15:52:50 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:52:51 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:52:52 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:52:52 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:52:53 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:52:54 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:52:55 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:52:56 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:52:56 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:52:57 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:52:58 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:52:59 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:53:00 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:53:01 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:53:01 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:53:02 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:53:03 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:53:04 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:53:05 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:53:05] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:05] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:53:05] INFO:     127.0.0.1:33792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:05] INFO:     127.0.0.1:33808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:05 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:53:06 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.75, #queue-req: 0, 
[2025-10-27 15:53:07 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:53:07 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:53:08 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:53:09 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:53:10 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:53:11 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:53:12 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:53:12 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:53:13 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:53:14 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:53:15 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:53:16 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:53:16 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:53:17 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-27 15:53:18 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:53:19 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:53:20 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:53:21 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:53:21 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:53:21] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:21] INFO:     127.0.0.1:36798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:21] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:21 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:53:22] INFO:     127.0.0.1:36818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:22 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-27 15:53:22 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.44, #queue-req: 0, 
[2025-10-27 15:53:23 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:53:24 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:53:25 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:53:26 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:53:27 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:27 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:28 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:53:29 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:53:30 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:53:31 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:53:32 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:53:32 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:53:33 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:53:34 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-27 15:53:35 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:53:36 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:53:37 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:53:37 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:53:38 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:53:38] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:38] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:53:38] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:38] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:38 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:53:39 TP0] Decode batch, #running-req: 4, #token: 12953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.62, #queue-req: 0, 
[2025-10-27 15:53:40 TP0] Decode batch, #running-req: 4, #token: 13113, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:53:41 TP0] Decode batch, #running-req: 4, #token: 13273, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:53:42 TP0] Decode batch, #running-req: 4, #token: 13433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:53:42 TP0] Decode batch, #running-req: 4, #token: 13593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:53:43 TP0] Decode batch, #running-req: 4, #token: 13753, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-27 15:53:44 TP0] Decode batch, #running-req: 4, #token: 13913, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-10-27 15:53:45 TP0] Decode batch, #running-req: 4, #token: 14073, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:53:46 TP0] Decode batch, #running-req: 4, #token: 14233, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:53:47 TP0] Decode batch, #running-req: 4, #token: 14393, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:53:47 TP0] Decode batch, #running-req: 4, #token: 14553, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:53:48 TP0] Decode batch, #running-req: 4, #token: 14713, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-27 15:53:49 TP0] Decode batch, #running-req: 4, #token: 14873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:50 TP0] Decode batch, #running-req: 4, #token: 15033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:51 TP0] Decode batch, #running-req: 4, #token: 15193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:52 TP0] Decode batch, #running-req: 4, #token: 15353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:53:52 TP0] Decode batch, #running-req: 4, #token: 15513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:53:53 TP0] Decode batch, #running-req: 4, #token: 15673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:53:54 TP0] Decode batch, #running-req: 4, #token: 15833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:53:55 TP0] Decode batch, #running-req: 4, #token: 15993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:53:55] INFO:     127.0.0.1:41254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:55] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:55] INFO:     127.0.0.1:41272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:55 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:53:55] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:53:55 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-27 15:53:56 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.24, #queue-req: 0, 
[2025-10-27 15:53:57 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:53:58 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:53:58 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-27 15:53:59 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-10-27 15:54:00 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-27 15:54:01 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:54:02 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-27 15:54:03 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:54:03 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:54:04 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-27 15:54:05 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-27 15:54:06 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-27 15:54:07 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-27 15:54:07 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-27 15:54:08 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-27 15:54:09 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-10-27 15:54:10 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:54:11 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:54:12 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-27 15:54:12] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:12] INFO:     127.0.0.1:37444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:54:12] INFO:     127.0.0.1:37452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:12] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:12 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:54:13 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.00, #queue-req: 0, 
[2025-10-27 15:54:13 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:54:14 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:54:15 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:54:16 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:54:17 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:54:18 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:54:18 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:54:19 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-27 15:54:20 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:21 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:54:22 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:23 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:23 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:54:24 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:25 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:54:26 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-10-27 15:54:27 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:54:28 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:28 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:54:28] INFO:     127.0.0.1:45816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:28] INFO:     127.0.0.1:45822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:28] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:54:28] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:28 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:54:29 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.31, #queue-req: 0, 
[2025-10-27 15:54:30 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:54:31 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:54:32 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:54:33 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:54:33 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:54:34 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:54:35 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:54:36 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:54:37 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:54:38 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:54:38 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:54:39 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:54:40 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:54:41 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:54:42 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:54:43 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:54:43 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:54:44 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:54:45 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:54:45] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:45] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:54:45] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:45] INFO:     127.0.0.1:58062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:54:45 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:54:46 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.69, #queue-req: 0, 
[2025-10-27 15:54:47 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-27 15:54:48 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-27 15:54:49 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-27 15:54:49 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:54:50 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:54:51 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:54:52 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:54:53 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:54:54 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:54:54 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:54:55 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:54:56 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:54:57 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:54:58 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:54:58 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:54:59 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:55:00 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:55:01 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:55:02 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:55:02] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:02] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:55:02] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:02] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:02 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:55:03 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.57, #queue-req: 0, 
[2025-10-27 15:55:04 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-27 15:55:04 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:55:05 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:55:06 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:55:07 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:55:08 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-10-27 15:55:09 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:55:09 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:55:10 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:55:11 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:55:12 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:55:13 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:55:14 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:55:14 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:55:15 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:55:16 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:55:17 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:55:18 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:55:18 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:55:19] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:19] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:55:19] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:19] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:19 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:55:19 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.55, #queue-req: 0, 
[2025-10-27 15:55:20 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-27 15:55:21 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:55:22 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:55:23 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:55:24 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:55:24 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:55:25 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:55:26 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:55:27 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:55:28 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:29 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:29 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:55:30 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:31 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:55:32 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:33 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:55:34 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:55:34 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:35 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:55:35] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:35] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:55:35] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:35] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:35 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:55:36 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.57, #queue-req: 0, 
[2025-10-27 15:55:37 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:55:38 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-10-27 15:55:39 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:55:39 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:55:40 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:55:41 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-10-27 15:55:42 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:55:43 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:55:44 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:55:44 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:55:45 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:55:46 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:55:47 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:55:48 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-27 15:55:49 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:55:49 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:55:50 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-27 15:55:51 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-10-27 15:55:52 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-10-27 15:55:52] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:52] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:55:52] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:52] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:55:52 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:55:53 TP0] Decode batch, #running-req: 4, #token: 12953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.23, #queue-req: 0, 
[2025-10-27 15:55:54 TP0] Decode batch, #running-req: 4, #token: 13113, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-27 15:55:55 TP0] Decode batch, #running-req: 4, #token: 13273, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:55:55 TP0] Decode batch, #running-req: 4, #token: 13433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:55:56 TP0] Decode batch, #running-req: 4, #token: 13593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:55:57 TP0] Decode batch, #running-req: 4, #token: 13753, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:55:58 TP0] Decode batch, #running-req: 4, #token: 13913, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:55:59 TP0] Decode batch, #running-req: 4, #token: 14073, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:56:00 TP0] Decode batch, #running-req: 4, #token: 14233, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:56:00 TP0] Decode batch, #running-req: 4, #token: 14393, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:56:01 TP0] Decode batch, #running-req: 4, #token: 14553, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:02 TP0] Decode batch, #running-req: 4, #token: 14713, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:56:03 TP0] Decode batch, #running-req: 4, #token: 14873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:04 TP0] Decode batch, #running-req: 4, #token: 15033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:56:04 TP0] Decode batch, #running-req: 4, #token: 15193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:56:05 TP0] Decode batch, #running-req: 4, #token: 15353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-10-27 15:56:06 TP0] Decode batch, #running-req: 4, #token: 15513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:56:07 TP0] Decode batch, #running-req: 4, #token: 15673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:56:08 TP0] Decode batch, #running-req: 4, #token: 15833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:56:09 TP0] Decode batch, #running-req: 4, #token: 15993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:56:09] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:09] INFO:     127.0.0.1:41382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:56:09] INFO:     127.0.0.1:41394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:09] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:09 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:56:10 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.54, #queue-req: 0, 
[2025-10-27 15:56:10 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:56:11 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:56:12 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:56:13 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:56:14 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:56:15 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-10-27 15:56:15 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-10-27 15:56:16 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:56:17 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:56:18 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:19 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:56:20 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:20 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:21 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:56:22 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:56:23 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:56:24 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:56:25 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:56:25 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:56:25] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:25] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:25] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:56:25] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-10-27 15:56:26 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.38, #queue-req: 0, 
[2025-10-27 15:56:27 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-27 15:56:28 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:56:29 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:30 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:56:30 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:56:31 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:32 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:33 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:34 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:35 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:35 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:36 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:56:37 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:56:38 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:39 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:56:40 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:56:40 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:41 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:56:42 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:42] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:42] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:42] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:56:42] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:56:43 TP0] Decode batch, #running-req: 4, #token: 12953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.80, #queue-req: 0, 
[2025-10-27 15:56:44 TP0] Decode batch, #running-req: 4, #token: 13113, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.82, #queue-req: 0, 
[2025-10-27 15:56:45 TP0] Decode batch, #running-req: 4, #token: 13273, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:56:46 TP0] Decode batch, #running-req: 4, #token: 13433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-27 15:56:46 TP0] Decode batch, #running-req: 4, #token: 13593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:56:47 TP0] Decode batch, #running-req: 4, #token: 13753, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:56:48 TP0] Decode batch, #running-req: 4, #token: 13913, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:56:49 TP0] Decode batch, #running-req: 4, #token: 14073, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:56:50 TP0] Decode batch, #running-req: 4, #token: 14233, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:56:50 TP0] Decode batch, #running-req: 4, #token: 14393, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:56:51 TP0] Decode batch, #running-req: 4, #token: 14553, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:56:52 TP0] Decode batch, #running-req: 4, #token: 14713, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:56:53 TP0] Decode batch, #running-req: 4, #token: 14873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:56:54 TP0] Decode batch, #running-req: 4, #token: 15033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:56:55 TP0] Decode batch, #running-req: 4, #token: 15193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:56:55 TP0] Decode batch, #running-req: 4, #token: 15353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:56:56 TP0] Decode batch, #running-req: 4, #token: 15513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:56:57 TP0] Decode batch, #running-req: 4, #token: 15673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:56:58 TP0] Decode batch, #running-req: 4, #token: 15833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:56:59 TP0] Decode batch, #running-req: 4, #token: 15993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-10-27 15:56:59] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:59] INFO:     127.0.0.1:43246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:56:59] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:59] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:56:59 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:57:00 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.90, #queue-req: 0, 
[2025-10-27 15:57:01 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:57:01 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:57:02 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:57:03 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:57:04 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:57:05 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:57:06 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-10-27 15:57:06 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-10-27 15:57:07 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-10-27 15:57:08 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-10-27 15:57:09 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:57:10 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:57:10 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-10-27 15:57:11 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-10-27 15:57:12 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:57:13 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:57:14 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-10-27 15:57:15 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:57:15 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-10-27 15:57:15] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:15] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:15] INFO:     127.0.0.1:50618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:57:16] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:16 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:57:16 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.07, #queue-req: 0, 
[2025-10-27 15:57:17 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:57:18 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:19 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:20 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:21 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:21 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:57:22 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-10-27 15:57:23 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:57:24 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-10-27 15:57:25 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:57:26 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-10-27 15:57:26 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-10-27 15:57:27 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-10-27 15:57:28 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-10-27 15:57:29 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-27 15:57:30 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-10-27 15:57:31 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-27 15:57:31 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.13, #queue-req: 0, 
[2025-10-27 15:57:32 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.12, #queue-req: 0, 
[2025-10-27 15:57:32] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:32] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:57:32] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:32] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:32 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:57:33 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.35, #queue-req: 0, 
[2025-10-27 15:57:34 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-10-27 15:57:35 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-10-27 15:57:36 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-10-27 15:57:36 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:57:37 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-10-27 15:57:38 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:57:39 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-10-27 15:57:40 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:57:41 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:41 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:57:42 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-10-27 15:57:43 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-10-27 15:57:44 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:57:45 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:57:46 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:57:46 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-10-27 15:57:47 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:57:48 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-10-27 15:57:49 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-10-27 15:57:49] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:49] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:57:49] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:49] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:57:49 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:57:50 TP0] Decode batch, #running-req: 4, #token: 12953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.65, #queue-req: 0, 
[2025-10-27 15:57:51 TP0] Decode batch, #running-req: 4, #token: 13113, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.84, #queue-req: 0, 
[2025-10-27 15:57:52 TP0] Decode batch, #running-req: 4, #token: 13273, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.80, #queue-req: 0, 
[2025-10-27 15:57:52 TP0] Decode batch, #running-req: 4, #token: 13433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-27 15:57:53 TP0] Decode batch, #running-req: 4, #token: 13593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:57:54 TP0] Decode batch, #running-req: 4, #token: 13753, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:57:55 TP0] Decode batch, #running-req: 4, #token: 13913, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:57:56 TP0] Decode batch, #running-req: 4, #token: 14073, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:57:57 TP0] Decode batch, #running-req: 4, #token: 14233, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:57:57 TP0] Decode batch, #running-req: 4, #token: 14393, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:57:58 TP0] Decode batch, #running-req: 4, #token: 14553, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:57:59 TP0] Decode batch, #running-req: 4, #token: 14713, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:00 TP0] Decode batch, #running-req: 4, #token: 14873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:01 TP0] Decode batch, #running-req: 4, #token: 15033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:58:01 TP0] Decode batch, #running-req: 4, #token: 15193, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:02 TP0] Decode batch, #running-req: 4, #token: 15353, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:58:03 TP0] Decode batch, #running-req: 4, #token: 15513, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:58:04 TP0] Decode batch, #running-req: 4, #token: 15673, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:05 TP0] Decode batch, #running-req: 4, #token: 15833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:06 TP0] Decode batch, #running-req: 4, #token: 15993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:06] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:06] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:58:06] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:06] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:06 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:58:07 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.79, #queue-req: 0, 
[2025-10-27 15:58:07 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-27 15:58:08 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:58:09 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.73, #queue-req: 0, 
[2025-10-27 15:58:10 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.72, #queue-req: 0, 
[2025-10-27 15:58:11 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:58:12 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:58:12 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:58:13 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:58:14 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:58:15 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:58:16 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:58:17 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:58:17 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:58:18 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:58:19 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:58:20 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:58:21 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:21 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:22 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:58:22] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:22] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:58:22] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:22] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:22 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:58:23 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.69, #queue-req: 0, 
[2025-10-27 15:58:24 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-27 15:58:25 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-10-27 15:58:26 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:58:27 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:58:27 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:58:28 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-27 15:58:29 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:58:30 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:58:31 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:32 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:32 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:58:33 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:34 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:58:35 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:36 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:58:37 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:58:37 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:38 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:39 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:39] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:39] INFO:     127.0.0.1:47106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:58:39] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:39] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:39 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:58:40 TP0] Decode batch, #running-req: 4, #token: 12954, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.76, #queue-req: 0, 
[2025-10-27 15:58:41 TP0] Decode batch, #running-req: 4, #token: 13114, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-27 15:58:42 TP0] Decode batch, #running-req: 4, #token: 13274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.76, #queue-req: 0, 
[2025-10-27 15:58:42 TP0] Decode batch, #running-req: 4, #token: 13434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.75, #queue-req: 0, 
[2025-10-27 15:58:43 TP0] Decode batch, #running-req: 4, #token: 13594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:58:44 TP0] Decode batch, #running-req: 4, #token: 13754, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-27 15:58:45 TP0] Decode batch, #running-req: 4, #token: 13914, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:58:46 TP0] Decode batch, #running-req: 4, #token: 14074, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.64, #queue-req: 0, 
[2025-10-27 15:58:47 TP0] Decode batch, #running-req: 4, #token: 14234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:58:47 TP0] Decode batch, #running-req: 4, #token: 14394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:58:48 TP0] Decode batch, #running-req: 4, #token: 14554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-10-27 15:58:49 TP0] Decode batch, #running-req: 4, #token: 14714, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-10-27 15:58:50 TP0] Decode batch, #running-req: 4, #token: 14874, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-10-27 15:58:51 TP0] Decode batch, #running-req: 4, #token: 15034, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-10-27 15:58:52 TP0] Decode batch, #running-req: 4, #token: 15194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:58:52 TP0] Decode batch, #running-req: 4, #token: 15354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:58:53 TP0] Decode batch, #running-req: 4, #token: 15514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:58:54 TP0] Decode batch, #running-req: 4, #token: 15674, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:58:55 TP0] Decode batch, #running-req: 4, #token: 15834, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:56 TP0] Decode batch, #running-req: 4, #token: 15994, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:58:56] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:56] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:58:56] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:56] INFO:     127.0.0.1:53948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:58:56 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-10-27 15:58:57 TP0] Decode batch, #running-req: 4, #token: 12952, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.82, #queue-req: 0, 
[2025-10-27 15:58:58 TP0] Decode batch, #running-req: 4, #token: 13112, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.79, #queue-req: 0, 
[2025-10-27 15:58:58 TP0] Decode batch, #running-req: 4, #token: 13272, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.78, #queue-req: 0, 
[2025-10-27 15:58:59 TP0] Decode batch, #running-req: 4, #token: 13432, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.74, #queue-req: 0, 
[2025-10-27 15:59:00 TP0] Decode batch, #running-req: 4, #token: 13592, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-10-27 15:59:01 TP0] Decode batch, #running-req: 4, #token: 13752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-10-27 15:59:02 TP0] Decode batch, #running-req: 4, #token: 13912, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-10-27 15:59:02 TP0] Decode batch, #running-req: 4, #token: 14072, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-10-27 15:59:03 TP0] Decode batch, #running-req: 4, #token: 14232, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:59:04 TP0] Decode batch, #running-req: 4, #token: 14392, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:59:05 TP0] Decode batch, #running-req: 4, #token: 14552, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-10-27 15:59:06 TP0] Decode batch, #running-req: 4, #token: 14712, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-10-27 15:59:07 TP0] Decode batch, #running-req: 4, #token: 14872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.68, #queue-req: 0, 
[2025-10-27 15:59:07 TP0] Decode batch, #running-req: 4, #token: 15032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-10-27 15:59:08 TP0] Decode batch, #running-req: 4, #token: 15192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:59:09 TP0] Decode batch, #running-req: 4, #token: 15352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-10-27 15:59:10 TP0] Decode batch, #running-req: 4, #token: 15512, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-10-27 15:59:11 TP0] Decode batch, #running-req: 4, #token: 15672, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-10-27 15:59:12 TP0] Decode batch, #running-req: 4, #token: 15832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-10-27 15:59:12 TP0] Decode batch, #running-req: 4, #token: 15992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-10-27 15:59:12] INFO:     127.0.0.1:56126 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-27 15:59:29] INFO:     127.0.0.1:47264 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-27 15:59:35] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:59:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:59:36] INFO:     127.0.0.1:37260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:59:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:59:37 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.89, #queue-req: 0, 
[2025-10-27 15:59:38 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 15:59:38 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:39 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 15:59:40 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 15:59:41 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 15:59:42 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 15:59:42 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 15:59:43 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 15:59:44 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 15:59:45 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 15:59:46 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 15:59:47 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 15:59:47 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 15:59:48 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 15:59:49 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 15:59:50 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 15:59:51 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 15:59:51 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 15:59:52 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 15:59:53] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 15:59:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 15:59:53 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 15:59:54 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:55 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:56 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:56 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:57 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 15:59:58 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 15:59:59 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:00 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:01 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:01 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:02 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:03 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:04 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:05 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:05 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:06 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:00:07 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:00:08 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:09 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:09] INFO:     127.0.0.1:46224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:00:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:00:10 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:00:10 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:00:11 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:00:12 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:00:13 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:00:14 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:15 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:00:15 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:16 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:17 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:18 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:19 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:20 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:20 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:21 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:22 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:23 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:24 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:24 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:25 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:26] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:00:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:00:26 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:00:27 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:28 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:29 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:29 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:30 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:31 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:32 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:33 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:34 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:34 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:35 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:36 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:37 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:38 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:38 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:39 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:40 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:00:41 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:00:42 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:42] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:00:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:00:43 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:00:43 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:00:44 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:00:45 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:00:46 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:00:47 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:48 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:48 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:49 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:50 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:00:51 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:00:52 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:00:52 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:53 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:54 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:55 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:56 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:57 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:57 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:00:58 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:00:59] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:00:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:00:59 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:01:00 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:01:01 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:01:02 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:01:02 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:01:03 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:04 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:05 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:06 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:07 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:07 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:08 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:09 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:10 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:11 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:11 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:12 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:13 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:14 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:15 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:15] INFO:     127.0.0.1:40262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:01:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:01:16 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:01:16 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:17 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:18 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:19 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:20 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:21 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:21 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:01:22 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:01:23 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:01:24 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:01:25 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:01:25 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:01:26 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:01:27 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:01:28 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:01:29 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:01:30 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:01:30 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:01:31 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:01:32] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:01:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:01:32 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.08, #queue-req: 0, 
[2025-10-27 16:01:33 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:34 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:35 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:35 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:36 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:37 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:38 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:39 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:01:39 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:01:40 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:01:41 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:42 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:43 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:01:44 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:44 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:45 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:01:46 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:01:47 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:01:48 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:01:48] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:01:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:01:49 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:01:49 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:01:50 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:01:51 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:01:52 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:01:53 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:01:54 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:01:54 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:01:55 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:01:56 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:01:57 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:01:58 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:01:58 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:01:59 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:00 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:01 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:02 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:03 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:03 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:04 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:05] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:02:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:02:05 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:02:06 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:02:07 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:08 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:08 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:09 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:10 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:02:11 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:12 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:12 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:13 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:14 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:15 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:16 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:17 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:17 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:18 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:19 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:20 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:21 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:21] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:02:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:02:22 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:02:22 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:02:23 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:02:24 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:02:25 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:26 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:26 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:02:27 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:28 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:29 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:02:30 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:02:31 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:31 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:02:32 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:02:33 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:34 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:35 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:36 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:36 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:37 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:38] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:02:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:02:38 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:02:39 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:02:40 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:41 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:02:41 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:42 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:43 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:02:44 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:02:45 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:02:45 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:46 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:47 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:48 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:49 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:50 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:50 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:02:51 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:52 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:02:53 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:54 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:02:54] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:02:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:02:55 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:02:55 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:56 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:57 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:02:58 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:02:59 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:02:59 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:03:00 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:03:01 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:02 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:03 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:04 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:04 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:05 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:06 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:07 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:08 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:09 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:09 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:10 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:11] INFO:     127.0.0.1:44182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:03:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:03:11 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:03:12 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:03:13 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:03:14 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:14 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:03:15 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:16 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:17 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:18 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:03:18 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:19 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:03:20 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:21 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:22 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:23 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:23 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:24 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:25 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:26 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:27 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:27] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:03:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:03:28 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:03:28 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:29 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:30 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:31 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:32 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:32 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:33 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:34 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:35 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:36 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:37 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:37 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:38 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:39 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:40 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:41 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:41 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:03:42 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:03:43 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:44] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:03:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:03:44 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:03:45 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:03:46 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:47 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:03:47 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:03:48 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:49 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:03:50 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:51 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:51 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:52 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:53 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:54 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:55 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:56 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:56 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:03:57 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:58 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:03:59 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:00 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:00] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:04:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:04:01 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:04:01 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:04:02 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:03 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:04 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:05 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:05 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:06 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:04:07 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:08 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:09 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:10 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:10 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:04:11 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:12 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:13 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:14 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:04:14 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:04:15 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:16 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:04:17] INFO:     127.0.0.1:43838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:04:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:04:17 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:04:18 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:04:19 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:04:19 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:04:20 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:21 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:04:22 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:23 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:24 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:24 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:25 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:26 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:27 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:28 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:29 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:29 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:30 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:31 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:32 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:04:33 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:04:33] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:04:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:04:34 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:04:34 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:04:35 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:04:36 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:04:37 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:38 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:38 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:04:39 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:04:40 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:41 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:42 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:43 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:43 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:04:44 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:45 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:46 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:04:47 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:04:47 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:04:48 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:04:49 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:04:50] INFO:     127.0.0.1:42298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:04:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:04:50 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:04:51 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:52 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:52 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:53 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:54 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:55 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:56 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:57 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:04:57 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:58 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:04:59 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:05:00 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:05:01 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:05:02 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:05:02 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:05:03 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:05:04 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:05:05 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:05:06 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:05:06] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:05:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:05:07 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:05:07 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:05:08 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:09 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:10 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:05:11 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:05:11 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:05:12 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:05:13 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:05:14 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:05:15 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:16 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:16 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:17 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:18 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:19 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:05:20 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:05:20 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:05:21 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:05:22 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:05:23] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:05:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:05:23 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:05:24 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:05:25 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:05:25 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:05:26 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:27 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:05:28 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:05:29 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:05:30 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:30 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:31 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:32 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:33 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:34 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:34 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:35 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:36 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:37 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:38 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:05:39 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:39] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:05:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:05:39 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:05:40 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:05:41 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:05:42 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:05:43 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:05:44 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:05:44 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:05:45 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:05:46 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:47 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:48 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:48 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:49 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:50 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:51 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:52 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:53 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:53 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:54 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:05:55 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:05:56] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:05:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:05:56 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:05:57 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:05:58 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:58 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:05:59 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:06:00 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:06:01 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:06:02 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:06:03 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:03 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:06:04 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:05 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:06 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:06:07 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:07 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:08 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:09 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:06:10 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:06:11 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:06:12 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:06:12] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:06:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:06:12 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:06:13 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:06:14 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:15 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:16 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:17 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:17 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:18 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:19 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:20 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:21 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:21 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:22 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:23 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:06:24 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:25 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:26 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:26 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:27 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:28 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:06:29] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:06:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:06:29 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-27 16:06:30 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:06:31 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:06:31 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:06:32 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:06:33 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:34 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:35 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:35 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:36 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:37 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:38 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:06:39 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:40 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:40 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:41 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:06:42 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:06:43 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:06:44 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:44 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:45] INFO:     127.0.0.1:47600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:06:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:06:45 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-27 16:06:46 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:06:47 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:48 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:06:49 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:49 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:50 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:06:51 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:52 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:06:53 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:06:54 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:06:54 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:06:55 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:06:56 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:06:57 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:06:58 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:06:58 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:06:59 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:00 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:01 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:02] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:07:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:07:02 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:07:03 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:03 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:04 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:05 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:06 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:07 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:08 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:07:08 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:09 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:07:10 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:11 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:12 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:07:12 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:13 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:07:14 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:07:15 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:07:16 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:07:17 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:07:17 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:07:18] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:07:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:07:18 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:07:19 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:07:20 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:07:21 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:07:22 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:07:22 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:07:23 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:07:24 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:07:25 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:07:26 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:07:26 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:07:27 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:07:28 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:07:29 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:07:30 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:07:31 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:07:31 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:32 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:33 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:34 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:35] INFO:     127.0.0.1:34302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:07:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:07:35 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.86, #queue-req: 0, 
[2025-10-27 16:07:36 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:07:36 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:07:37 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:07:38 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:39 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:40 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:07:40 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:41 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:42 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:43 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:07:44 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:45 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:07:45 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:46 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:47 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:48 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:07:49 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:50 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:07:50 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:07:51] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:07:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:07:51 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:07:52 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:07:53 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:07:54 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:07:55 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:07:55 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:07:56 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:07:57 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:07:58 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:07:59 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:07:59 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:00 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:01 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:02 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:03 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:04 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:04 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:05 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:06 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:07 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:07] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:08:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:08:08 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.86, #queue-req: 0, 
[2025-10-27 16:08:09 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:08:09 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:08:10 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:08:11 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:12 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:08:13 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:13 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:08:14 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:15 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:16 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:17 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:18 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:08:18 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:19 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:20 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:21 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:22 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:22 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:23 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:24] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:08:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:08:24 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:08:25 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:08:26 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:27 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:08:27 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:28 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:08:29 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:08:30 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:31 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:32 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:32 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:33 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:34 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:35 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:36 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:37 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:37 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:38 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:39 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:40 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:40] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:08:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:08:41 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:08:42 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:08:42 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:43 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:08:44 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:45 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:46 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:46 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:47 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:48 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:08:49 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:50 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:51 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:51 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:08:52 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:53 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:08:54 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:08:55 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:08:55 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:08:56 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:08:57] INFO:     127.0.0.1:35936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:08:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:08:57 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.14, #queue-req: 0, 
[2025-10-27 16:08:58 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:08:59 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:00 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:00 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:01 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:02 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:03 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:04 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:05 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:05 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:06 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:07 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:08 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:09 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:10 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:09:10 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:09:11 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:12 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:13 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:09:13] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:09:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:09:14 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:09:15 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:09:15 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:09:16 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:09:17 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:09:18 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:19 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:19 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:20 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:21 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:22 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:23 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:24 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:24 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:25 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:26 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:27 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:28 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:28 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:29 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:30] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:09:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:09:30 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:09:31 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:32 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:33 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:33 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:34 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:35 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:36 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:37 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:38 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:38 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:39 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:40 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:09:41 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:42 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:09:42 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:43 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:44 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:09:45 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:09:46 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:09:46] INFO:     127.0.0.1:38676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:09:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:09:47 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:09:47 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:09:48 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:49 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:50 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:51 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:52 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:52 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:09:53 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:54 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:55 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:56 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:57 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:57 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:09:58 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:09:59 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:00 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:01 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:01 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:02 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:03] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:10:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:10:03 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:10:04 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:05 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:06 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:06 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:07 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:08 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:09 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:10 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:11 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:11 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:12 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:13 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:14 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:15 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:10:15 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:10:16 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:17 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:10:18 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:10:19 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:10:19] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:10:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:10:20 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:10:20 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:10:21 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:10:22 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:23 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:24 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:25 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:25 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:26 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:27 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:28 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:29 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:29 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:30 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:31 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:32 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:33 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:34 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:10:34 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:35 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:36] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:10:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:10:36 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:10:37 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:38 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:39 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:39 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:40 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:41 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:10:42 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:43 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:44 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:44 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:45 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:46 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:10:47 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:10:48 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:48 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:49 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:50 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:51 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:52 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:10:52] INFO:     127.0.0.1:59392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:10:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:10:53 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:10:53 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:10:54 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:10:55 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:10:56 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:10:57 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:10:58 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:10:58 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:10:59 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:00 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:01 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:02 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:02 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:03 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:04 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:05 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:06 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:07 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:11:07 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:08 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:11:09] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:11:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:11:09 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:11:10 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:11:11 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:11:12 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:11:12 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:11:13 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:11:14 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:11:15 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:11:16 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:11:16 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:11:17 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:11:18 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:11:19 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:11:20 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:11:21 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:11:21 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:11:22 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:11:23 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:11:24 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:11:25 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:11:25] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:11:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:11:26 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:11:26 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:27 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:28 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:29 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:30 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:11:31 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:31 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:11:32 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:11:33 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:11:34 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:11:35 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:11:35 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:36 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:37 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:11:38 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:39 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:40 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:40 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:11:41 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:11:42] INFO:     127.0.0.1:58266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:11:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:11:42 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:11:43 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:11:44 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:11:45 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:11:45 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:11:46 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:11:47 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:11:48 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:11:49 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:11:49 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:11:50 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:11:51 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:11:52 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:11:53 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:11:54 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:11:54 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:11:55 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:56 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:57 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:58 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:11:58] INFO:     127.0.0.1:39606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:11:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:11:59 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:11:59 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:00 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:01 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:02 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:03 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:03 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:04 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:12:05 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:06 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:07 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:08 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:08 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:09 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:10 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:11 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:12:12 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:12 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:12:13 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:14 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:15] INFO:     127.0.0.1:50606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:12:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:12:15 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.84, #queue-req: 0, 
[2025-10-27 16:12:16 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:12:17 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:17 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:18 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:19 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:20 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:21 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:22 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:12:22 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:23 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:24 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:25 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:12:26 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:12:27 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:12:27 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:12:28 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:12:29 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:30 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:12:31 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:12:31] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:12:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:12:32 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:12:32 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:12:33 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:12:34 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:12:35 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:12:36 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:12:36 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:12:37 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:12:38 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:12:39 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:12:40 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:12:41 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:12:41 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:12:42 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:12:43 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:12:44 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:12:45 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:12:45 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-27 16:12:46 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.64, #queue-req: 0, 
[2025-10-27 16:12:47 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:12:48] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:12:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:12:48 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.11, #queue-req: 0, 
[2025-10-27 16:12:49 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:50 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:50 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:51 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:12:52 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:53 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:12:54 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:12:55 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:12:55 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:12:56 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:12:57 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:12:58 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:12:59 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:00 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:00 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:01 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:13:02 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:03 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:04 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:04] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:13:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:13:05 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:13:05 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:13:06 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:13:07 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:13:08 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:13:09 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:13:09 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:10 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:11 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:12 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:13 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:14 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:14 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:15 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:16 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:17 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:18 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:18 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:13:19 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:20 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:21] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:13:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:13:21 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.75, #queue-req: 0, 
[2025-10-27 16:13:22 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:23 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:13:23 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:24 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:13:25 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:26 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:27 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:28 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:28 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:29 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:30 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:31 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:32 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:33 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:33 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:34 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:35 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:36 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:37 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:37] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:13:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:13:38 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:13:38 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:39 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:40 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:41 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:42 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:42 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:43 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:44 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:45 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:46 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:47 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:47 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:48 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:49 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:50 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:51 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:13:51 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:52 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:53 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:13:54] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:13:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:13:54 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:13:55 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:13:56 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:56 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:13:57 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:13:58 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:13:59 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:00 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:01 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:01 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:02 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:03 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:04 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:05 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:06 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:06 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:07 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:08 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:09 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:10 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:10] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:14:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:14:11 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:14:11 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:14:12 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:14:13 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:14 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:15 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:15 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:14:16 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:17 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:18 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:19 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:20 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:20 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:21 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:14:22 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:23 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:24 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:24 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:14:25 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:14:26 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:14:27] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:14:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:14:27 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.14, #queue-req: 0, 
[2025-10-27 16:14:28 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:29 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:29 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:30 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:31 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:32 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:33 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:34 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:34 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:35 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:36 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:37 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:38 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:39 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:39 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:40 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:41 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:42 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:43 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:14:43] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:14:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:14:44 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:14:44 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:14:45 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:14:46 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:47 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:48 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:48 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:14:49 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:50 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:51 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:52 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:14:53 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:53 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:54 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:55 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:56 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:57 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:14:57 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:58 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:14:59 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:00] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:15:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:15:00 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:15:01 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:02 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:02 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:03 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:04 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:05 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:06 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:07 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:07 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:08 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:09 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:10 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:11 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:12 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:12 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:13 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:14 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:15 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:16 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:16] INFO:     127.0.0.1:40636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:15:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:15:17 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:15:17 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-10-27 16:15:18 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:15:19 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:15:20 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:15:21 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:15:21 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:15:22 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:15:23 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:15:24 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:15:25 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:15:26 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:15:26 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:15:27 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:15:28 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:15:29 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:15:30 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:15:30 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:15:31 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:15:32 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:15:33] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:15:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:15:33 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:15:34 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:15:35 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:15:35 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:15:36 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:15:37 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:15:38 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:15:39 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:15:40 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:40 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:15:41 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:42 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:43 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:15:44 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:44 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:15:45 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:46 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:47 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:48 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:49 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:49] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:15:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:15:49 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:15:50 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:51 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:15:52 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:53 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:54 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:15:54 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:15:55 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:56 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:15:57 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:15:58 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:15:58 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:15:59 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:16:00 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:01 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:02 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:16:03 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:03 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:04 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:05 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:06] INFO:     127.0.0.1:60338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:16:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:16:06 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:16:07 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:08 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:08 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:09 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:10 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:11 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:12 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:13 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:13 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:14 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:15 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:16 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:17 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:17 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:18 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:19 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:20 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:21 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:22 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:22] INFO:     127.0.0.1:55926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:16:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:16:22 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:16:23 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:16:24 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:16:25 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:16:26 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:27 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:27 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:28 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:29 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:30 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:31 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:31 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:16:32 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:16:33 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:34 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:16:35 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:36 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:36 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:37 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:38 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:16:39] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:16:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:16:39 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:16:40 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:16:41 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:16:41 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:16:42 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:43 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:44 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:45 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:45 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:16:46 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:47 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:48 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:49 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:50 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:50 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:51 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:52 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:53 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:54 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:55 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:16:55] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:16:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:16:55 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:16:56 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:16:57 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:16:58 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:16:59 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:00 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:00 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:01 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:02 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:03 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:04 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:04 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:05 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:06 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:07 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:17:08 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:09 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:17:09 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:17:10 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:17:11 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:17:12] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:17:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:17:12 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.88, #queue-req: 0, 
[2025-10-27 16:17:13 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:17:14 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:17:14 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:15 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:16 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:17 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:17:18 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:18 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:19 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:20 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:21 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:22 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:23 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:23 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:24 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:25 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:17:26 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:27 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:28 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:17:28] INFO:     127.0.0.1:47320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:17:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:17:28 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:17:29 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:17:30 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:17:31 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:17:32 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:17:33 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:17:33 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:17:34 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:17:35 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:17:36 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:17:37 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:17:37 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:17:38 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:17:39 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:17:40 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:17:41 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:17:42 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:17:42 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:17:43 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:17:44 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:17:45] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:17:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:17:45 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:17:46 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:17:47 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:17:47 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:17:48 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:17:49 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:50 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:51 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:17:51 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:52 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:17:53 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:17:54 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:55 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:17:56 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:56 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:57 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:17:58 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:17:59 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:00 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:00 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:01] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:18:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:18:01 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:18:02 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:18:03 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:18:04 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:18:05 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:18:05 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:18:06 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:18:07 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:18:08 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:18:09 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:10 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:18:10 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:11 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:12 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:13 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:18:14 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:18:14 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:18:15 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:18:16 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:18:17 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:18:18] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:18:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:18:18 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:18:19 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:18:19 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:20 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:18:21 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:18:22 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:18:23 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:18:24 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:18:24 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:18:25 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:26 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:27 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:28 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:28 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:29 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:30 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:31 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:32 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:18:33 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:33 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:34] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:18:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:18:34 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:18:35 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:18:36 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:18:37 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:18:38 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:38 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:18:39 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:40 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:41 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:42 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:43 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:43 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:44 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:45 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:46 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:47 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:47 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:48 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:49 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:50 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:18:51] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:18:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:18:51 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:18:52 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:18:52 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:18:53 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:54 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:55 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:56 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:18:57 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:18:57 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:18:58 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:18:59 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:00 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:01 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:01 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:02 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:03 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:04 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:05 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:06 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:06 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:07] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:19:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:19:07 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:19:08 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:19:09 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:19:10 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:19:11 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:19:11 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:12 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:13 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:14 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:15 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:15 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:16 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:17 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:18 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:19 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:20 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:20 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:21 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:22 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:23 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:24] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:19:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:19:24 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:19:25 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:19:25 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:19:26 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:19:27 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:19:28 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:19:29 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:30 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:30 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:19:31 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:32 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:33 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:34 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:34 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:35 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:36 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:37 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:38 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:39 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:39 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:19:40] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:19:40 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:19:41 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:42 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:19:43 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:19:44 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:44 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:19:45 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:19:46 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:19:47 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:19:48 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:19:48 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:49 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:50 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:51 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:52 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:19:53 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:19:53 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:54 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:55 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:56 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:19:57] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:19:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:19:57 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:19:58 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:19:58 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:19:59 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:20:00 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:20:01 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:02 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:02 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:20:03 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:04 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:05 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:06 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:07 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:20:07 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:20:08 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:20:09 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:20:10 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:20:11 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:20:11 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:20:12 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:20:13] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:20:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:20:13 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:20:14 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:20:15 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:20:16 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:20:16 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:20:17 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:20:18 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:20:19 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:20:20 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:20:21 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:21 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:20:22 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:23 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:24 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:25 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:20:26 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:20:26 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:20:27 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:20:28 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.43, #queue-req: 0, 
[2025-10-27 16:20:29 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:29] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:20:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:20:30 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.16, #queue-req: 0, 
[2025-10-27 16:20:31 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:20:31 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:20:32 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:33 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:20:34 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:35 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:20:35 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:36 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:37 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:38 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.41, #queue-req: 0, 
[2025-10-27 16:20:39 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:20:40 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:40 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:41 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:42 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:20:43 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:44 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:20:45 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:20:45 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:20:46] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:20:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:20:46 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.15, #queue-req: 0, 
[2025-10-27 16:20:47 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:20:48 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-10-27 16:20:49 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:50 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:50 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:51 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:52 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:53 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:20:54 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:54 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:55 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:56 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:57 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:58 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:20:59 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:20:59 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:21:00 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:21:01 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:21:02 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:21:03] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:21:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:21:03 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.15, #queue-req: 0, 
[2025-10-27 16:21:04 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:21:04 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:21:05 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:21:06 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:07 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:08 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:08 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:09 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:10 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:11 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:12 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:13 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:13 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:14 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:15 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:16 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:21:17 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:18 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:21:18 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:19] INFO:     127.0.0.1:37762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:21:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:21:19 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:21:20 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:21 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:22 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:23 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:23 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:24 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:25 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:26 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:27 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:27 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:28 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:29 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:30 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:31 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:32 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:32 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:21:33 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:34 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:21:35 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:21:36] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:21:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:21:36 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:21:37 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:21:37 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:21:38 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:21:39 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:21:40 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:41 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:21:41 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:42 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:21:43 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:21:44 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:45 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:46 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:46 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:21:47 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:48 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:49 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:50 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:50 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:51 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:21:52] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:21:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:21:52 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:21:53 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:54 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:55 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:21:55 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:56 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:57 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:58 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:21:59 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:00 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:00 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:01 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:02 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:03 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:04 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:05 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:05 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:06 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:07 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:08 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:08] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:22:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:22:09 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:22:10 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:22:10 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:22:11 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:22:12 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:22:13 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:14 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:22:14 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:15 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:16 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:17 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:18 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:19 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:19 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:20 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:21 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:22 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:23 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:23 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:24 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:25] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:22:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:22:25 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:22:26 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:22:27 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:22:28 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:22:28 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:29 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:30 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:31 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:32 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:33 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:33 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:34 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:35 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:36 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:37 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:37 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:38 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:39 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:40 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:41 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:41] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:22:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:22:42 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:22:42 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:22:43 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:44 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:22:45 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:46 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:47 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:47 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:48 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:49 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:50 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:51 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:52 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:52 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:22:53 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:54 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:55 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:22:56 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:56 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:22:57 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:22:58] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:22:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:22:58 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:22:59 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:23:00 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:01 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:01 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:02 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:03 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:04 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:05 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:06 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:06 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:07 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:08 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:09 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:10 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:10 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:11 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:12 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:13 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:23:14 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:14] INFO:     127.0.0.1:37688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:23:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:23:15 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:23:15 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:23:16 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:23:17 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:23:18 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:23:19 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:20 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:20 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:21 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:22 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:23 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:24 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:24 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:25 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:26 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:27 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:28 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:29 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:29 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:30 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:31] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:23:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:23:31 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:23:32 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:23:33 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:34 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:34 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:35 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:36 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:37 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:38 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:39 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:39 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:40 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:41 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:42 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:43 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:43 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:44 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:45 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:46 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:47 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:47] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:23:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:23:48 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:23:48 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:23:49 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:50 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:23:51 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:52 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:53 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:53 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:54 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:23:55 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:56 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:23:57 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:57 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:23:58 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:23:59 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:24:00 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:24:01 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:24:02 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:24:02 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:24:03 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:24:04] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:24:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:24:04 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:24:05 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:24:06 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:24:07 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:24:07 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:24:08 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:09 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:24:10 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:11 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:11 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:12 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:24:13 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:14 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:15 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:16 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:16 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:17 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:18 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:24:19 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:20 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:20] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:24:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:24:21 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-10-27 16:24:21 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:22 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:23 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:24 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:25 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:25 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:26 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:27 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:28 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:29 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:30 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:30 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:31 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:32 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:33 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:24:34 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:24:34 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:24:35 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:36 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:24:37] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:24:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:24:37 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-27 16:24:38 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:39 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:39 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:40 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:41 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:24:42 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:24:43 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:44 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:44 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:45 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:46 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:24:47 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:24:48 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:49 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:49 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:50 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:51 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:24:52 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:24:53 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:24:53] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:24:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:24:54 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:24:54 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:24:55 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:24:56 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:57 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:24:58 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:24:58 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:24:59 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:25:00 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:01 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:02 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:03 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:03 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:25:04 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:25:05 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:06 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:25:07 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:25:07 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:25:08 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:25:09 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:25:10] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:25:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:25:10 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-27 16:25:11 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:12 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-27 16:25:12 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:13 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:14 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:15 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:16 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-27 16:25:17 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:25:17 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:18 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:25:19 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:25:20 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-10-27 16:25:21 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:25:21 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:25:22 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:25:23 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:25:24 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:25:25 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:25:26 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:25:26] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:25:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:25:26 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-10-27 16:25:27 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:25:28 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:29 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:25:30 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:25:31 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:31 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:25:32 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:33 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:34 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:35 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:35 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:36 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:37 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:38 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:39 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:40 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:40 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:41 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:42 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:43] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:25:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:25:43 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:25:44 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:25:45 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:25:45 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:25:46 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:25:47 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:48 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:25:49 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:49 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:50 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:51 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:52 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:53 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:25:54 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:54 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:55 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:56 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:57 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:25:58 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:25:58 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:25:59] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:25:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:25:59 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:26:00 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:26:01 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:26:02 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:26:03 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:26:03 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:26:04 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:26:05 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:26:06 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:26:07 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:26:08 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:26:08 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:09 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:10 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:11 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:12 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:26:12 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:26:13 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:26:14 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:26:15 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:16] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:26:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:26:16 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-10-27 16:26:17 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:26:17 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:26:18 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:26:19 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:26:20 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:26:21 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:26:22 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:26:22 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:26:23 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:26:24 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:26:25 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:26 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:26:27 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:26:27 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:26:28 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:29 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:30 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:31 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:31 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:32] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:26:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:26:32 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:26:33 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:34 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:35 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:36 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:26:36 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:26:37 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:38 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:26:39 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:26:40 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:26:41 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:26:41 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:26:42 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:26:43 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:26:44 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:26:45 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:26:45 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:26:46 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:26:47 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:26:48 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:26:49] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:26:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:26:49 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-10-27 16:26:50 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:26:50 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:26:51 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:26:52 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:26:53 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:26:54 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:26:55 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:26:55 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:26:56 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:26:57 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:26:58 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:26:59 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:26:59 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:27:00 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:27:01 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:27:02 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:27:03 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:27:04 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:27:04 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:27:05] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:27:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:27:05 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-10-27 16:27:06 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:07 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:08 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:09 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:09 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:10 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:11 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:12 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:13 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:14 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:14 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:15 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:16 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:17 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:18 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:18 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:19 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:27:20 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:27:21 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:27:22] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:27:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:27:22 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:27:23 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:23 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:24 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:25 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:26 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:27 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:28 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:28 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:29 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:30 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:31 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:32 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:32 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:33 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:34 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:35 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:27:36 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:27:37 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:27:37 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:27:38] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:27:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:27:38 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.17, #queue-req: 0, 
[2025-10-27 16:27:39 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:27:40 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:41 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:42 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:42 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:43 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:44 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:27:45 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:46 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:47 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:47 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:48 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:27:49 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:50 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:51 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:51 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.70, #queue-req: 0, 
[2025-10-27 16:27:52 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:53 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:27:54 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:27:55] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:27:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:27:55 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-10-27 16:27:56 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:27:56 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:27:57 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:27:58 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:27:59 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:28:00 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:01 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:01 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:28:02 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:03 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:28:04 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:05 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:05 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:06 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:07 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:08 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:09 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:10 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:10 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:11] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:28:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:28:11 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.80, #queue-req: 0, 
[2025-10-27 16:28:12 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:28:13 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:28:14 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:28:15 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:28:15 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:16 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:17 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:18 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:19 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:20 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:20 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:21 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:22 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:23 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:24 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:24 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:25 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:26 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:28:27 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:28:28] INFO:     127.0.0.1:34120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:28:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:28:28 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:28:29 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:28:29 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:28:30 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:28:31 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:32 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:33 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:28:34 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.69, #queue-req: 0, 
[2025-10-27 16:28:34 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:35 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:28:36 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:37 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:28:38 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:38 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:39 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:28:40 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.67, #queue-req: 0, 
[2025-10-27 16:28:41 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.68, #queue-req: 0, 
[2025-10-27 16:28:42 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:28:43 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:28:43 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.66, #queue-req: 0, 
[2025-10-27 16:28:44] INFO:     127.0.0.1:48744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:28:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:28:44 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.13, #queue-req: 0, 
[2025-10-27 16:28:45 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:46 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:47 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:28:48 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:48 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:49 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:50 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:51 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:52 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:53 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:53 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:54 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:55 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.74, #queue-req: 0, 
[2025-10-27 16:28:56 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.73, #queue-req: 0, 
[2025-10-27 16:28:57 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:28:57 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:28:58 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.71, #queue-req: 0, 
[2025-10-27 16:28:59 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:29:00 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.72, #queue-req: 0, 
[2025-10-27 16:29:01] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:29:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:29:01 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:29:02 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:02 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:03 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:29:04 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:05 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:06 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:29:07 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:07 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:08 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:29:09 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:10 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:11 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:11 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:12 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:13 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:29:14 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:29:15 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:29:16 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:29:16 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:29:17] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:29:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:29:17 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:29:18 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-10-27 16:29:19 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:29:20 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:29:21 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:29:21 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:29:22 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:29:23 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-10-27 16:29:24 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:29:25 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:29:25 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:29:26 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:29:27 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:29:28 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:29:29 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:29:30 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:29:30 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:29:31 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:32 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:29:33 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:29:34] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:29:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:29:34 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-10-27 16:29:35 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:29:35 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:29:36 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:29:37 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:29:38 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:29:39 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:39 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:40 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:29:41 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:42 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:29:43 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:29:44 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:44 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:29:45 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:29:46 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:29:47 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:29:48 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:48 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:49 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:50] INFO:     127.0.0.1:51652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:29:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:29:50 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:29:51 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:29:52 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:29:53 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:29:53 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:29:54 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:55 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:56 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:29:57 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:29:58 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:58 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:29:59 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:00 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:01 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:02 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:03 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:03 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:04 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:05 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:30:06 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:06] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:30:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:30:07 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:30:08 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:30:08 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:30:09 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:10 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:11 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:12 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:12 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:13 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:14 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:15 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:16 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:17 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:17 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:18 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:19 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:20 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:21 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-10-27 16:30:21 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:22 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:23] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:30:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:30:23 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:30:24 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:30:25 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:30:26 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:26 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:27 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:28 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:29 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:30 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:31 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:31 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:32 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:33 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:34 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:35 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:35 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:36 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:37 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:38 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:39 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:39] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:30:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:30:40 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:30:40 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:30:41 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:30:42 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:43 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:44 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:30:45 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:45 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:46 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:47 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:48 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:49 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:30:49 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:50 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:30:51 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:52 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:30:53 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:54 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:54 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:30:55 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:30:56] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:30:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:30:56 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:30:57 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:30:58 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:30:59 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:30:59 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:31:00 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:01 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:02 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:03 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:04 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:04 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:05 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:06 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:07 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:08 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:08 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:09 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:10 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:11 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:12 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:12] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:31:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:31:13 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:31:13 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:31:14 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:31:15 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:16 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:17 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:18 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:18 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:19 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:20 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:21 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:22 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:22 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:23 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:24 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:25 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:26 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:27 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:27 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:28 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:29] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:31:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:31:29 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.25, #queue-req: 0, 
[2025-10-27 16:31:30 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:31 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:31:32 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:32 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:31:33 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:34 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:35 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:31:36 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:36 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:37 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:38 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:31:39 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:40 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:41 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:41 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:42 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:43 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:44 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:45 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:45] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:31:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:31:46 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-10-27 16:31:46 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:31:47 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:48 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:49 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:50 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:50 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:51 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:52 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:53 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:54 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:31:55 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:55 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:56 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:31:57 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:31:58 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:31:59 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:31:59 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-10-27 16:32:00 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:32:01 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:32:02] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:32:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:32:02 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:32:03 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:32:04 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:32:04 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:32:05 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:32:06 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:32:07 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:08 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:09 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:32:09 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:10 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:11 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:12 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:13 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:13 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:32:14 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:15 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:16 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:17 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:18 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:18] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:32:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:32:18 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.23, #queue-req: 0, 
[2025-10-27 16:32:19 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:20 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:21 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:22 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:32:23 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:23 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:24 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:25 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:26 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:27 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:28 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:28 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:29 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:30 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:31 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:32 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:32 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:33 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:34 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:32:35] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:32:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:32:35 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:32:36 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:37 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:37 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:32:38 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:39 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:32:40 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:41 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:42 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:42 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:43 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:44 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:45 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:32:46 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:32:46 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:32:47 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:32:48 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:32:49 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:32:50 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:32:51 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:32:51] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:32:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:32:51 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:32:52 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:32:53 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:54 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:55 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:32:56 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:32:56 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:32:57 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:32:58 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:32:59 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:00 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:00 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:01 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:02 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:03 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:04 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:05 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:05 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:06 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:07 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:08] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:33:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:33:08 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:33:09 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:10 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:10 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:11 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:12 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:13 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:14 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:14 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:15 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:16 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:17 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:18 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:19 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:19 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:33:20 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:21 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:33:22 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:23 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-10-27 16:33:23 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:24] INFO:     127.0.0.1:52528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:33:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:33:24 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.20, #queue-req: 0, 
[2025-10-27 16:33:25 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:33:26 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:33:27 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:33:28 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:28 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:29 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:30 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:31 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:32 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:33 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:33 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:34 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:35 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:36 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:37 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:38 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:38 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:39 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:33:40 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:41] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:33:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:33:41 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.24, #queue-req: 0, 
[2025-10-27 16:33:42 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:33:43 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:33:43 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:44 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:45 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:33:46 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:47 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:47 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:33:48 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:33:49 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:50 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:51 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:52 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:52 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:53 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:54 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:55 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:56 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:33:56 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:33:57] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:33:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:33:57 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.26, #queue-req: 0, 
[2025-10-27 16:33:58 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-10-27 16:33:59 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:34:00 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:34:01 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:01 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:34:02 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:03 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:34:04 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:05 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:06 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:06 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:07 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:08 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:09 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:10 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:10 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:34:11 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:12 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:13 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:14] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:34:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:34:14 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.06, #queue-req: 0, 
[2025-10-27 16:34:15 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:15 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:16 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:34:17 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:18 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:19 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:20 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:20 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:21 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:22 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:23 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:24 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:24 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:34:25 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:34:26 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:34:27 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:34:28 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-10-27 16:34:29 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-10-27 16:34:29 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-10-27 16:34:30] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-27 16:34:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-27 16:34:30 TP0] Decode batch, #running-req: 1, #token: 3208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-10-27 16:34:31 TP0] Decode batch, #running-req: 1, #token: 3248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-10-27 16:34:32 TP0] Decode batch, #running-req: 1, #token: 3288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-10-27 16:34:33 TP0] Decode batch, #running-req: 1, #token: 3328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-10-27 16:34:34 TP0] Decode batch, #running-req: 1, #token: 3368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:34:34 TP0] Decode batch, #running-req: 1, #token: 3408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:35 TP0] Decode batch, #running-req: 1, #token: 3448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-10-27 16:34:36 TP0] Decode batch, #running-req: 1, #token: 3488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:37 TP0] Decode batch, #running-req: 1, #token: 3528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:38 TP0] Decode batch, #running-req: 1, #token: 3568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:38 TP0] Decode batch, #running-req: 1, #token: 3608, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:39 TP0] Decode batch, #running-req: 1, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:40 TP0] Decode batch, #running-req: 1, #token: 3688, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-10-27 16:34:41 TP0] Decode batch, #running-req: 1, #token: 3728, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:42 TP0] Decode batch, #running-req: 1, #token: 3768, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:43 TP0] Decode batch, #running-req: 1, #token: 3808, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:34:43 TP0] Decode batch, #running-req: 1, #token: 3848, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-10-27 16:34:44 TP0] Decode batch, #running-req: 1, #token: 3888, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:45 TP0] Decode batch, #running-req: 1, #token: 3928, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-10-27 16:34:46 TP0] Decode batch, #running-req: 1, #token: 3968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-10-27 16:34:47] INFO:     127.0.0.1:40410 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-10-27 16:34:54] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-27 16:34:55] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
